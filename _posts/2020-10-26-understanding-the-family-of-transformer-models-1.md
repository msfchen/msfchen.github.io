---
layout: post
title: "Understanding the Family of Transformer Models. Part I - Language Models"
date: 2020-10-26
---
Language understanding and language generation are inherently sequential processes that have been modeled by various forms of gated recurrent neural network (RNN) in the mid-2010s. The recursive compression nature of gated RNNs suffer from the lack of structural alignment ability and computational parallelizability. To address the alignment issues between input elements and output elements in sequence-to-sequence models, Bahdanau et al., 2014<sup>[\[1\]](#ref1)</sup> and Chorowski et al. 2014<sup>[\[2\]](#ref2)</sup> introduced attention mechanism in RNNs for machine translation and speech recognition, respectively. To develop a structure-aware general-purpose language model, Cheng et al. 2016<sup>[\[3\]](#ref3)</sup> introduced self-attention mechanism in RNN to relate tokens within the entire input sequence. To address the parallelizability issue, Parikh et al. 2016<sup>[\[4\]](#ref4)</sup> introduced a pure attention model, abandoning RNN, for natural language inference, which outperformed state-of-the-art RNNs with much fewer parameters. The idea of pure attention model was greatly scaled up, by increasing the number of attention layers and the number of attention functions (i.e. attention heads) per layer, in the Transformer model<sup>[\[5\]](#ref5)</sup> that achieved state-of-the-art performance on machine translation. The transformer has become the de facto architecture for language modeling. There is a trend of scaling up the capacity of the transformer model and pre-training it on ever increasing amount of data, in pursuit of new state-of-the-art benchmark scores. Some of the most prominent models in this trend are reviewed here.

- [The Transformer Model](#the-transformer-model)
    - [Model Architecture](#model-architecture)
    - [Model Performance](#model-performance)
- [Language Models Based on the Transformer](#language-models-based-on-the-transformer)
    - [Standard Language Models](#standard-language-models)
        - [GPT](#gpt)
        - [GPT-2](#gpt-2)
        - [GPT-3](#gpt-3)
    - [Masked Language Models](#masked-language-models)
        - [BERT](#bert)
        - [RoBERTa](#roberta)
    - [Permutation Language Models](#permutation-language-models)
        - [XLNet](#xlnet)
    - [Denoising Language Models](#denoising-language-models)
        - [T5](#t5)
        - [BART](#bart)
- [Codes](#codes)
- [References](#references)

## **The Transformer Model**

The Transformer model is an encoder-decoder architecture that encodes an input sequence *(x<sub>1</sub>, ...,x<sub>n</sub>)* to a concept sequence *z=(z<sub>1</sub>, ...,z<sub>n</sub>)* for all input tokens simultaneously and then decodes *z* to an output sequence *(y<sub>1</sub>, ...,y<sub>m</sub>)* one output element at a time. When generating the next output element, all the previously generated output elements are used as additional inputs to the decoder.

### **Model Architecture**

<p align="center"><img src="../../../assets/images/transformer_architecture.png">
<br><em>Source of the Diagram: Vaswani et al., 2017</em></p>
The Embedding layers convert input and output tokens to vectors of dimension *d<sub>model</sub>* (512 for base and 1024 for big). The two embedding layers share the same weight matrix that is also shared with the linear transformation layer before the softmax function of the decoder output. But the weights in the embedding layers are multiplied by $$\sqrt{d_{model}}$$.

The positions of the input tokens in the input sequence are encoded by "positional encodings" into the same dimension *d<sub>model</sub>* as the embeddings, so that the two can be summed. Each dimension of the positional encoding corresponds to a sine or cosine functions of different frequences: $$PE_{(pos, 2i)}=sin(pos/10000^{2i/d_{model}})$$ and $$PE_{(pos, 2i+1)}=cos(pos/10000^{2i/d_{model}})$$, where *pos* is the position and *i* is the dimension.

The encoder has N = 6 identical layers, each with a multi-head self-attention sub-layer, followed by a fully connected feed-forward sub-layer. Both sub-layers have a residual connection, followed by layer normalization. Each attention head first linearly transforms K (keys), V (values), and Q (queries) using a set of "reading" weight matrices and then computes scaled dot-product attention. Different attention heads have different set of "reading" weight matrices and their scaled dot-product attention ressults are concatenated together, followed by a linear transformation to complete the multi-head attention. The dimensions of K, V, Q are $$d_k$$, $$d_v$$, and $$d_k$$, respectively; and the dimensions of their corresponding "reading" weight matrices are $$d_{model}\times d_k$$, $$d_{model}\times d_v$$, and $$d_{model}\times d_k$$, respectively. If the number of heads is *h*, then $$d_k = d_v = d_{model}/h$$. The dimension of the weight matrix for the linear projection of the concatenated result is $$hd_v\times d_{model}$$.
<p align="center">$$MultiHead(Q, K, V)=Concat(head_1, ..., head_h)W^O$$, where $$head_i=Attention(QW_i^Q, KW_i^K, VW_i^V)=softmax(\frac{QW_i^Q(KW_i^K)^T}{\sqrt{d_k}})VW_i^V$$.</p>
The fully connected feed-forward sub-layer contains two linear transformations with a ReLU activation in between, $$FFN(x)=max(0, xW_1+b_1)W_2+b_2$$.

The decoder also has N = 6 identical layers, but differs from the encoder with an additional multi-head encoder-decoder attention sub-layer in the middle and a "masked" multi-head self-attention sub-layer. In the encoder-decoder attention, the Q are from the previous decoder layer and the K and V are from the output of the encoder. This allows every element in the decoder to attend all elements in the input sequence. In the "masked" self-attention, each position in the decoder can only attend to left positions up to and including itself, which was implemented in scaled dot-product by setting values to $$-\infty$$ in the input to softmax for right-ward attention. The masking ensures that the predictions for the next output element can only depend on previously predicted output elements.

### **Model Performance**

The big transformer model established new state-of-the-art performance on both the WMT 2014 English-to-German and English-to-French translation tasks. The base transformer model also surpassed previous state-of-the-art performance on the WMT 2014 English-to-German translation task at a fraction of the training cost.

In this study, both input and output sentences were tokenized into subword units so that rare words could be handled deterministically. The subword tokenization was implemented with Byte Pair Encoding<sup>[\[6\]](#ref6)</sup> for English-German dataset and with Wordpiece Model<sup>[\[7\]](#ref7)</sup> for English-French dataset.

The number of attention heads *h = 8* yielded the best performance for the base transformer model; decreasing or increasing *h* (corresponding to increasing or decreasing $$d_k$$) reduced performance.

Using a 4-layer transformer with $$d_{model}=1024$$ for English constituency parsing task, the transformer model outperformed most of the previously reported models, except the state-of-the-art RNN models at that time.

## **Language Models Based on the Transformer**

Because language models can be trained on unlabeled text data and large amount of text data are readily available, general-purpose language models using the transformer architecture have been built with ever-increasing capacity. The learned language knowledge in such large-scale models can then be transfered to perform a wide variety of tasks by fine-tuning with small amount of task-specific labeled data. These models can be categorized into four groups, based on their language modeling approaches.

| Type of LM | Objective Function | Definitions |
| :----: | :----: | ---- |
| Standard Autoregressive (AR) | $$\underset{\theta}{\max}\;\log\mathit{p}_{\theta}(\mathrm{\mathbf{x}})=\sum\limits_{t=1}^{T}\log\mathit{p}_{\theta}(x_{t}\|\mathrm{\mathbf{x}}_{\lt t})$$ | 1. $$\mathrm{\mathbf{x}}=[x_{1},...,x_{T}]$$ is token sequence.<br>2. $$\theta$$ is model parameters. |
| Masked Autoencoding (AE) | $$\underset{\theta}{\max}\;\log\mathit{p}_{\theta}(\mathrm{\mathbf{\bar x}}\|\mathrm{\mathbf{\hat x}})\approx \sum\limits_{t=1}^{T} m_{t}\log\mathit{p}_{\theta}(x_{t}\|\mathrm{\mathbf{\hat x}})$$ | 1. $$\mathrm{\mathbf{\hat x}}=\mathrm{\mathbf{x}}$$ with 15% of tokens replaced by [MASK].<br>2. $$\mathrm{\mathbf{\bar x}}=$$ masked tokens<br>3. $$m_{t}=1$$ when $$x_{t}$$ is masked, 0 otherwise. |
| Permutation Autoregressive | $$\underset{\theta}{\max}\;\mathbb{E}_{\mathrm{\mathbf{z}}\sim \mathcal{Z}_{T}}\bigg[\sum\limits_{t=1}^{T}\log\mathit{p}_{\theta}(x_{z_{t}}\|\mathrm{\mathbf{x}}_{\mathrm{\mathbf{z}}_{\lt t}})\bigg]$$ | 1. $$\mathcal{Z}_{T}=$$ the set of all possible, $$T!$$, permutations of the index sequence [1, 2,..., T].<br>2. a permutation $$\mathrm{\mathbf{z}}\in \mathcal{Z}_{T}$$.<br>3. $$z_{t}=$$ the t-th element of $$\mathrm{\mathbf{z}}$$.<br>4. $$\theta$$ is shared across all permutations. |
| Denoising Autoencoding | $$\underset{\theta}{\max}\;\log\mathit{p}_{\theta}(\mathrm{\mathbf{y}}\|\mathrm{\mathbf{x}})=\sum\limits_{t=1}^{T_{y}}\log\mathit{p}_{\theta}(y_{t}\|\mathrm{\mathbf{x}},\mathrm{\mathbf{y_{\lt t}}})$$ | 1. $$\mathrm{\mathbf{x}}=[x_{1},...,x_{T_{x}}]$$ is the noisy source sequence.<br>2. $$\mathrm{\mathbf{y}}=[y_{1},...,y_{T_{y}}]$$ is the corresponding clean sequence.<br>3. sequence-to-sequence on encoder-decoder architecture. |

Lewis et al., 2020<sup>[\[19\]](#ref19)</sup> provided a succinct illustration below to compare BERT, a masked LM, GPT, a standard autoregressive LM, and BART, a denoising LM.
<p align="center"><img src="../../../assets/images/LM.png"></p>

### **Standard Language Models**

Standard language model's objective is to maximize the conditional probability of generating a token given all the *k* previously generated tokens, where *k* is the size of the context window. This type of language modeling is referred to as Causal Language Modeling or Autoregressive Language Modeling by some authors. The decoder part of the transformer model, without the encoder-decoder attention sub-layer, is a natural fit for a language model. The groups at OpenAI adopted the decoder portion of the transformer model to build a series of high-capacity language models in a process named **G**enerative **P**re-**T**raining (GPT).

#### **GPT**

Radford et al., 2018<sup>[\[8\]](#ref8)</sup> introduced the GPT model, as illustrated below. The Transformer blocks shown in the right figure are the GPT variant decoder-only stack (the left figure) of the Transformer.
<p align="center"><img src="../../../assets/images/gpt_architecture.png"></p>

The total number of parameters of GPT are between Transformer<sub>Base</sub> and Transformer<sub>Big</sub>, with the number of layers *N = 12*, the number of heads *h = 12*, and the dimension of the embedding *d<sub>model</sub> = 768*. Learned positional embeddings were used, instead of the original sinusoidal positional encoding. The BooksCorpus dataset of 11,038 books in 16 different genres, tokenized with a bytepair encoding vocabulary of 40,000, was used to train the language model.

The parameters of the pre-trained language model were then used for supervised fine-tuning tasks. In a labeled dataset, each instance contains a sequence of input tokens and a label. A new layer consisted of a linear transformation and a softmax function is added to convert the final output element (the Extract token in the diagram) of the decoder to the probability of the corresponding label. The objective of the fine-tuning is to maximize the probability of the given label, conditional to the given sequence of tokens. To improve generalization and accelerate convergence, language model objective is added, as an auxiliary objective, to the fine-tuning objective.

For text classification task, the fine-tuning model above can be used directly. But for some other tasks, structured inputs have to be converted into an ordered sequence with a delimiter token in between. For textual entailment or natural language inference (NLI) tasks, each premise-hypothesis statements pair is concatenated into an ordered sequence with three possible labels: entailing, contradictory, or neutral. For semantic similarity or paraphrase detection tasks, both orders of the two sentences are processed independently and their final element outputs from the decoder are added element-wise before being fed into the linear output layer that predicts whether the two sentences are equivalent or not. For multiple-choice problems, such as Question Answering or Commonsense Reasoning, given context document *z*, question *q*, and a set of *k* possible answers are concatenated into [*z*, *q*, delimiter token, *$$a_i$$*], each of which is processed independently to predict whether the answer is correct. The *k* predictions are then normalized via a softmax layer to produce an output distribution. Fine-tuning could be done quickly and 3 epochs of training was sufficient for most cases.

In NLI tasks, the GPT model significantly outperformed previous best models in four of the five datasets examined. In Question Answering using RACE dataset and Commonsense Reasoning using Story Cloze, the GPT model significantly outperformed previous best models. In semantic similarity tasks, the GPT model outperformed the previous best on two of the three datasets examined. In classification tasks, the GPT model significantly outperformed the previous best model on one of the two datasets examined. The GPT model achieved a new state-of-the-art overall score on the GLUE benchmark, a 9-task benchmark for natural language understanding<sup>[\[9\]](#ref9)</sup>.

Analyses of zero-shot (performing downstream tasks without fine-tuning) behaviors demonstrated that the language model acquired useful linguistic knowledge for downstream tasks, including linguistic acceptability (grammatical correctness), sentiment binary classification, question answering, and commonsense reasoning (winograd schemas challenge).

#### **GPT-2**

To test the hypothesis that language model with sufficient capacity can perform well on multiple tasks without transfer learning, Radford et al., 2019<sup>[\[10\]](#ref10)</sup> introduced GPT-2 model that has the same architecture as GPT, but with drastically increased capacity (number of layers *N = 48* and dimension of the embedding *d<sub>model</sub> = 1600*). Other modifications to GPT included moving layer normalization from the output to the input of each sub-layer, adding layer normalization to the output of the final self-attention block, and scaling the weights of residual layers at initialization by a factor of *1/$$\sqrt{N}$$* where *N* is the number of residual layers. The context size was increased from 512 to 1024. The training dataset, WebText, was 40GB of text from over 8 million web pages that were outbound links from Reddit with at least 3 karma (proportional to user upvotes), excluding Wikipedia pages. Byte Pair Encoding vocabulary size was 50,257.

The GPT-2 language model (LM) was evaluated for LM accuracy or perplexity in zero-shot setting on 8 different datasets and it outperformed the state-of-the-art models on 7 out of the 8 datasets. The GPT-2 also matched or exceeded some supervised baseline models on commonsense reasoning (Winograd Schema challenge) and reading comprehension (CoQA) tasks, but performed far worse than some supervised or fine-tuned models on other tasks, including summarization, translation, and question answering. The GPT-2 may still underfit the WebText, because the perplexity of the test set continued to go down.

#### **GPT-3**

Moving further along the same path, Brown et al., 2020<sup>[\[11\]](#ref11)</sup> introduced GPT-3 model that has the same architecture as GPT-2, but with drastically increased capacity (number of layers *N = 96*, dimension of the embedding *d<sub>model</sub> = 12288*, number of heads *h=96*, and context window size 2048). Another modification was to use factorized self-attention heads with alternating dense and locally banded sparse attention patterns, similar to the Sparse Transformer<sup>[\[12\]](#ref12)</sup>, for faster attention operations. The training dataset were from 4 sources: WebText2 (similar to WebText of GPT-2, but over longer period), Wikipedia, two internet-based books corpora (Books1 and Books2), and Common Crawl. Fuzzy deduplication at document level was performed within and across datasets. Low quality documents in Common Crawl dataset were removed using a classifier trained with high quality examples from WebText2. The final total byte-pair-encoded tokens were about 500 billion. During training, higher-quality datasets were sampled more frequently. Data overlaps between LM training dataset and test datasets of benchmarks studies were reduced, but not completely removed.

The GPT-3 LM model was evaluated on over two dozen benchmarks datasets and several novel tasks for three conditions, "few-shot learning", "one-shot learning", and "zero-shot learning". The "*K*-shot learning" refers to the number of examples included in the input sequence, in the form of $$(task description, (prompt, answer)\times K, prompt)$$, at inference time without any weight updates. The *K* in "few-shot learning" was typically in the range of 10 and 100. Overall, increasing *K* increased task performance and such increase was steeper for larger models.

The GPT-3 achieved new state-of-the-art performance on some datasets, including zero-shot perplexity on the Penn Tree Bank (PTB) dataset (a traditional language modeling dataset), few-shot accuracy on LAMBDA dataset (reading a paragraph and predicting the last word of sentences), one-shot and few-shot results on TriviaQA dataset (closed-book question answering), few-shot results on WMT'14 Fr->En and WMT'16 De->En translation, zero-, one-, and few-shot results on PIQA dataset (common sense questions about how the physical world works).

The GPT-3 still performed worse, by a large margin in some cases, than fine-tuned state-of-the-art models on many other datasets, including HellaSwag dataset (selecting the best ending to a story or set of instructions) and StoryCloze dataset (selecting the correct ending sentence for five-sentence long stories), Natural Question and WebQuestions datasets (closed-book question answering), WMT'14 En->Fr, WMT'16 En->De, and WMT'16 En<->Ro translation, Winograd Schema Challenge and adversarially-mined Winogrande datasets (determining which word a gramatically ambiguous pronoun refers to), ARC (Easy) and ARC (Challenge) datasets (multiple-choice questions from 3rd to 9th grade science exams), OpenBookQA dataset (multi-hop reasoning with partial context provided by elementary level science facts), 5 reading comprehesion datasets of different formats (CoQA, QuAC, DROP, SQuADv2, RACE), the standardized collection of datasets of the SuperGLUE benchmark, Adversarial Natural Language Inference (ANLI) dataset.

Few-shot settings of the GTP-3 also demonstrated some abilities in many synthetic and qualitative tasks, including simple arithmetic operations on integers with 3 or less digits, character manipulation and word unscrambling tasks (cycle letters in words, anagrams of all but first and last/last 2 characters, random insertion in word, reversed words), SAT Analogies (multiple choice questions from the college entrance exam for selecting the same type of word pair relationship), correcting English grammar, learning and using novel words. The GPT-3 can generate samples of news articles which human evaluators have difficult distinguishing from articles written by humans.

### **Masked Language Models**

In addition to the unidirectional generative approach, as applied in building the GPT models, language models can also be built using bidirectional masked approach, where some tokens from the input are randomly masked and the objective is to predict the original token of the masked position based on its context on both sides. The BERT (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers) subfamily of models are masked language models based on the encoder portion of the transformer model.

#### **BERT**

Devlin et al., 2019<sup>[\[13\]](#ref13)</sup> introduced the BERT model, as illustrated below. Similar to GPT, BERT consists of two stages: pre-training and fine-tuning. The model is first pre-trained on unlabeled data over two different tasks; then, each downstream task has a separate fine-tuned model, initialized with the same pre-trained parameters and trained with task-specific labeled data.
<p align="center"><img src="../../../assets/images/bert_architecture.png"></p>

The model architecture of BERT is almost identical to the encoder portion of the transformer. The number of layers *N*, the number of heads *h*, and the dimension of the embedding *d<sub>model</sub>* are (*N=12*, *h=12*, *d<sub>model</sub>=768*) for *BERT<sub>BASE</sub>* and (*N=24*, *h=16*, *d<sub>model</sub>=1024*) for *BERT<sub>LARGE</sub>*. Text are tokenized with WordPiece Model of 30,000 token vocabulary. Input can represent a single text segment or a pair of text segments with a special token [SEP] in between. Learnable segment embeddings are added to indicate segment A or segment B. Final input representation for each token is the sum of token embedding, segment embedding, and positional embedding. The first token of every input sequence is always a special classification token [CLS], whose corresponding hidden vector in the output of the final layer is used as the aggregate representation for the output of classification tasks.

The pre-training datasets include BooksCorpus (800M words) and English Wikipedia (2,500M words). Two unsupervised tasks are included in the pre-training: Masked Language Modeling (MLM, also known as Auto-Encoding) and Next Sentence Prediction (NSP). In the MLM, 15% of all tokens in each input sequence are masked at random for prediction by softmax of the corresponding final hidden vectors over the vocabulary. The NSP is to pre-train the model to predict sentence relationships as binary classification (B IsNext/NotNext of A) for paired sentence inputs (A, B). For each fine-tuning task, just one additional classification layer is added and task-specific inputs and outputs are fed into BERT and all the weights are updated end-to-end.

*BERT<sub>LARGE</sub>* achieved new state-of-the-art results on eleven natural language processing datasets, including the GLUE score of 80.5 (7.7 point absolute improvement), MultiNLI accuracy of 86.7 (4.6 point absolute improvement), SQuAD v1.1 question answering Test F1 of 93.2 (1.5 point absolute improvement), SQuAD v2.0 Test F1 of 83.1 (5.1 point absolute improvement), and SWAG sentence-pair completion accuracy of 86.3 (8.3 point absolute improvement).

*BERT<sub>BASE</sub>* has the same model size as GPT, but gained 4.5 point improvement on GLUE score. Abalation studies show that both the bidirectionality of the MLM and the NSP task of the pre-training are significant contributors to GLUE score. In addition to fine-tuning pre-trained large model, transfer learning can also be done with feature-based approach, where contextual embeddings of tokens are extracted from the pre-trained large model to represent inputs for training small models of downstream tasks. In a Named Entity Recognition task, concatenation of the token representations from the top four hidden layers of the pre-trained *BERT<sub>BASE</sub>* achieved comparable performance to the fine-tuned *BERT<sub>BASE</sub>*.

#### **RoBERTa**

Liu et al., 2019<sup>[\[14\]](#ref14)</sup> investigated the effect of larger training dataset size and alternative training hyperparameters of the BERT, without any change to the model architecture. The new training configuration of the BERT is called **R**obustly **o**ptimized **BERT** **a**pproach (**RoBERTa**). The new training dataset is collected from more diverse sources, with total size of 160GB, 10 times of the size used by BERT. The new batch size is 8K, more than 30 times of the batch size of BERT. The training steps are 0.5M and 1M for RoBERTa and BERT, respectively; thus, total instances of training are 4B and 256M for RoBERTa and BERT, respectively. Larger training dataset, larger batch size, and longer training have been shown to be substantially beneficial; but other hyperparameter changes included in the new configuration have been shown to have little benefit, including larger BPE vocabulary size, dynamic token masking, training on longer sequence, and removal of NSP pre-training task.

RoBERTa achieved new state-of-the-art GLUE score of 88.5, far above the 80.5 by *BERT<sub>LARGE</sub>*. It also set new state-of-the-art results on two question answering tasks, RACE and SQuAD, with large margin over corresponding scores by *BERT<sub>LARGE</sub>*. These results showed that the original BERT model was significantly underfit.

### **Permutation Language Models**

#### **XLNet**

The standard language modeling in GPT cannot capture relationship to right-hand side tokens, which may be required by downstream tasks. On the other hand, the masked language modeling in BERT cannot capture relationship between masked tokens in the same input sequence during pre-training and does not have the artificial token [MASK] in the input data of fine-tuning tasks. To overcome the drawbacks in both approaches, Yang et al., 2019<sup>[\[16\]](#ref16)</sup> introduced XLNet, a permutation language modeling method with two-stream self-attention architecture. The objective functions of the three types of language modeling are compared below. With all possible permutations of a token sequence, the relationship to right-hand side tokens in the original sequence can be learned by autoregressive approach. The original sequence order is preserved by positional encodings.

There are two sets of hidden representations used in this model, dependent on whether the content $$x_{z_{t}}$$ at the position $$z_{t}$$ is used or not. The content representation $$h_{z_{t}}=h_{\theta}(\mathrm{\mathbf{x}}_{\leq t})$$ encodes both the left context and $$x_{z_{t}}$$ itself, similar to the standard hidden states in Transformer. The query representation $$g_{z_{t}}=g_{\theta}(\mathrm{\mathbf{x}}_{\lt t}, z_{t})$$ encodes the left context and the position $$z_{t}$$, but not $$x_{z_{t}}$$. The next token distribution $$\mathit{p}_{\theta}(x_{z_{t}} |\mathrm{\mathbf{x}}_{\mathrm{\mathbf{z}}_{\lt t}})=\frac{exp(e(x_{z_{t}})^{\top}g_{z_{t}})}{\sum_{x'} exp(e(x')^{\top}g_{z_{t}})}$$, where $$e(x)$$ is the embedding of $$x$$. For each attention layer $$m=1,..., M$$, the two streams of representations are schematically updated with a shared set of parameters as follows: query stream $$g_{z_{t}}^{(m)}\leftarrow Attention(Q=g_{z_{t}}^{(m-1)}, KV=h_{\mathrm{\mathbf{z}}_{\lt t}}^{(m-1)};\theta)$$, content stream $$h_{z_{t}}^{(m)}\leftarrow Attention(Q=h_{z_{t}}^{(m-1)}, KV=h_{\mathrm{\mathbf{z}}_{\leq t}}^{(m-1)};\theta)$$, as illustrated in the figure below.
<p align="center"><img src="../../../assets/images/xlnet_architecture.png"></p>

To reduce time to convergence, only the right-most tokens ($$t\gt c$$) of a permutation $$\mathrm{\mathbf{z}}$$ are used as targets for prediction. A hyperparameter $$K\approx \left\lvert{\mathrm{\mathbf{z}}}\right\rvert/(\left\lvert{\mathrm{\mathbf{z}}}\right\rvert-c)$$ is used to determine the target subsequence. For non-target tokens, their query representations do not need to be computed. The $$K=6$$ is used in the experiments of this study.

To enable capturing longer-term dependency, two techniques from Transformer-XL<sup>[\[17\]](#ref17)</sup> are integrated: the relative positional encoding scheme and the segment recurrence mechanism. The relative positional encoding is done by relative distance between two positions, which is required for the segment recurrence mechanism. The input is divided into multiple equal length segments. During training, the hidden state sequence computed for the previous segment is fixed (stop-gradient) and cached to be reused (concatenated with the next segment's hidden state) as an extended context when the model processes the next new segment. This segment-level recurrence avoids context fragmentation problem and speeds up training and evaluation. Unlike the same-layer recurrence in RNN, the segment recurrence here feeds to the next layer. Therefore, the largest possible dependency length grows with the number of layers as well as the segment length.

The XLNet has the same architecture hyperparameters as BERT<sub>Large</sub>. Trained on the same datasets and hyperparameters with an almost identical training recipe, XLNet outperforms BERT<sub>Large</sub> by a considerable margin on all the tested tasks, including GLUE, text classification, reading comprehension (RACE), document ranking (ClueWeb09-B), and question answering (SQuAD) tasks. Trained on the same full data and the hyperparameters of RoBERTa, XLNet generally outperforms RoBERTa on RACE, ClueWeb09-B, SQuAD, and GLUE tasks.

### **Denoising Language Models**

All the models mentioned above use either encoder-only or decoder-only Transformers, which limit the applicable mappings from inputs to outputs. By contrast, using sequence-to-sequence models on encoder-decoder Transformers allows arbitrary mappings from noisy to clean sequences, such as deletion, infilling, rotation, and permutation. Denoising language models are trained by corrupting documents and then optimizing a reconstruction loss, the cross-entropy between the decoder’s output and the original document.

#### **T5**

Raffel et al., 2020<sup>[\[18\]](#ref18)</sup> introduce **T**ext-**t**o-**T**ext **T**ransfer **T**ransformer (T5) model that uses encoder-decoder architecture of the transformer and unifies the input and output format of all the downstream tasks, as illustrated below, so that multi-task learning can be done easily for all tasks at once. They also systematically compare different architectures, unsupervised objectives, pre-training dataset sizes, task training strategies, and scaling to gain insight on optimal modeling choices.
<p align="center"><img src="../../../assets/images/t5_architecture.png"></p>

The T5 encoder-decoder architecture is largely the same as the transformer, with the exception of removing the bias term in layer normalization, placing the layer normalization outside of residual path, and using a simplified form of positional embedding. The pre-training dataset is named **C**olossal **C**lean **C**rawled **C**orpus (C4) that is 750GB of web extracted text, derived from one month of Common Crawl dataset with a series of cleaning, filtering, and deduplication. Every task is treated as a text-to-text problem where some text for context or conditioning is fed into the encoder and some output text is generated by the decoder. The text-to-text framework provides consistent model, objective, training procedure, and decoding process, regardless of the task. A task-specific prefix (as in the figure above) is added to the original input sequence to indicate which task the model should perform. There are 18 tasks in this study.

The T5 baseline model's encoder and decoder are each similar in size and configuration to the BERT<sub>BASE</sub> that consists of 12 layers, 12 heads per layer, *d<sub>model</sub>=768*, *d<sub>KV</sub>=64*, *d<sub>ff</sub>=3072*, and about 220 million parameters (twice the number of parameters of BERT<sub>BASE</sub>). The pre-training uses a vocabulary of 32,000 wordpieces. The pre-training objective (named denoising objective in this study) is to predict dropped-out tokens in the input sequence. 15% of tokens are randomly dropped out and replaced by special sentinel tokens. The target is the combination of all the dropped-out spans of tokens, delimited by the corresponding sentinel tokens. Training always uses standard maximum likelihood and a cross-entropy loss; and testing uses greedy decoding. Pre-training runs for $$2^{19}$$ steps on C4, with batch size of 128 and maximum length of 512, which results in pre-training on $$2^{35}\approx 34B$$ tokens, a fraction of the C4 dataset. Fine-tuning runs for $$2^{18}$$ steps on all tasks and validation is done every 5000 steps. Results are reported based on highest validation performance per task. Overall, the performance of T5 baseline model is comparable to existing models of similar size, such as BERT<sub>BASE</sub>.

Five architectural variants are compared: (1) the T5 baseline encoder-decoder, (2) equivalent encoder-decoder but with shared parameters, (3) encoder-decoder but with only 6 layers each, (4) decoder-only language model, (5) decoder-only prefix language model. The prefix language model uses fully-visible masking on the prefix portion of the sequence and causal masking for target portion. For example, during training for input sequence "translate English to German: That is good. target:" and target sequence "Das ist gut.", fully-visible masking is applied to the former and causal masking is applied to the latter. Also, for each of the five architectural variants, two objectives are compared: (1) the baseline model's denoising objective and (2) standard language model objective. In the latter case, the input and target are concatenated and the entire span is predicted from beginning to end. The results show that models using a denoising objective always perform better than the corresponding ones using a language model objective. For all tasks, the T5 baseline encoder-decoder with the denoising objective performed the best, and sharing parameters across encoder and decoder performed nearly as well. The two decoder-only variants performed significantly worse than encoder-decoder variants, suggesting that the addition of an explicit encoder-decoder attention is beneficial.

Using the encoder-decoder architecture, three disparate objectives are compared: (1) BERT-style Masked LM (10% of masked tokens replaced by random tokens), (2) Prefix LM, and (3) Deshuffling (input sequence is shuffled and deshuffled original sequence is used as target). The BERT-style objective performs best. Three additional variants of BERT-style objectives are further compared: (1) MASS-style (all masked tokens replaced by \<MASK\>), (2) consecutive masked tokens (span) replaced with single sentinel token (used by T5 baseline above), and (3) the masked tokens are dropped without replacements. The targets of both BERT-style and MASS-style are the entire original text; the targets of the latter two are the masked tokens only. All the three variants perform similarly to BERT-style objective. The span replacement variant is chosen for the rest of study, due to shorter target length and less processing. The token masking rates of 10%, 15%, 25%, and 50% are compared; but they had limited effect on model performance. 15% is used for the rest of the study. The average span lengths of 2, 3, 5, 10 are compared with the baseline model's random masking approach. Overall, the differences are limited and the baseline model's objective is chosen for the rest of study, due to shorter target length and less processing.

The cleaning, filtering, and deduplication steps in producing C4 dataset reduced size from 6.1TB to 745GB and improved performance of downstream tasks uniformly. Some domain-specific subsets of C4 or other corpora much smaller than C4 outperformed C4 in some downstream tasks, when the smaller pre-training datasets contain in-domain data of the tasks. When C4 is artificially truncated to various smaller sizes, corresponding to repeating data during pre-training for 64, 256, 1,024, and 4,096 times, the performance degrades as the dataset size shrinks, due to overfitting. Therefore, the authors suggest using large pre-training datasets whenever possible.

In the baseline model, the fine-tuning stage updates all parameters. Two partial parameter update strategies are compared: (1) adding an adapter layer (dense-ReLU-dense) after each feed-forward network in each block of the transformer and only updating parameters in the adapter layers and layer normalization, (2) gradual unfreezing that unfreezes layers for parameter update from the last layer to the first layer gradually. Both of the partial update strategies caused performance degradation. Also, multi-task training is examined, where both unlabeled dataset and supervised downstream task dataset are mixed in a single stage training. Three different data mixing strategies are compared: (1) examples-proportional mixing (sampling in proportion to the size of each task's dataset, sampling rate of the *mth* task $$r_{m}=\min(e_{m},K)/\sum \min(e_{n},K)$$, where *e* is the number of examples and *K* is an artificial dataset size limit), (2) temperature-scaled mixing (sampling rate of the *mth* task = $$r_{m}^{\frac{1}{T}}/\sum r_{n}^{\frac{1}{T}}$$, where $$r_{m}$$ is the same as in the (1)), and (3) equal mixing (sampling from each task with equal probability). In general, multi-tasking training underperforms pre-training followed by fine-tuning on most tasks. To close the gap between multi-tasking training and pre-training followed by fine-tuning, three different strategies of multi-task pre-training followed by fine-tuning are compared: (1) examples-proportional multi-task pre-training (with $$K=2^{19}$$) followed by task-specific fine-tuning, (2) same multi-task pre-training as in (1) except that one downstream task is excluded in the pre-training but used for fine-tuning (leave-one-out multi-task training), and (3) same multi-task pre-training as in (1) except that unsupervised task is excluded (supervised multi-task pre-training). The (1) strategy, multi-task pre-training followed by fine-tuning, results in comparable performance to the T5 baseline, but the (2) and (3) strategies perform slightly and significantly, respectively, worse than the baseline.

Finally, different ways of scaling up the baseline model are compared: (1) $$4\times$$ training steps, (2) $$2\times$$ training steps and $$2\times$$ bigger (by number of parameters), (3) $$4\times$$ bigger, (4) $$4\times$$ larger batch sizes, (5) ensemble of 4 separately pre-trained and fine-tuned models, and (6) single pre-trained model and 4 separately fine-tuned models for ensemble. All the 6 ways of scaling up improved performance over the baseline on all the tasks, except the 2 ensemble methods on SuperGLUE task. There was no clear winner between training for $$4\times$$ as many steps or using a $$4\times$$ larger batch size. The $$4\times$$ bigger model appeared to slightly outperform the $$4\times$$ training steps model. However, using a larger model can make downstream fine-tuning and inference more expensive.

Combining the insights above, the authors put together their best model named T5-11B with the configuration: 24-layer encoder-decoder, 128 heads per layer, *d<sub>model</sub>=1,024*, *d<sub>KV</sub>=128*, *d<sub>ff</sub>=65,536*, and about 11 billion parameters. The span-corruption objective is used with 15% corruption rate and mean span length of 3. The pre-training is run for 1 million steps with a batch size of $$2^{11}$$ sequences of length 512, corresponding to a total of about 1 trillion pre-training tokens. The pre-training is run on a multi-task examples-proportional mixing. During fine-tuning, a smaller batch size of 8 and length of 512 are used. Overall, T5-11B achieved state-of-the-art performance on 18 out of the 24 tasks.

#### **BART**

Lewis et al., 2020<sup>[\[19\]](#ref19)</sup> introduce **B**idirectional and **A**uto-**R**egressive  **T**ransformers (BART) model that is a denoising autoencoder built with the encoder-decoder architecture of the transformer. The bidirectional encoder takes in a corrupted text and the left-to-right autoregressive decoder generates the corresponding clean text. The pre-training optimizes the negative log likelihood of the original document. BART-base and BART-large use 6 and 12 layers, respectively, in each of the encoder and decoder. BART differs from BERT in two ways: (1) each layer of the decoder performs additional cross-attention over the final hidden layer of the encoder; and (2) BART does not have an additional feed-forward network before word prediction. BART contains roughly 10% more parameters than the equivalently sized BERT model.

BART allows any type of document corruption. Five types of noising approaches, as illustrated below, are experimented: (1) token masking, where random tokens are sampled and replaced with [MASK] elements; (2) token deletion, where random tokens are deleted from the input and the model must decide which positions are missing inputs; (3) text infilling, where a number of text spans are sampled with length drawn from a Poisson distribution and each span is replaced with a single [MASK] token; (4) sentence permutation, where sentences are shuffled in a random order; (5) document rotation, where a token is chosen uniformly at random and the document is rotated so that it begins with that token.
<p align="center"><img src="../../../assets/images/denoising.png"></p>
The sentence permutation and document rotation approaches perform poorly. Token masking and token deletion perform next to the best, with token deletion better than token masking on generation tasks. The text infilling approach shows the most consistenly strong performance.

BART can be fine-tuned for 4 types of downstream tasks: (1) sequence classification (Figure (a) below), where the final hidden state of the final decoder token is fed into a multi-class linear classifier; (2) token classification, where the top hidden state of the decoder is used as a representation for each word and the representation is used to classify the token; (3) sequence generation, such as abstractive question answering and summarization, where the decoder generates output autoregressively; (4) machine translation, where BART's encoder embedding layer is replaced with a new randomly initialized encoder (Figure (b) below) that translates a foreign language to noised target language that in turn serves as inputs to the entire BART as a single pre-trained decoder.
<p align="center"><img src="../../../assets/images/BART_finetuning.png"></p>

For large-scale pre-training experiments, BART-large model with 12 layers, hidden size of 1024, and batch size of 8000 is trained for 500K steps. Documents are tokenized with the same byte-pair encoding as GPT-2. Document noising is done with a combination of text infilling and sentence permutation. The pre-training data consist of 160GB of news, books, stories, and web text.

On classification tasks, SQuAD and GLUE tasks, BART performs similarly to RoBERTa and XLNet. On two standard summarization tasks, CNN/DailyMail and XSum, BART outperforms all previous work, but does not reach human performance on XSum. For dialogue response generation on CONVAI2, with response conditioned on both the previous context and a textually-specified persona, BART outperforms previous work. For abstractive QA task on the EL15 dataset that expects long free-form answers, BART outperforms the best previous work by 1.2 ROUGE-L. For translation using WMT16 Romanian-English dataset, BART outperforms Transformer only if it is pre-trained in English.

BART and T5 use slightly different training objective for masked spans in the inputs: BART reconstructs the complete input, but T5 only predicts the sequence of corrupted tokens. This may give BART some advantage on text generation task. BART achieves higher performance than T5 with similar model sizes, particularly on summarization tasks.

## **Codes**

- [Transformers](https://github.com/huggingface/transformers) or [Transformers](https://github.com/tensorflow/tensor2tensor)
- [minGPT](https://github.com/karpathy/minGPT)
- [GPT-2](https://github.com/openai/gpt-2)
- [GPT-3](https://github.com/openai/gpt-3)
- [BERT](https://github.com/google-research/bert)
- [RoBERTa](https://github.com/pytorch/fairseq)
- [XLNet](https://github.com/zihangdai/xlnet)
- [Transformer-XL](https://github.com/kimiyoung/transformer-xl)
- [T5](https://github.com/google-research/text-to-text-transfer-transformer)
- [BART](https://github.com/pytorch/fairseq/tree/master/examples/bart) or [BART](https://huggingface.co/transformers/model_doc/bart.html)

## **References**

<a name="ref1">[1]</a> Bahdanau, D., Cho, K., and Bengio, Y. (2014) [Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473.pdf). arXiv:1409.0473.

<a name="ref2">[2]</a> Chorowski, J., Bahdanau, D., Cho, K., and Bengio, Y. (2014) [End-to-end continuous speech recognition using attention-based recurrent NN: First results](https://arxiv.org/pdf/1412.1602.pdf). CoRR, vol. abs/1412.1602.

<a name="ref3">[3]</a> Cheng, J., Dong, L., and Lapata, M. (2016) [Long short-term memory-networks for machine reading](https://arxiv.org/pdf/1601.06733.pdf). In: Proc. EMNLP, 551–561.

<a name="ref4">[4]</a> Parikh, A., Täckström, D., Das, D., and Uszkoreit, J. (2016) [A decomposable attention model for natural language inference](https://arxiv.org/pdf/1606.01933.pdf). In: Proc. EMNLP, 2249–2255.

<a name="ref5">[5]</a> Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., and Polosukhin, I. (2017) [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf). In: Advances in Neural Information Processing Systems, 6000–6010.

<a name="ref6">[6]</a> Sennrich, R., Haddow, B., Birch, A. (2016) [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909v5.pdf). arXiv preprint arXiv:1508.07909

<a name="ref7">[7]</a> Wu, Y., Schuster, M., Chen, Z., Le, Q., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., et al. (2016) [Google’s neural machine
translation system: Bridging the gap between human and machine translation](https://arxiv.org/pdf/1609.08144.pdf). arXiv preprint arXiv:1609.08144

<a name="ref8">[8]</a> Radford, A., Narasimhan, K., Salimans, T., Sutskever, I. (2018) [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).

<a name="ref9">[9]</a> Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2018) [GLUE: A multi-task benchmark and analysis platform for natural language understanding](https://arxiv.org/pdf/1804.07461.pdf). arXiv preprint arXiv:1804.07461

<a name="ref10">[10]</a> Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I. (2019) [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf).

<a name="ref11">[11]</a> Brown, T. B., et al. (2020) [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf). arXiv preprint arXiv:2005.14165

<a name="ref12">[12]</a> Child, R., Gray, S., Radford, A., and Sutskever, I. (2019) [Generating long sequences with sparse transformers](https://arxiv.org/pdf/1904.10509.pdf). arXiv preprint arXiv:1904.10509

<a name="ref13">[13]</a> Devlin, J., Chang, M., Lee, K., Toutanova, K. (2019) [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf?source=post_elevate_sequence_page---------------------------). arXiv preprint arXiv:1810.04805v2

<a name="ref14">[14]</a> Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., et al. (2019) [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692v1.pdf). arXiv preprint arXiv:1907.11692

<a name="ref16">[16]</a> Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., and Le, Q. (2019) [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf). arXiv preprint arXiv:1906.08237

<a name="ref17">[17]</a> Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q., and Salakhutdinov, R. (2019) [Transformer-XL: Attentive language models beyond a fixed-length context](https://arxiv.org/abs/1901.02860). arXiv preprint arXiv:1901.02860

<a name="ref18">[18]</a> Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. (2020) [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf). Journal of Machine Learning Research 21:1-67

<a name="ref19">[19]</a> Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O. (2020) [BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension](https://www.aclweb.org/anthology/2020.acl-main.703.pdf). In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 7871–7880.
