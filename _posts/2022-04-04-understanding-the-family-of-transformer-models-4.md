---
layout: post
title: "Understanding the Family of Transformer Models. Part IV - Memory"
date: 2022-04-04
---
Memory is the recordings of experiences, knowledge, or skills acquired passively or learned actively. In human neural network, memory consolidation, storage, and retrieval involve different brain regions, dependent upon the type of learning and modality of stimuli.<sup>[\[1\]](#ref1)[\[2\]](#ref2)</sup> In von Neumann computer architecture, the necessity to have a memory organ, separated from central processing organs, to carry out long and complicated sequences of operations has been considered at the onset.<sup>[\[3\]](#ref3)</sup> On the contrary, in artificial neural networks, especially Transformer-based architectures, model parameters play a mixed roles of both memory and compute (e.g. [\[4\]](#ref4)) and scaling up memory is accompanied with increasingly prohibitive computing cost in either pre-training-centric<sup>[\[5\]](#ref5)</sup> or fine-tuning-centric<sup>[\[6\]](#ref6)[\[7\]](#ref7)</sup> approaches. To reduce computing cost while maintaining memory scale, strategies of activating only a subset of model parameters for each input example have been developed to achieve comparable or better performance on mixture-of-experts architecture.<sup>[\[8\]](#ref8)[\[9\]](#ref9)[\[10\]](#ref10)</sup>

Before the advent of the Transformer for large-scale language modeling, explicit memory modules have been incorporated into neural networks for various purposes. Long Short-Term Memory (LSTM)<sup>[\[25\]](#ref25)</sup> recurrent neural network (RNN) uses memory cells and gate units to overcome the vanishing error problem in backpropagation through extended time steps. **Neural Turing Machine** (NTM)<sup>[\[26\]](#ref26)</sup> takes inspiration from biological working memory and digital computer design to extend neural network, feedforward or LSTM, with an external memory matrix that has multi-head attentional reading/writing operations. **Memory Networks** (MN)<sup>[\[27\]](#ref27)</sup> attempt to rectify RNN's difficulty in performing memorization, by combining a memory (an array of objects, e.g., vectors or strings) and four components: I (maps input to feature space), G (updates, compresses, and generalizes memories), O (produces output by selecting top matching memories for the input), and R (can be an RNN that maps output to response). **End-To-End Memory Networks** (MemNN)<sup>[\[28\]](#ref28)</sup> improves MN by replacing the hard max operations within each layer of MN with a continuous (attention-like) weighting from the softmax so that the MemNN can be trained end-to-end from input-output pairs. **Differentiable Neural Computer** (DNC)<sup>[\[11\]](#ref11)</sup> tries to bring the benefits of an addressable memory of a computer to neural networks by providing a LSTM with read-write access to an external memory matrix, which differs from NTM and MN by using differentiable attention mechanisms for memory access. **Sparse Access Memory** (SAM)<sup>[\[29\]](#ref29)</sup> tries to resolve the difficulty in training NTM and DNC when memory size is scaled up, by using sparse content-based read and write operations. **Dynamic Neural Turing Machine** (D-NTM)<sup>[\[30\]](#ref30)</sup> addresses the limitation of fixed distance between consecutive memory cells in the location-based addressing strategy of NTM, by introducing a learnable address vector for each memory cell of the NTM with least recently used memory addressing mechanism. It showed that discrete, non-differentiable attention mechanism for memory addressing can outperform continuous, differentiable attention mechanism for episodic QA task. **Temporal Automatic Relation Discovery in Sequences** (TARDIS) tests the hypothesis that memory augmented RNNs can reduce the effects of vanishing gradients by creating shortcut (or wormhole) connections through time to the past to propagate the gradients more effectively. The memory structure of TARDIS is similar to NTM and D-NTM, but its memory read and write operations use discrete addressing. The results show that the wormhole connections can significantly reduce the effects of the vanishing gradients by shortening the paths that the signal needs to travel between the dependencies.

Since the advent of Transformer, various forms of memories have been developed to extend Transformer-based models for lower computational cost and/or better performance on tasks involving long temporal dimension. This article surveys recent studies on Transformer-based architecture for memory-augmented language modeling and memory replay for lifelong language learning.

- [Memory-Augmented Language Modeling](#memory-augmented-language-modeling)
    - [External Key-Value Datastore as Memory](#external-key-value-datastore-as-memory)
        - [kNN-LM](#knn-lm)
        - [SPALM](#spalm)
        - [Memorizing Transformer](#memorizing-transformer)
    - [Special Network Layer as Memory](#special-network-layer-as-memory)
        - [PKM](#pkm)
        - [Memformer](#memformer)
        - [Infinite Memory Transformer](#infinite-memory-transformer)
        - [MemSizer](#memsizer)
    - [Special Input Tokens as Memory](#special-input-tokens-as-memory)
        - [MemTransformer](#memtransformer)
- [Memory Replay for Lifelong Language Learning](#memory-replay-for-lifelong-language-learning)
    - [Episodic Memory Model](#episodic-memory-model)
- [Codes](#codes)
- [References](#references)

## **Memory-Augmented Language Modeling**

Understanding a long document with linguistic relations between temporally distant parts has been [a challenge for Transformer-based language models, where input lengths are limited](https://msfchen.github.io/2020/11/30/understanding-the-family-of-transformer-models-2.html). An approach to address the challenge is to extend the attention span beyond input segments by introducing memory modules for past context, as in [Transfomer-XL and Compressive Transformer](https://msfchen.github.io/2020/11/30/understanding-the-family-of-transformer-models-2.html#extending-attention-span-by-segment-recurrence), where the "memory" represents stored hidden states of past tokens or segments. Transfomer-XL and Compressive Transformer have become standard baseline models in recent studies on memory-augmented language modeling. Different approaches of incorporating memory modules into Transformer-based language models have been shown to improve perplexity performance and/or reduce computational cost. These recent studies can be divided into three categories: using external key-value datastore as memory, using special network layer as memory, and using special input tokens as memory.

### **External Key-Value Datastore as Memory**

Using external key-value datastore as memory to augment Transformer-based language models bears some resemblance to [retrieval-augmented language models](https://msfchen.github.io/2020/10/26/understanding-the-family-of-transformer-models-1.html#retrieval-augmented-language-models). The external knowledge store used in the latter cannot be referred to as "memory" because (1) they often are not the dataset used for pre-training the language models, but are a dataset for downstream question-answering tasks; (2) they are stored in input embedding space, not in the key space encoded by the learned parameters of the language models being evaluated; (3) they are used more akin to open-book exam, not memory-dependent closed-book exam. Three examples of memory-augmented language models using external key-value datastore are covered here: kNN-LM<sup>[\[13\]](#ref13)</sup>, SPALM<sup>[\[12\]](#ref12)</sup>, and Memorizing Transformer<sup>[\[17\]](#ref17)</sup>. In kNN-LM and SPALM, the external memory stores are built by a single pass of training dataset through the pre-trained LMs and then fixed afterwards. Memory retrieval is done for inference only in kNN-LM, but for both training and inference in SPALM. In Memorizing Transformer, the external memory is built during training, causing a distributional shift in the keys and values in the external memory early in the training. All the three approaches substantially outperform Transformer-XL of comparable size on long-form text modeling measured in perplexity.

#### **kNN-LM**

Khandelwa et al. (2019)<sup>[\[13\]](#ref13)</sup> introduced **$$\mathit{k}$$NN-LM** ($$\mathit{k}$$-nearest neighbors language model) that uses a key-value datastore as memory, where keys are encoded context vectors, values are next tokens, and retrieval is based on $$\mathit{k}$$-nearest neighbors to encoded query vectors, to augment pre-trained, decoder-only Transformer-based language models. The write process of the datastore is not a part of the model training or evaluation. The datastore is static after it is built with a pre-trained LM in a separate process. The read (i.e., retrieval) process is used only at the inference time, as illustrated in the Figure below.
<p align="center"><img src="../../../assets/images/kNN-LM.png"></p>
Given a context sequence of tokens $$c_t=(w_1,...,w_{t-1})$$, an autoregressive LM estimates $$p(w_t|c_t)$$, the distribution over the target token $$w_t$$. Let $$f(\cdot)$$ be the function that maps a context $$c$$ to a fixed-length vector representation computed by the pre-trained LM. Then, given the $$i$$-th training example $$(c_i,w_i)\in\mathcal{D}$$, the key $$k_i$$ is defined as the vector representation of the context $$f(c_i)$$ and the value $$v_i$$ is the target word $$w_i$$. The datastore $$(\mathcal{K},\mathcal{V})$$ is the set of all key-value pairs constructed from all the training examples in $$\mathcal{D}$$: $$(\mathcal{K},\mathcal{V})=\{(f(c_i),w_i)|(c_i,w_i)\in\mathcal{D}\}$$. This can be done with a single forward pass over the training set of the LM, where the representations learned by the LM remain unchanged. At test time, for an input context $$x$$, the LM generates (1) the output distribution over next words $$p_{LM}(y|x)$$ and (2) the context representation $$f(x)$$. The model queries the datastore with $$f(x)$$ to retrieve its $$k$$-nearest neighbors $$\mathcal{N}$$ according to squared $$L^2$$ (Euclidean) distance function $$d(k_i,f(x))$$. Then, it computes a distribution over neighbors based on a softmax of their negative distances, while aggregating probability mass for each vocabulary item across all its occurrences in the retrieved targets (items that do not appear in the retrieved targets have zero probability): $$p_{kNN}(y|x)\propto\sum\limits_{(k_i,v_i)\in\mathcal{N}}\mathrm{\mathbb{1}}_{y=v_i}\exp(-d(k_i,f(x)))$$. The final $$\mathit{k}$$NN-LM distribution $$p(y|x)$$ is a linear interpolation between the nearest neighbor distribution $$p_{kNN}$$ and the model distribution $$p_{LM}$$ using a tuned parameter $$\lambda$$: $$p(y|x)=\lambda p_{kNN}(y|x)+(1-\lambda)p_{LM}(y|x)$$.

The datastore contains one entry per target in the training set, which can be up to billions of examples. To speed up search, [FAISS library](https://github.com/facebookresearch/faiss) is used, which clusters the keys, looks up neighbors based on the cluster centroids, and stores compressed versions of the vectors. In this study, $$L^2$$ distance outperforms inner product distance for FAISS retrieval for $$\mathit{k}$$NN-LM.

Four corpora are used: Wiki-3B, WikiText-103, Wiki-100M from Wikipedia and Toronto Books Corpus, containing 2.87B, 103M, 100M, and 0.7B tokens, respectively. The average number of tokens per Wikipedia article is about 3,625. The average number of words per book is 89,223 in the Book Corpus. A decoder-only Transformers with 16 layers, each with 16 self-attention heads, is used for LM. The context length for (WikiText-103, other three corpora) are (3,072, 1,024), (2,560, 512), and ($$\geqslant$$1,536,$$\geqslant$$512) for training, evaluation, and key-encoding, respectively. The LM is trained to minimize the negative log-likelihood of the training corpus and evaluated by perplexity on held out data. The keys used for $$\mathit{k}$$NN-LM are the 1024-dimensional representations fed to the feedforward network, after self-attention and layernorm, in the final layer of the Transformer LM. A single forward pass is performed over the training set with the trained model in order to save the keys and values. A FAISS index is created using 1M randomly sampled keys to learn 4,096 cluster centroids. Keys are quantized to 64-bytes for efficiency. During inference, the index looks up 32 cluster centroids and 1,024 nearest neighbors are retrieved. The computational cost to generate the datastore in a single pass over the training set amounts to a fraction of the cost of training for one epoch on the same data.

The $$\mathit{k}$$NN-LM substantially outperforms the base LM, the base LM + Transformer-XL, and the base LM + Continuous Cache on WikiText-103. The Continuous Cache is the technique of saving and retrieving neighbors from earlier examples in the $$\mathit{test}$$ document. Combining $$\mathit{k}$$NN-LM and Continuous Cache outperforms $$\mathit{k}$$NN-LM, indicating that the two approaches are complementary. On BookCorpus, the $$\mathit{k}$$NN-LM also outperforms the base LM, indicating that the approach works for multiple domains.

Combining the LM trained with Wiki-100M and the datastore built with Wiki-3B significantly outperforms LM trained with Wiki-100M or Wiki-3B without a datastore, suggesting that rather than training LMs on ever larger datasets, a better performing model can be built with a smaller training dataset and augmented with a $$\mathit{k}$$NN datastore built with a larger corpus. The performance of $$\mathit{k}$$NN-LM improves as the amount of the data used for the datastore increases. The tuned values of $$\lambda$$ also increases as the datastore size increases, indicating that the model relies more on the kNN component as the size of the datastore increases.

Domain adaptation experiments compare in-domain training (LM trained and evaluated on BooksCorpus), out-of-domain training (LM trained on Wiki-3B and evaluated on BooksCorpus), and out-of-domain $$\mathit{k}$$NN-LM (datastore on BooksCorpus + LM trained on Wiki-3B and evaluated on BooksCorpus). The out-of-domain $$\mathit{k}$$NN-LM substantially outperforms out-of-domain LM, demonstrating
that $$\mathit{k}$$NN-LM allows a single model to be useful in multiple domains, by simply adding a datastore per domain, although the improvement does not reach the level of in-domain LM.

Four hyperparameters are introduced for nearest neighbor search. (1) Key Function. Using the input to the final layer's feedforward network achieved the largest improvement. Also, normalized representations taken immediately after the layer norm perform better. (2) Number of Neighbors per Query. Each query returns the top-$$k$$ neighbors. The performance monotonically improves as the value of $$k$$ increases. (3) Interpolation Parameter. The optimal $$\lambda$$ values are 0.25 and 0.65 for BookCopus LM + BookCorpus Datastore and Wiki-3B LM + BookCorpus Datastore, respectively. (4) Precision of Similarity Function. In FAISS, the nearest neighbor search computes $$L^2$$ distances with quantized keys. The perplexity performance was improved by computing squared $$L^2$$ distances with full precision keys (e.g., improved from 16.5 to 16.06 on WikiText-103).

Manually examining cases in which $$p_{kNN}$$ was significantly better than $$p_{LM}$$ reveals that examples where $$\mathit{k}$$NN-LM is most helpful typically contain rare patterns, such as factual knowledge, names, and near-duplicate sentences from the training set. When $$p_{kNN}$$ is significantly better than $$p_{LM}$$ for a context-target pair, it means that assigning similar representations to train and test instances appears to be an easier problem than implicitly memorizing the next word in model parameters. Training a vanilla Transformer without dropout will eventually result in zero training loss, meaning the model memorizes all training examples, and low generalization. Interpolating such memorizing LM with a normal LM (i.e., trained with dropout) barely improves the perplexity. In contrast, $$\mathit{k}$$NN-LM memorizes all training data while improving generalization. These results might suggest that autoregressive LM weights immediate prior context more than distant past context for predicting the next word, while $$\mathit{k}$$NN similarity score between train and test contexts weights the entire range of context evenly.

This work offers an alternative method for scaling language models, in which relatively small models learn context representations, and a nearest neighbor search acts as a highly expressive classifier.

#### **SPALM**

Yogatama et al. (2021)<sup>[\[12\]](#ref12)</sup> introduced SPALM (**S**emi**PA**rametric **L**anguage **M**odel) that adaptively combines short-term memory and long-term memory with a Transformer to make predictions. SPALM consists of three main components: (i) a large parametric neural network in the form of a transformer to process local context, (ii) a short-term memory to store extended context, and (iii) a non-parametric episodic memory module that stores information from long-term context. These components are integrated in a single architecture with a gating mechanism, as illustrated in the Figure below.
<p align="center"><img src="../../../assets/images/SPALM.png"></p>
Transformer has limited input sequence length. Instead of considering all previous tokens $$x_{\leq t}$$ of a long document, transformer truncates the input to be the most recent $$N$$ tokens $$\tilde x_{\leq t}=\{x_{t-N+1},...,x_t\}$$ and only operates on this fixed-length window. The extended context of Transformer-XL is used as the short-term memory in this study. Given the current context $$\tilde x_{<t}$$, the extended context of length $$M$$ is denoted as $$\tilde x_{\leq t-N}=\{x_{t-N-M+1},...,x_{t-N}\}$$. The hidden states for $$\tilde x_{\leq t-N}$$ are cached and then used as additional states that can be attended to during the forward pass when computing hidden states for the current context $$\tilde x_{\leq t}$$, but the values of the states are not updated during the backward pass. Denote $$\mathrm{h}_t^r$$, $$\mathrm{H}^r=[\mathrm{h}_{t-N+1}^r,...,\mathrm{h}_t^r]$$, and $$\mathrm{E}^r=[\mathrm{SG}(\mathrm{h}_{t-N-M+1}^r),...,\mathrm{SG}(\mathrm{h}_{t-N}^r)]$$ as hidden states for token $$x_t$$ at layer $$r$$, the current (truncated) context $$\tilde x_{\leq t}$$, and the extended context $$\tilde x_{\leq t-N}$$, respectively, where $$\mathrm{SG}$$ is the stop gradient function. $$\mathrm{E}^r$$ and $$\mathrm{H}^r$$ are concatenated along the length dimension and fed to attention function where each vector is transformed into a query, key, value triplet which are used to produce hidden states, $$\mathrm{H}^{r+1}$$, for the next layer.

The long-term memory module is implemented as a key-value database. The key is a vector representation, denoted as $$\mathrm{d}_i$$, of a context $$\tilde x_{\leq i}$$. Each context is paired with the output token for that context $$x_{i+1}$$, which is stored as the value. A key-value entry is stored for each context-token pair in the training corpus, so the number of entries is equal to the number of tokens in the training corpus. A separate vanilla transformer language model is pretrained and its final-layer hidden state is used for $$\mathrm{d}_i$$. To predict the next token $$x_{t+1}$$ for a given context $$\tilde x_{\leq t}$$, the $$\mathrm{d}_t$$ is first obtained from the separate pretrained language model. The $$\mathrm{d}_t$$ is then used to do a $$k$$-nearest neighbor search on the database to find contexts that are similar to $$\tilde x_{\leq t}$$ in the database. The values associated with the top $$k$$ such contexts are retrieved from the database as candidate output tokens $$y_1,...,y_K$$.

For each $$y_k$$, a vector representation $$\mathrm{y}_k$$ is created by using the same word embedding matrix that is used in the base model. Then, the information from $$y_1,...,y_K$$ are aggregated with a simple attention mechanism using local context $$\mathrm{h}_t^R$$ as attention query: $$\mathrm{m}_t=\sum\limits_{k=1}^K\frac{\exp{\mathrm{y}_k}^{\top}\mathrm{h}_t^R}{\sum_{j=1}^K\exp{\mathrm{y}_j}^{\top}\mathrm{h}_t^R}\mathrm{y}_k$$. Then a context-dependent gate $$\mathrm{g}_t=\sigma({\mathrm{w}_g}^{\top}\mathrm{h}_t^R)$$ that decides how much the model needs to use local information ($$\mathrm{h}_t^R$$) versus long-term information ($$m_t$$) for making the current prediction based on the current context: $$\mathrm{z}_t=(1-\mathrm{g}_t)\odot\mathrm{m}_t+\mathrm{g}_t\odot\mathrm{h}_t^R$$ and $$p(x_{t+1}\vert x_{\leq t})=\mathrm{softmax}(\mathrm{z}_t;\mathrm{W})$$, where $$\mathrm{w}_g$$ is a parameter vector, $$\sigma$$ is the sigmoid function, and $$\mathrm{W}$$ is the word embedding matrix that is shared for input and output word embeddings. Note that the only additional parameter that needs to be trained is $$\mathrm{w}_g$$.

A separate standard transformer language model is first trained and used to generate key ($$\mathrm{d}_i$$) for the database. The key representations are not updated when training the overall model. On the other hand, the value encoder is updated during training since the word embedding matrix is used to represent $$\mathrm{y}_k$$. $$k$$-nearest neighbors search is done using ScaNN method for efficient vector similarity search, which includes search space pruning and quantization for Maximum Inner Product Search. During the evaluation, new tokens from the evaluation data are not stored and the database remains static.

SPALM differs from $$k$$NN-LM on three aspects. (1) $$k$$NN-LM only has long-term memory, but SPALM has long-term and short-term memory. (2) $$k$$NN-LM integrates long-term memory at output level (an ensemble technique) during evaluation time, but SPALM integrates long-term memory at hidden states level during training and evaluation. Integration at hidden states level enables multi-modality integration, which is not possible in output-level integration. (3) $$k$$NN-LM uses a fixed interpolation weight $$\lambda$$ for all tokens, but SPALM uses a context-dependent gate $$\mathrm{g}_t$$ per token.

Word-level language modeling is first evaluated on WikiText-103 for vanilla Transformer, Transformer-XL, $$k$$NN-LM, and SPALM. All models have 18 layers, 512 hidden dimension, 142M total parameters, and 512 input sequence length. For Transformer-XL, the short-term memory length is set to 512 during training and 512 or 3072 during test. The perplexity results show that Transformer-XL substantially outperforms Transformer and $$k$$NN-LM significantly outperforms Transformer-XL, but SPALM and $$k$$NN-LM are not much different (on the development set, SPALM is marginally better; on the test set, $$k$$NN-LM is marginally better). Further improvement over both SPALM and $$k$$NN-LM can be achieved by interpolating SPALM's output probability with the $$p_{kNN}$$ used by $$k$$NN-LM, indicating that incorporating long-term memory into training and interpolating probabilities at test time have some complementary benefits. Word-level language modeling is further evaluated on approximately ten times larger dataset WMT 2019, containing news articles, using the same set of model hyperparameters. The perplexity results show that SPALM significantly outperforms $$k$$NN-LM, Transformer-XL, and Transformer. Unlike WikiText-103, there is no further improvement interpolating the probabilities of SPALM with $$p_{kNN}$$. Also, when the distributions of the dev and test sets can be different (e.g., articles from different months), $$k$$NN-LM that relies on tuning $$\lambda$$ on the dev set has larger performance discrepancy between the dev and test sets. Character-level language modeling is evaluated on enwik8 dataset, using 24-layer model, 512 hidden size, 100M parameters, and sequence length of 768. Transformer-XL short-term memory length is 1536 for training and 4096 for evaluation. The bits-per-character results show that SPALM outperforms all other models. Interpolating the probabilities of SPALM with $$p_{kNN}$$ does not improve performance.

Inspection of neighbor tokens retrieved from the long-term memory for WMT dataset reveals that retrieved neighbors are generally relevant even when they do not match a target word exactly. Retrieved neighbors on enwik8 dataset reveals that information from the long-term memory helps when completing common words, named entities, and corpus-specific formats. SPALM is generally better than both transformer and transformer-XL for predicting (completing) common phrases and named entities that exist in the training set, especially when they are encountered for the first time and have not appeared in the extended context. In cases when Transformer-XL outperforms SPALM, usually the same word has appeared in the extended context, but its probability in SPALM is smoothed by information from the long-term memory.

Distributions of the values of the gates for tokens in WMT and enwik8 show that the values are concentrated around 1 on enwik8, indicating that the model relies on local context most of the time; but the values on WMT are less concentrated around 1, suggesting that the model uses long term memory more than on enwik8. Thus, the gate in SPALM can learn when the long-term memory is needed. When the number of nearest neighbors on the WikiText-103 development set is varied from 1 to 16, the SPAML perplexity is the best at 4 neighbors.

The biggest limitation of SPALM is the necessity to retrieve neighbors for each training token, which results in time consuming training process.

#### **Memorizing Transformer**

Wu et al. (2022)<sup>[\[17\]](#ref17)</sup> introduced Memorizing Transformer that uses an external memory of key-value pairs of previously seen subsequences and a retrieval mechanism of an approximate $$k$$NN attention over the memory. The architecture of the $$k$$NN-augmented transformer is shown in the Figure below. The core model is a vanilla decoder-only transformer. The input text is tokenized, and the tokens are embedded into vector space. The embedding vectors are passed through a series of transformer layers, each of which does dense self-attention, followed by a feed-forward network (FFN). The decoder-only language model uses a causal attention mask and the token embeddings of the last layer are used to predict the next token. Long documents are split into subsequences of 512 tokens, and each subsequence is used as the input for one training step. Subsequences of a long document are fed into the transformer sequentially, from beginning to end. Transformer-XL style cache is also used, which holds the keys and values from the previous training step. When doing self-attention, the cached keys and values are prepended to the current keys and values, and a sliding-window causal mask is used so that each token has a local context that includes the previous 512 tokens.
<p align="center"><img src="../../../assets/images/memorizingT.png"></p>
One of the transformer layers near the top of the stack is a $$k$$NN-augmented attention layer, which combines two forms of attention: standard dense self-attention on the local context, which is the input subsequence for the current training step, and an approximate $$k$$NN attention to search into the external memory. The same queries are used for both the local context and the external memory. After each training step, the (key, value) pairs in the local context are appended to the end of the external memory. If the document is very long, old (key, value) pairs will be dropped from the memory to make room for new ones, in a FIFO queue fashion. Thus, for each head, the external memory keeps a cache of the prior $$M$$ (key, value) pairs, where $$M$$ is the memory size. The $$k$$NN lookup will return a set of retrieved memories, which consist of the top-$$k$$ (key, value) pairs that $$k$$NN search returns for each query (i.e., each token) in the input subsequence. As with standard dense attention, an attention matrix is constructed first by computing the dot product of each query against the retrieved keys, then softmax is applied, and finally a weighted sum of the retrieved values is returned. Unlike standard dense attention, the retrieved memories contain a different set of (key, value) pairs for each query.

The results of $$k$$NN-attention and local attention are then combined using a learned gate: $$g=\sigma(b_g)$$, $$V_a=V_m\odot g+V_c\odot(1-g)$$ where $$\sigma$$ is the sigmoid function, and $$\odot$$ is element-wise multiplication, $$V_a$$ is the combined result of attention, $$V_m$$ is the result of attending to external memory, and $$V_c$$ is the result of attending to the local context. The bias $$b_g$$ is a learned per-head scalar parameter, which allows each head to choose between local and long-range attention. Over time, most heads learned to attend almost exclusively to external memory. For dense attention within the local context, the T5 relative position bias is used. For the retrieved memories, no position bias is used. Multiple long documents of different lengths are packed into a batch, and split into subsequences. Each subsequence in the batch comes from a different document, and thus requires a separate external memory, which is cleared at the start of each new document. The primary difference between Memorizing Transformer and the SPALM above is that the external memory here is gradually filled up during training and continues to be updated during training and testing rather than filled up in a separate process before training and fixed throughout training and testing in SPALM.

During training, as the model moves from early subsequence to later subsequence of a long document, there is a distributional shift in the keys and values that are stored in the external memory. For very large memories, older records may become "stale". To reduce the effects of staleness, keys and queries are normalized using QKNorm that applies $$\mathcal{l}_2$$ normalization to $$Q$$ and $$K$$ only along the head dimension to make each element of $$QK^{\top}$$ the cosine similarity instead of dot product. Normalization does not eliminate staleness, but it ensures that older keys and newer keys do not differ in magnitude. It also helps stabilize training with the Transformer-XL cache. In some experiments, training models from scratch with a large memory sometimes resulted in worse performance than pretraining the model with a small memory of size 8192, and then finetuning it on a larger memory. An approximate $$k$$NN search rather than exact $$k$$NN search is used because it significantly improves the computational speed while achieving a search recall of about 90% of the true top-$$k$$.

Five long-form text datasets are used for evaluation: English language books (PG-19), long web articles (C4), technical math papers (arXiv Math), source code (Github), and formal theorems (Isabelle). PG-19 contains full-length books published before 1919, extracted from the Project Gutenberg archive. C4(4K+), the colossal cleaned common crawl, contains documents scraped from the internet and only includes documents $$\geq 4096$$ tokens. The arXiv Math dataset contains mathematics papers downloaded from arXiv; the number of tokens per paper is roughly comparable to the number of tokens per book in PG19. Github source code files include languages C, C++, Java, Python, Go, and TypeScript. To capture dependencies between files in a repository, one long document is created for each Github repository by traversing the directory tree and concatenating all of the files within it without specific order. The Isabelle corpus consists of formal mathematical proofs of 684 theories, on topics such as foundational logic, advanced analysis, algebra, or cryptography. All files that make up a theory are concatenated together into one long document in the order according to their import dependencies.

A 12-layer decoder-only transformer is used in this study, with or without Transformer-XL cache, with $$d_{embedding}=1024$$, 8 attention heads of $$d=128$$, $$d_{\mathrm{FFN}}=4096$$. The 9th layer is used as the $$k$$NN-augmented ($$k=32$$) attention layer, unless specified otherwise. A sentence-piece tokenizer is used with a vocabulary size of 32K. To compare models with different context lengths, the number of documents in a batch is adjusted so that there are always $$2^{17}$$ tokens in a batch.

Adding external memory to either vanilla Transformer or Transformer-XL improves perplexity across all five datasets by a substantial amount. Increasing the size (up to 65K) of the memory increases the benefit of the memory. For Transformer-XL with context size 2048 and total receptive field of $$2048\times 12\sim25K$$, adding an external memory of size 8192 still results in substantial performance gain, suggesting that $$k$$NN attention on memory is more effective than Transformer-XL's recurrent cache in retrieving information from the distant past. On the other hand, the XL cache provides additional local short-range context at the start of a sequence, which complements the long-range context provided by external memory. Adding a small external memory (size 1536) to a vanilla Transformer performs equally to using local context of size 2048 without external memory, suggesting that lower layers of a Transformer may not need long-range context and having a differentiable memory may not be important. External memory provides a consistent improvement to the model as it is scaled up. The smaller Memorizing Transformer with just 8k tokens in memory can match the perplexity of a larger vanilla Transformer with 5X more trainable parameters.

When using large memories, training was sometimes unstable, possibly due to distributional shift early in the training. Thus, for memories $$\geq 131K$$ tokens, the model is first pretrained with a small memory size of 8192 or 65K for 500K steps, and then finetuned with the larger memory for an additional 20K steps. Increasing the size of external memory during finetuning provided consistent gains up to a size of 262K, which is longer than almost all of the documents in arXiv dataset. A Transformer that is pretrained without external memory can be finetuned with an external memory. A pre-trained one billion parameter vanilla Transformer model fine-tuned with external memory quickly learns to use external memory. Within 20K steps (4% of the pre-training time) the fine-tuned model has already closed 85% of the gap between it and the 1B Memorizing Transformer, and after 100k steps it has closed the gap entirely.

Retrieved memories are studied by finding which tokens showed the biggest improvements in cross-entropy loss when the size of the memory was increased and then examining the top-$$k$$ retrieved memories for those tokens. The model gained the most when looking up rare words, such as proper names, references, citations, and function names, where the first use of a name is too far away from subsequent uses to fit in the local context. The tokens that contribute to a large improvement in perplexity correspond to sparse memory positions and constitute only a small percentage of total memory. The top-$$k$$ retrieved memories for tokens which show the largest improvement in cross-entropy loss show that the model retrieved function and variable names for arXiv math and Github datasets. When predicting the name of a mathematical object or a lemma for the Isabelle corpus, the model looked up the definition from earlier in the proof and found the body of the lemma it needs to predict 6 out of 10 times. This is the first demonstration that attention is capable of looking up definitions and function bodies from a large corpus. The Isabelle case study used a model with two memory layers of size 32K.

Memorizing Transformer is capable of making use of newly defined functions and theorems during test time.

### **Special Network Layer as Memory**

Another category of memory architecture introduces novel memory mechanisms tightly integrated within individual layers of Transformer. Four examples are covered here. (1) Product Key Memory<sup>[\[14\]](#ref14)</sup> replaces feed-forward sub-layer in 1~3 selected layers of decoder-only Transformer with a memory sub-layer that factorizes attention keys as product sets. (2) Memformer<sup>[\[16\]](#ref16)</sup> adds a memory reader sub-layer between self-attention sub-layer and feed-forward sub-layer in each layer of the encoder and only one memory writer layer above the encoder stack of the encoder-decoder Transformer. (3) Infinite Memory Transformer<sup>[\[22\]](#ref22)</sup> adds a different long-term memory at each layer of decoder-only Transformer to store past text sequence's input embeddings or hidden states. The model uses a set of Gaussian Radial Basis Functions and a multivariate ridge regression to transform a discrete sequence representation into a continuous-space representation and a continuous attention mechanism to obtain long-term memory context representation for a given query. (4) MemSizer<sup>[\[18\]](#ref18)</sup> replaces self-attention sub-layer in each layer of a decoder-only Transformer with a novel key-value memory layer that uses two adaptor weight matrices to make the attention mechanism scales linearly with input length. The model handles generation steps in a recurrent procedure similar to kernel-based Transformers. Another example, modern Hopfield networks<sup>[\[24\]](#ref24)</sup> for dense associative memories have been proposed to substitute attention layers of Transformer, but the idea has not been experimented on language modeling task; thus, it is not covered here.

#### **PKM**

Lample et al. (2019)<sup>[\[14\]](#ref14)</sup> introduced **PKM** (Product Key Memory) memory layer that replaces a Feed-forward layer in Transformer architecture, as illustrated in the Figure below. The memory layer is integrated with a residual connection in the network, and the input $$x$$ to the memory layer produces $$x+\mathrm{PKM}(x)$$ instead of $$x+\mathrm{FFN}(x)$$.
<p align="center"><img src="../../../assets/images/PKM.png"></p>
The memory layer is composed of three components: (1) a query network, (2) a key selection module containing two sets of sub-keys, and (3) a value lookup table. The query network $$q$$ maps the input $$x\in\mathrm{\mathbb{R}}^d$$ to a latent space $$q(x)\in\mathrm{\mathbb{R}}^{d_q}$$, where $$q$$ is typically a linear mapping or a multi-layer perceptron and $$d_q=512$$. A batch normalization layer is added on the top of the query network to help increasing key coverage during training.

In standard key selection and weighting approach, top-$$k$$ keys are selected from a set of keys $$\mathcal{K}=\{k_1,...,k_{\vert\mathcal{K}}\vert\}$$ by the largest inner product with the query $$q(x)$$, where $$k_i\in\mathrm{\mathbb{R}}^{d_q}$$. There are three steps: (1) finding the indices of the $$k$$ nearest neighbors $$\mathcal{I}=\mathcal{T}_k(q(x)^{\top}k_i)$$, where $$\mathcal{T}_k$$ denotes the top-$$k$$ operator; (2) normalizing the top-$$k$$ scores $$w=\mathrm{softmax}((q(x)^{\top}k_i)_{i\in\mathcal{I}})$$, where $$w$$ is the vector representing normalized scores; and (3) aggregating selected values $$m(x)=\sum_{i\in\mathcal{I}}w_iv_i$$, where $$m(x)$$ is the resulting memory value vector, as illustrated in the Figure below. All these operations can be implemented using auto-differentiation mechanisms, making the memory layer pluggable in a neural network. The steps (2) and (3) are computationally efficient, to handle only the top-$$k$$ keys/values. In contrast, step (1) is not efficient for large memories, for the exhaustive comparison. To circumvent this issue, the **product key** approach is introduced.
<p align="center"><img src="../../../assets/images/PKM1.png"></p>

In the **product key** approach, the query $$q(x)$$ is split into two sub-queries $$q_1\in\mathrm{\mathbb{R}}^{d_q/2}$$ and $$q_2\in\mathrm{\mathbb{R}}^{d_q/2}$$ that are used to find top-$$k$$ nearest neighbors in the sub-key ($$\in\mathrm{\mathbb{R}}^{d_q/2}$$) sets $$\mathcal{C}$$ and $$\mathcal{C}^{\prime}$$, respectively: $$\mathcal{I}_{\mathcal{C}}=\mathcal{T}_k((q_1(x)^{\top}c_i)_{i\in\{1...\vert\mathcal{C}\vert\}})$$ and $$\mathcal{I}_{\mathcal{C}^{\prime}}=\mathcal{T}_k((q_2(x)^{\top}c_j^{\prime})_{j\in\{1...\vert\mathcal{C^{\prime}}\vert\}})$$. The two sets of selected $$k$$ sub-keys are then concatenated in Cartesian product fashion to form $$k^2$$ candidate product keys ($$\in\mathrm{\mathbb{R}}^{d_q}$$) that are then used to search for the top-$$k$$ product keys nearest to $$q(x)$$ in $$\{(c_i,c_j^{\prime})\vert i\in\mathcal{I}_{\mathcal{C}},j\in\mathcal{I}_{\mathcal{C}^{\prime}}\}$$. The total number of product keys is $$\vert\mathcal{K}\vert=\vert\mathcal{C}\vert\times\vert\mathcal{C}^{\prime}\vert$$, where $$\mathcal{K}=\{(c,c^{\prime})\vert c\in\mathcal{C},c^{\prime}\in\mathcal{C}^{\prime}\}$$, as illustrated in the Figure below.
<p align="center"><img src="../../../assets/images/PKM2.png"></p>
In standard top-$$k$$ flat key selection, the complexity is $$\mathcal{O}(\vert\mathcal{K}\times d_q\vert)$$, for $$\vert\mathcal{K}\vert$$ comparisons of vectors of size $$d_q$$. In the two-stage top-$$k$$ product key selection, the complexity is $$\mathcal{O}(\sqrt{\vert\mathcal{K}\vert}\times d_q)$$ for selecting two sets of top-$$k$$ sub-keys and $$\mathcal{O}(k^2\times d_q)$$ for selecting top-$$k$$ candidate product keys, resulting in the overall complexity $$\mathcal{O}((\sqrt{\vert\mathcal{K}\vert}+k^2)\times d_q)$$. Thus, for $$\vert\mathcal{K}\vert=1024^2$$ and a small $$k$$, selecting top-$$k$$ product keys requires about $$10^3$$-fold less operations than selecting top-$$k$$ flat keys.

To make the model more expressive, a **multi-head memory** mechanism is introduced. Each head has its own query network and its own set of sub-keys, but all heads share the same values. As the query networks are independent from each other and randomly initialized, they often map the same input to very different values of the memory. The final multi-head memory value is the sum of the outputs $$m_i(x)$$ of each head $$i$$: $$m(x)=\sum_{i=1}^H m_i(x)$$ where $$H$$ is the number of heads. The multi-head memory is different from multi-head attention in that it creates a query per head instead of splitting a query into $$H$$ heads.

The Common Craw News corpus is used for this study, which contains about 40M news articles, 28B words, 140GB of data. The validation and test sets are both composed of 5,000 news articles removed from the training set. Byte Pair Encoding (BPE), with 60K BPE splits, is used to reduce the vocabulary size. Three evaluation metrics are used in this study: (1) perplexity on the test set, (2) memory usage defined as the fraction of accessed values during test, and (3) the KL divergence between normalized access weight matrix and uniform distribution. Let $$z^{\prime}\in\mathrm{\mathbb{R}}^{\vert\mathcal{K}\vert}$$ be a vector representing access weights to product key memory slots $$z_i^{\prime}=\sum_x w(x)_i$$, where $$i$$ is the index of a memory slot and $$w(x)$$ represents the weights of the keys accessed in the memory when the network is fed with an input $$x$$ from the test set (i.e., the $$w(x)$$ are sparse with at most $$H\times k$$ non-zero elements). Then, $$z=z^{\prime}/ \Vert z^{\prime}\Vert_1$$ represents L1-norm-normalized $$z^{\prime}$$. The memory usage is $$\frac{\#\{z_i\neq 0\}}{\vert\mathcal{K}\vert}$$. The KL divergence is $$\log(\vert\mathcal{K}\vert)+\sum z_i\log(z_i)$$, which reflects imbalance in the access patterns to the memory.

The Transformer architecture is decoder only with 16 attention heads and learned positional embeddings. Three layer numbers, 12, 16, or 24 and two hidden dimensions 1024 or 1600 are considered. To retrieve key indices efficiently, the search over sub-keys is performed with FAISS. The memory layers are interspersed at regular intervals; for example, 2 memory layers in 16 layers network are placed at 6th and 12th layers. The main experiments use $$H=4$$ memory heads, $$k=32$$ keys per head, and $$\vert\mathcal{K}\vert=512^2$$ memory slots.

The results show that increasing either the dimensionality or the number of layers leads to significant perplexity improvements in all the models. However, adding a memory layer to the model is more beneficial than increasing the number of layers; adding 2 or 3 memory layers further improves performance. Comparing models with the same number of layers and dimensions, adding memory layers caused small reduction in inference speed for models with 1024 dimensions, but negligible reduction for models with 1600 dimensions. A model with 12 layers and a memory layer obtains better perplexity and almost twice faster than a model with 24 layers and without a memory layer.

Increasing memory size ($$\vert\mathcal{K}\vert$$, i.e., $$\vert\mathcal{C}\vert\times\vert\mathcal{C}^{\prime}\vert$$) improves perplexity performance, with inference speed unchanged. Inference speed is mainly affected by the memory usage, which is governed by the number of memory heads and the parameter $$k$$, but not the memory size. The batch normalization layer improves the memory usage significantly, along with the perplexity, for large memory sizes ($$\geqslant 147K$$), but it doesn't help when memory size is small (16K or 65K) where the usage is already close to 100% without batch normalization. When the memory layer replaces the FFN of the layers 4 or 5 in a 6-layer Transformer, the model benefits the most, suggesting that effective use of the memory layer requires operating at higher layer and that it is important to have some layers on top of the memory layer. Increasing the number of memory heads $$h$$ or the number of $$k$$ for $$k$$-NN improves both the perplexity of the model and the memory usage. Models with identical $$h\times k$$ have a similar memory usage and perplexity. Increasing $$h$$ or $$k$$ also increases computation cost. A good trade-off between performance and speed is $$h=4$$ and $$k=32$$.

Transformer-based Language models with integrated product key memory layer drastically improve the capacity of the neural network and the perplexity performance with a negligible computational overhead, due to a much better memory usage. Two key ingredients contribute to the efficiency: the factorization of keys as a product set and the sparse read/write access to the memory values.

#### **Memformer**

Wu et al. (2020)<sup>[\[16\]](#ref16)</sup> introduced Memformer that extends encoder-decoder Transformer architecture with a learnable memory system. Given a long document $$x$$ that is split into $$T$$ segments of length $$L$$, each segment $$s_t$$ is denoted as $$s_t=[x_{t,1},x_{t,2},...,x_{t,L}]$$. The encoder recurrently encodes a segment level memory $$M_t=\mathrm{Encoder}(s_t,M_{t-1})$$. The final output of the encoder is fed into the decoder's cross attention layers to predict the token probabilities of the next segment $$s_{t+1}$$ as standard language modeling, $$P(s_t)=\prod\limits_{n=1:L}P_{\mathrm{Decoder}}(x_{t,n}\vert x_{t,<n},M_{t-1})$$. The joint probability of the document is defined as the product of each segment's probability conditioned on all of its previous segments, $$P(x)=\prod\limits_{t=1:T}P_{\mathrm{Model}}(s_t\vert s_{<t})$$. Given a text segment as the input, the model can generate the next segment, as a text continuation task. Since the memory of all the past segments are stored, the model can autoregressively generate all the text segments in a document. Because the model and the memory handle one segment at a time, the term "timestep" also refers to a segment in this paper.

In Memformer, $$k$$ number of vectors, a.k.a. memory slots, are allocated as the external dynamic memory. The memory at timestep $$t$$ is denoted as $$M_t=[m_t^1,m_t^2,...,m_t^k]$$. The memory reading is performed by a Memory Reader sublayer (Figure 1 below) in each encoder layer of the Memformer, which leverages cross attention (Figure 2 below) to achieve this function: $$Q_x,K_M,V_M=xW_Q,M_tW_K,M_tW_V$$; $$A_{x,M}=\mathrm{MHAttn}(Q_x,K_M)$$; $$H_x=\mathrm{Softmax}(A_{x,M})V_M$$. The input sequence $$x$$ is projected into queries $$Q_x$$ and the memory slot vectors $$M_t$$ are projected into keys $$K_M$$ and values $$V_M$$. MHAttn refers to Multi-Head Attention. $$H_x$$ denotes the output hidden states. Memory reading occurs multiple times as every encoder layer incorporates a memory reading module. This process ensures a higher chance of successfully retrieving the necessary information from a large memory.
<p align="center"><img src="../../../assets/images/MemformerV2.png"></p>
The memory writing involves a slot attention module to update memory information and a forgetting method to clean up unimportant memory information. Contrary to memory reading, memory writing only happens at the last layer of the encoder. This helps to store the high-level contextual representations into the memory. In practice, some classification tokens are appended to the input sequence to better extract the sequence representations. As shown in Figure 3 above, each slot is separately projected into queries and keys: $$Q_{m^i},K_{m^i}=m^iW_Q,m^iW_K$$. The segment token representations are projected into keys and values: $$K_x,V_x=xW_K,xW_V$$. In slot attention, each memory slot can only attend to itself and the token representations, but not directly attend to other slots: $$A_{m^i}^{\prime}=\mathrm{MHAttn}(Q_{m^i},[K_{m^i};K_x])$$. This is implemented using a special type of sparse attention pattern (Figure 9 below). The final attention scores are computed by dividing the raw attention with a temperature $$\tau$$ ($$\tau <1$$), to sharpen the attention distribution and focus on fewer slots or token outputs: $$A_{m^i}=\frac{\exp(A_{m^i}^{\prime}/\tau)}{\sum_j\exp(A_{m^j}^{\prime}/\tau)}$$. The next timestep's memory is $${m_{t+1}^i}^{\prime}=\mathrm{Softmax}(A_{x,M})[m_t^i;V_x]$$. The attention mechanism helps each memory slot to choose to whether preserve its old information or update with the new information.

A forgetting mechanism called Biased Memory Normalization (BMN) is introduced for slot memory representations. The memory slots are normalized for every step to prevent memory weights from growing infinitely and maintain gradient stability over long timesteps. A learnable bias vector $$v_{bias}$$ is added to each memory slot to help forget previous information: $$m_{t+1}^i\leftarrow m_{t+1}^i+v_{bias}^i$$, $$m_{t+1}^i\leftarrow\frac{m_{t+1}^i}{\Vert m_{t+1}^i\Vert}$$. The initial state of a memory slot is set as normalized $$v_{bias}$$: $$m_0^i\leftarrow\frac{v_{bias}^i}{\Vert v_{bias}^i\Vert}$$. Because of the normalization, all memory slots will be projected onto a sphere distribution. When adding $$v_{bias}$$ to the memory slot, it would cause the memory to move along
the sphere and forget part of its information. If a memory slot is not updated for many timesteps, it will eventually reach the terminal state $$T$$ that is also the initial state and is learnable. The speed of forgetting is controlled by the magnitude of $$v_{bias}$$ and the cosine distance between $$m_{t+1}^{\prime}$$ and $$v_{bias}$$. As the examples in Figure 4 below, $$m_b$$ is nearly opposite to the terminal state, and thus would be hard to forget its information, while $$m_a$$ is closer to the terminal state and thus easier to forget.

This learnable memory design requires back-propagation through time (BPTT) over a long range of timesteps so that the memory writer network can be trained to retain long-term information. The problem with traditional BPTT is that it unrolls the entire computational graph during the forward pass and stores all the intermediate activations. This process would lead to impractically huge memory consumption for Memformer. To eliminate this problem, Memory Replay Back-Propagation (MRBP) method is introduced to replay the memory at each timestep to accomplish gradient back-propagation over long unrolls. MRBP is designed specifically for recurrent neural networks. The algorithm takes an input with a rollout $$x_t$$, $$x_{t+1}$$,..., $$x_T$$ and the previous memory $$M_t, M_{t+1},...,M_T$$, if already computed. MRBP only traverses the critical path in the computational graph during the forward pass to obtain each timestep's memory and store those memories in the replay buffer. During the backward pass, MRBP backtracks the memories in the replay buffer from time $$T$$ to $$t$$ and recomputes the partial computational graph for the local timestep. It continues the computation of the remaining graph with the output $$O_t$$ to get the loss for back-propagation. There are two directions of gradients for the model. One direction of gradients comes from the local back-propagation of loss, while the other part comes from the back-propagation of the next memory's Jacobin $$\bigtriangledown M_{t+1}$$. The full algorithm is shown below.
<p align="center"><img src="../../../assets/images/MRBPV2.png"></p>
Transformer-XL and Compressive Transformer use limited-length FIFO queue to store past hidden states as memories. If a sequence is longer than the maximum temporal range, the models will lose information when the stored memories are discarded. Transformer-XL has a memory cost of $$O(K\times L)$$, where $$K$$ is the memory size, and $$L$$ is the number of layers. Compressive Transformer extends the memory cost to $$O((K+K_{cm})\times L)$$, by compressing the memories in Transformer-XL into the new compressed memories with a size of $$K_{cm}$$ using a compression ratio $$c$$. Memformer only stores $$K$$ vectors to be shared by all layers, thus with memory cost of $$O(K)$$.

WikiText-103 dataset, containing 28K articles with an average of 3.6K tokens per article, is used for all language modeling experiments in this paper. Byte pair encoding (BPE) is applied to avoid unknown tokens. Different from the attention length of 1,600 tokens in the original Transformer-XL and Compressive Transformer, this study uses much smaller input size of 128 and memory size of 512, compressed memory size of 512, and compression ratio of 4. Baseline models include Transformer-XL base ($$L=16$$) and Compressive Transformer ($$L=16$$). Memformer Encoder-Decoder has $$L_{encoder}=4$$ and $$L_{decoder}=16$$. For fair comparison, all models have $$d_{hidden}=512$$, $$d_{ff}=2048$$, $$N_{heads}=8$$, $$d_{head}=64$$. The number of inference FLOPs and perplexity median from three trials are used as evaluation metrics. As Transformer-XL's memory size increased from 32 to 1600, the perplexity dropped as expected, but the number of FLOPs grew quickly because the attention length was also increased. Compressive Transformer achieves slightly better performance with extra FLOPS compared to Transformer-XL with memory size 1024. Memformer with
encoders $$L=4$$, decoder $$L=16$$, and memory size 1,024 significantly outperforms Transformer-XL with memory size 1024, using much less computation cost. Ablation studies by reducing decoder layers ($$L_{encoder}=4$$ and $$L_{decoder}=12$$), reducing memory size to 512, or completely removing memory module significantly reduce perplexity performance of Memformer.

Analyses of normalized attention values in the attention outputs from the memory writer reveal that there are three types of memory slots: (1) majority (60%~80%) of the memory slots during the middle of processing a document focus attention on themselves, meaning not updating for the current timestep; (2) the second type of slots have some partial attention over itself and the rest of attention over other tokens, as if they are aggregating information from other tokens at the current timestep; (3) the third type of slots completely attend to the input tokens, such as named entities and verbs. The third type of slots have larger magnitudes in their forgetting vectors' bias, suggesting that these memory slots change repidly.

#### **Infinite Memory Transformer**

Martins et al. (2021)<sup>[\[22\]](#ref22)</sup> introduced $$\infty$$-former that extends Transformer with an unbounded long-term memory (LTM) at each layer, using a continuous-space attention framework<sup>[\[23\]](#ref23)</sup> that trades off the number of tokens stored into memory (basis functions) with the granularity of their representations. In this framework, the input sequence is represented as a continuous signal, expressed as a linear combination of radial basis functions (RBFs). The input context with length $$L$$ can be represented using $$N$$ number of basis functions, where $$N<L$$; thus, reducing attention complexity. The $$N$$ can be fixed, making it possible to represent unbounded context in memory without increasing its attention complexity, $$O(L^2+L\times N)$$, at the cost of losing resolution. The concept of "sticky memories" is introduced to mitigate the problem of losing resolution for old memories, which attributes larger spaces in the LTM's new signal to the relevant regions of the previous memory's signal.

In Transformer, self-attention linearly projects the input sequence $$X=[x_1,...,x_L]\in\mathrm{\mathbb{R}}^{L\times e}$$, where $$e$$ is the embedding size of the attention layer, to queries $$Q=XW^Q$$, keys $$K=XW^K$$, and Values $$V=XW^V$$, where $$W^Q,W^K,W^V\in\mathrm{\mathbb{R}}^{e\times e}$$ are learnable projection matrices. In multi-head self-attention, $$Q$$, $$K$$, and $$V$$ are split into $$H$$ number of heads $$Q_h$$, $$K_h$$, $$V_h\in\mathrm{\mathbb{R}}^{L\times d}$$ for $$h\in\{1,...,H\}$$ where $$d=e/H$$ is the dimension of each head. Then, the context representation $$Z_h\in\mathrm{\mathbb{R}}^{L\times d}$$ from an attention head $$h$$ is $$Z_h=\mathrm{softmax}(\frac{Q_hK_h^{\top}}{\sqrt{d}})V_h$$ where the softmax is performed row-wise. The $$Z_h$$ are concatenated to obtain the final context representation $$Z\in\mathrm{\mathbb{R}}^{L\times e}$$: $$Z=[Z_1,...,Z_H]W^R$$, where $$W^R\in\mathrm{\mathbb{R}}^{e\times e}$$ is another projection matrix that aggregates all head's representations.

In continuous attention, the discrete text sequence representation $$X\in\mathrm{\mathbb{R}}^{L\times e}$$ is first transformed into a continuous signal by expressing it as a linear combination of basis functions. Each $$x_i$$ for $$i\in\{1,...,L\}$$ is first associated with a position $$t_i\in [0,1]$$, e.g., by setting $$t_i=i/L$$. Then, a continuous-space representation, i.e., continuous signal, $$\bar{X}(t)\in\mathrm{\mathbb{R}}^e$$ for any $$t\in[0,1]$$ is obtained by $$\bar{X}(t)=B^{\top}\psi(t)$$, where $$B\in\mathrm{\mathbb{R}}^{N\times e}$$ is a coefficient matrix and $$\psi(t)\in\mathrm{\mathbb{R}}^N$$ are $$N$$ 1D RBFs located in $$[0,1]$$. The $$B$$ is obtained with a multivariate ridge regression criterion so that $$\bar{X}(t_i)\approx x_i$$ for each $$i\in[L]$$, which leads to the closed form: $$B^{\top}=X^{\top}F^{\top}(FF^{\top}+\lambda I)^{-1}=X^{\top}G$$, where $$F=[\psi(t_1),...,\psi(t_L)]\in\mathrm{\mathbb{R}}^{N\times L}$$ packs the basis vectors for the $$L$$ locations. $$F$$ and $$G\in\mathrm{\mathbb{R}}^{L\times N}$$ can be computed offline. To do continuous attention over $$\bar{X}(t)$$, a probability density $$p$$, such as Gaussian $$\mathcal{N}(t;\mu,\sigma^2)$$, is used, where $$\mu$$ and $$\sigma^2$$ are computed by a neural component. Finally, the context vector $$c$$ can be computed as $$c=\mathrm{\mathbb{E}}_p[\bar{X}(t)]$$.

In $$\infty$$-former, as illustrated in the Figure below, it is first assumed that the LTM contains an explicit input discrete sequence $$X$$ that consists of the past text sequence's input embeddings or hidden states, depending on the layer. Each layer has a different LTM; the gradient with respect to the word embeddings or hidden states are stopped before storing them in the LTM. The $$X$$ is transformed into a continuous approximation $$\bar{X}(t)$$ by $$\bar{X}(t)=B^{\top}\psi(t)$$, where $$\psi(t)\in\mathrm{\mathbb{R}}^N$$ are basis functions and coefficients $$B\in\mathrm{\mathbb{R}}^{N\times e}$$ are computed as $$B^{\top}=X^{\top}G$$. Then, the LTM keys $$K\in\mathrm{\mathbb{R}}^{N\times e}$$ and values $$V\in\mathrm{\mathbb{R}}^{N\times e}$$ are computed as $$K=BW^K, V=BW^V$$, where $$W^K,W^V\in\mathrm{\mathbb{R}}^{e\times e}$$ are learnable projection matrices that are not shared between layers. For each query $$q_{h,i}$$ for $$i\in\{1,...,L\}$$, a parameterized network takes as input the attention scores to compute $$\mu_{h,i}\in]0,1[$$ and $$\sigma_{h,i}^2\in\mathrm{\mathbb{R}}_{>0}$$: $$\mu_{h,i}=\mathrm{sigmoid}(\mathrm{affine}(\frac{K_h\ q_{h,i}}{\sqrt{d}}))$$, $$\sigma_{h,i}^2=\mathrm{softplus}(\mathrm{affine}(\frac{K_h\ q_{h,i}}{\sqrt{d}}))$$. Then, using the continuous softmax transformation<sup>[\[23\]](#ref23)</sup>, the probability density $$p_{h,i}$$ as $$\mathcal{N}(t;\mu_{h,i},\sigma_{h,i}^2)$$. Finally, given the value function $$\bar{V}_h(t)=V_h^{\top}\psi(t)$$, the head-specific representation vectors are computed as: $$z_{h,i}=\mathrm{\mathbb{E}}_{p_{h,i}}[\bar{V}_h(t)]=V_h^{\top}\mathrm{\mathbb{E}}_{p_{h,i}}[\psi(t)]$$, which forms the rows of the matrix $$Z_{\mathrm{LTM},h}\in\mathrm{\mathbb{R}}^{L\times d}$$ that goes through an affine transformation $$Z_{\mathrm{LTM}}=[Z_{\mathrm{LTM},1},...,Z_{\mathrm{LTM},H}]W^O$$. The long-term representation, $$Z_{\mathrm{LTM}}$$, is then summed to the transformer context vector, $$Z_{\mathrm{T}}$$, to obtain the final context representation $$Z\in\mathrm{\mathbb{R}}^{L\times e}$$: $$Z=Z_{\mathrm{T}}+Z_{\mathrm{LTM}}$$, which will be the input to the transformer's feedforward layer.
<p align="center"><img src="../../../assets/images/infinity_former.png"></p>
The key matrix size $$K_{\mathrm{LTM},h}\in\mathrm{\mathbb{R}}^{N\times d}$$ depends only on the number of basis functions, but not on the length of the context being attended to. Thus, the $$\infty$$-former's attention complexity is also independent of the context's length. Therefore, the $$\infty$$-former can attend to unbounded contexts without increasing the amount of computation.

To build an unbounded representation, $$M$$ locations in $$[0,1]$$ are sampled and $$\bar{X}(t)$$ are evaluated at those locations that can be linearly spaced, or sampled according to the region importance. Then, the corresponding vectors are concatenated with the new vectors coming from the short-term memory that is the same as Transformer-XL. For that, a contraction by a factor of $$\tau\in[0,1]$$ is done to make room for the new vectors: $$X^{contracted}(t)=X(t/\tau)=B^{\top}\psi(t/\tau)$$. Then, $$\bar{X}(t)$$ are evaluated at the $$M$$ locations $$0\leq t_1,t_2,...,t_M\leq\tau$$ as: $$x_m=B^{\top}\psi(t_m/\tau)$$, for $$m\in[M]$$. The $$x_m$$ vectors are used as rows of the past matrix $$X_{past}=[x_1,x_2,...,x_M]^{\top}\in\mathrm{\mathbb{R}}^{M\times e}$$ that is then concatenated with the new vectors $$X_new$$ to obtain $$X=[X_{past}^{\top},X_{new}^{\top}]^{\top}\in\mathrm{\mathbb{R}}^{(M+L)\times e}$$. A multivariate ridge regression is performed to compute the new coefficient matrix $$B\in\mathrm{\mathbb{R}}^{N\times e}$$, via $$B^{\top}=X^{\top}G$$, in which the vectors in $$X_{past}$$ are associated with positions in $$[0,\tau]$$ and the vectors in $$X_{new}$$ are associated with positions in $$[\tau,1]$$, as illustrated in the Figure below. The vectors are considered to be linearly spaced.
<p align="center"><img src="../../../assets/images/infinity_former2.png"></p>
The linearly spaced sampling approach above may not perform well in cases where some regions are more relevant than others. A "sticky memories" approach is introduced to deal with this issue, which samples the $$M$$ locations according to the signal's relevance at each region. To find the relevance of a region, a histogram is constructed based on the attention given to each interval of the signal on the previous step. For that, the signal is first divided into $$D$$ linearly spaced bins $$\{d_1,...,d_D\}$$. Then, the probability given to each bin, $$p(d_j)$$ for $$j\in\{1,...,D\}$$, is computed as $$p(d_j)\propto\sum\limits_{h=1}^H\sum\limits_{i=1}^L\int_{d_j}\mathcal{N}(t;\mu_{h,i},\sigma_{h,i}^2)dt$$, where $$H$$ is the number of attention heads and $$L$$ is the sequence length. The integral can be evaluated efficiently using the erf function: $$\int_a^b\mathcal{N}(t;\mu,\sigma^2)=\frac{1}{2}(\mathrm{erf}(\frac{b}{\sqrt{2}})-\mathrm{erf}(\frac{a}{\sqrt{2}}))$$. Then, the $$M$$ locations are sampled according to the $$p(d_j)$$.

A simple convolutional layer (with stride = 1 and width = 3) is used as gate to smooth the LTM's input discrete sequence: $$\tilde{X}=\mathrm{sigmoid}(\mathrm{CNN}(X))\odot X$$, before applying multivariate ridge regression to convert $$X$$ into $$\bar{X}$$. Given a corpus of $$T$$ tokens, the language model is trained by minimizing its negative log likelihood loss: $$\mathcal{L}_{\mathrm{NLL}}=-\sum\limits_{t=0}^{T-1}\log p(x_{t+1}\vert x_t,...,x_{t-L})$$. To avoid having uniform distributions over the LTM, the continuous attention given to the LTM is regularized by minimizing the Kullback-Leibler (KL) divergence, $$D_{KL}$$, between the attention probability density, $$\mathcal{N}(\mu_h,\sigma_h)$$, and a Gaussian prior, $$\mathcal{N}(\mu_0,\sigma_0)$$. As different heads should attend to different regions, $$\mu_0=\mu_h$$ can be set to regularize only the attention variance: $$\mathcal{L}_{\mathrm{KL}}=\sum\limits_{t=0}^{T-1}\sum\limits_{h=1}^H D_{\mathrm{KL}}(\mathcal{N}(\mu_h,\sigma_h)\Vert\mathcal{N}(\mu_h,\sigma_0))=\sum\limits_{t=0}^{T-1}\sum\limits_{h=1}^H\frac{1}{2}(\frac{\sigma_h^2}{\sigma_0^2}-\log(\frac{\sigma_h}{\sigma_0})-1)$$. Thus, the final loss to minimize is $$\mathcal{L}=\mathcal{L}_{\mathrm{NLL}}+\lambda_{\mathrm{KL}}\mathcal{L}_{\mathrm{KL}}$$, where $$\lambda_{\mathrm{KL}}$$ is a hyperparameter that controls the amount of KL regularization.

The Transformer-XL and the Compressive Transformer are used as baselines in this paper, both of which used relative positional encodings. In contrast, there is no need for positional encodings in the memory of $$\infty$$-former because the memory vectors represent basis coefficients in a predefined continuous space.

The first experiment is to sort tokens by their frequencies in a long sequence, e.g., $$1\ 2\ 1\ 3\ 1\ 0\ 3\ 1\ 3\ 2\ \mathrm{<SEP>}\ 1\ 3\ 2\ 0$$. The input consists of a sequence of tokens sampled according to a token probability distribution, not known to the system. The objective is to generate the tokens in the decreasing order of their frequencies in the sequence. The token probability distribution is designed to change over time: $$p=\alpha p_0+(1+\alpha)p_1$$, where the mixture coefficient $$\alpha\in[0,1]$$ is progressively increased from 0 to 1 as the sequence is generated. The vocabulary has 20 tokens and sequence lengths of 4,000, 8,000, and 16,000 are experimented. Transformer in this experiment has 3 layers, 6 attention heads, 1,024 input length, and 2,048 memory size. For the Compressive Transformer, both memories have size of 1,024. For the $$\infty$$-former, a STM of size 1,024 and a LTM with 1,024 Gaussian RBFs $$\mathcal{N}(t;\tilde{\mu},\tilde{\sigma}^2)$$ with $$\tilde{\mu}$$ linearly spaced in [0,1] and $$\tilde{\sigma}\in\{0.01,0.05\}$$, $$\tau=0.75$$, $$\lambda_{\mathrm{KL}}=1\times 10^{-5}$$, and $$\sigma_0=0.05$$. For shorter sequence length (4,000), the Transformer-XL slightly outperforms the Compressive Transformer and the $$\infty$$-former, because the Transformer-XL is able to keep almost the entire sequence in memory. As the sequence length increases (8,000, and 16,000), the sorting task accuracy decreases in all three models. However, this decrease is not so significant for $$\infty$$-former and $$\infty$$-former significantly outperforms the Compressive Transformer that in turn substantially outperforms Transformer-XL, indicating that it is better at modeling long sequences.

The second experiment is language modeling on the Wikitext-103 dataset. The Transformer-XL contains 16 layers, 10 attention heads, embedding size 410, feed-forward hidden size 2,100, and memory size 150. The Compressive Transformer has a compression rate of 4 and memory size of 150 for both memories. The $$\infty$$-former has STM size of 150, LTM with 150 Gaussian RBFs, $$\tau=0.5$$, $$\sigma_0=0.1$$, and a memory threshold of 900 tokens. Extending the model with a long-term memory leads to a better perplexity for both Compressive Transformer and $$\infty$$-former, with $$\infty$$-former slightly better than Compressive Transformer. Using sticky memories leads to slightly better perplexity. Histograms of attention given to the LTM by $$\infty$$-former for different layers show that in the first and middle layers, the $$\infty$$-former tends to focus more on the older memories, while in the last layer, the attention pattern is more uniform. Plots of memory space vs word index in the last layer's long-term memory (after 5 updates) without or with the sticky memories show that using sticky memories does in fact attribute large spaces to old memories, creating memories that stick over time.

The third experiment fine-tunes a pre-trained language model, GPT-2 small, on Wikitext-103 or a subset of PG-19 containing the first 2,000 books of the training set. The GPT-2 small contains 12 layers, 12 attention heads, input sequence length 512, LTM with 512 Gaussian RBFs, and a memory threshold of 2,048 tokens. The results show that by simply adding the long-term memory to GPT-2 and fine-tuning, the perplexity is improved on both Wikitext-103 and PG19. This shows the versatility of the $$\infty$$-former: it can be trained from scratch or used to improve a pre-trained model.

#### **MemSizer**

Zhang and Cai (2022)<sup>[\[18\]](#ref18)</sup> introduced MemSizer that replaces the self-attention layer of Transformer with a novel key-value memory layer, which leads to linear time complexity in sequence length and running autoregressive sequence generation with constant memory complexity. Although MemSizer significantly reduces time and space complexities, it actually underperforms vanilla Transformer on language modeling task.

A key-value memory network projects a set of input source vectors $$\mathrm{X}^s=\{\mathrm{x}_i^s\}_{i=1}^M$$ into memory key vectors $$\mathrm{K}\in\mathrm{\mathbb{R}}^{M\times h}$$ and value vectors $$\mathrm{V}\in\mathrm{\mathbb{R}}^{N\times h}$$. A target vector $$\mathrm{x}^t$$ is also projected to a query vector $$\mathrm{q}\in\mathrm{\mathbb{R}}^{h}$$ in the same embedding space as key vectors. Then, a probability vector $$\alpha$$ is computed based on inner product similarity: $$\alpha=f(\mathrm{qK}^{\top})$$, where $$f$$ denotes an activation function, typically softmax function. The output vector $$\mathrm{x}^{out}$$ of a layer is simply summarizing over the value vectors according to their probabilities: $$\mathrm{x}^{out}=\alpha\mathrm{V}$$.

The multi-head self-attention layer in a Transformer maps input vectors $$\mathrm{X}^s\in\mathrm{\mathbb{R}}^{M\times d}$$ and target vectors $$\mathrm{X}^t\in\mathrm{\mathbb{R}}^{N\times d}$$, where $$d$$ is the model dimension, to $$\mathrm{Q}=\mathrm{X}^t\mathrm{W}_q+\mathrm{b}_q$$, $$\mathrm{K}=\mathrm{X}^s\mathrm{W}_k+\mathrm{b}_k$$, and $$\mathrm{V}=\mathrm{X}^s\mathrm{W}_v+\mathrm{b}_v$$, where $$\mathrm{W}_*\in\mathrm{\mathbb{R}}^{d\times h}$$, $$\mathrm{b}_*\in\mathrm{\mathbb{R}}^{h}$$, and $$h$$ is the dimension of the query, key, value vectors. The number of attention heads $$r=\frac{d}{h}$$. The attention weight $$\alpha$$ is the normalized similarities between query vectors and key vectors: $$\alpha=\mathrm{softmax}({\frac{\mathrm{Q}\mathrm{K}^{\top}}{\sqrt{h}}})$$. The output of each head $$\mathrm{X}_{(i)}^{out}$$ is a weighted average of the value vectors $$\mathrm{X}_{(i)}^{out}=\alpha\mathrm{V}$$. The output vectors of the $$r$$ heads are concatenated to get the final vector: $$\mathrm{X}^{out}=[\mathrm{X}_{(1)}^{out},...,\mathrm{X}_{(r)}^{out}]W_o+b_o$$, where $$W_o\in\mathrm{\mathbb{R}}^{d\times d}$$ and $$b_o\in\mathrm{\mathbb{R}}^{d}$$ are the output projection weights. The self-attention in the Transformer can be perceived as an instance of the key-value memory network described above, where the memory keys $$\mathrm{K}$$ and values $$\mathrm{V}$$ are projections of the source $$\mathrm{X}^s$$.

MemSizer replaces the self-attention layer in Transformer with a different specifications of query, key, and value of a key-value memory layer: $$\mathrm{Q}=\mathrm{X}^t$$, $$\mathrm{K}=\Phi$$, $$\mathrm{V}=\mathrm{LN}(\mathrm{W}_l(\mathrm{X}^s)^{\top})\mathrm{LN}(\mathrm{X}^s\mathrm{W}_r)$$. The key matrix $$\Phi\in\mathrm{\mathbb{R}}^{k\times d}$$ is a learnable matrix shared across different input instances, where $$k$$ is the number of memory slots. The source information $$\mathrm{X}^s$$ is encoded into the value matrix $$\mathrm{V}\in\mathrm{\mathbb{R}}^{k\times d}$$ via two adaptor weights $$\mathrm{W}_l\in\mathrm{\mathbb{R}}^{k\times d}$$ and $$\mathrm{W}_r\in\mathrm{\mathbb{R}}^{d\times d}$$ that project source information into global representation $$\mathrm{\mathbb{R}}^{k\times d}$$ regardless of the input length $$M$$ and $$N$$. The $$\mathrm{LN}(\cdot)$$ denotes the layer normalization, which makes the training robust. To control the magnitude of $$\mathrm{V}$$ across variable-length input sequences, the $$\mathrm{V}$$ is multiplied by a scaling factor of $$1/\sqrt{M}$$. The memory layer is made multi-head by sharing $$\mathrm{V}$$ across $$r$$ different heads but using a distinct $$\mathrm{K}$$ for each head, unlike vanilla Transformer's $$d=hr$$. The outputs from multi-head are then aggregated through mean-pooling: $$\mathrm{X}^{out}=\frac{1}{r}\sum\limits_{i=1}^r\mathrm{X}_{(i)}^{out}$$, where $$\mathrm{X}_{(i)}^{out}$$ is the output from $$i$$-th head. The final output $$\mathrm{X}^{out}$$ has dimension $$d$$ and there is no need for output projection layer.

To perform autoregressive generation, MemSizer uses a recurrent procedure. At each generation step $$i$$, define $$\mathrm{V}_{i}$$ as the recurrent states: $$\mathrm{V}_{i}=\sum\limits_{j=1}^i\mathrm{LN}(\mathrm{W}_l(\mathrm{x}_j^s)^{\top})\mathrm{LN}(\mathrm{x}_j^s\mathrm{W}_r)$$, where $$\mathrm{x}_j^s$$ is the j-th row of $$\mathrm{X}^s$$ and $$\mathrm{V}_{i}$$ can be perceived as a rolling-sum matrix: $$\mathrm{V}_{i}=\mathrm{V}_{i-1}+\mathrm{LN}(\mathrm{W}_l(\mathrm{x}_j^s)^{\top})\mathrm{LN}(\mathrm{x}_j^s\mathrm{W}_r)$$. Thus, the output $$\mathrm{X}_{i}^{out}$$ can be computed in an incremental manner from cached recurrent matrix $$\mathrm{V}_{i-1}$$. This avoids quadratic computation overhead with respect to input sequence length.

The generation time complexity of MemSizer is $$O(Mdk+Md^2+Ndk)$$, linear with respect to input length $$M$$ and $$N$$, as opposed to $$O(MNd+Md^2+Nd^2)$$ of the self-attention. MemSizer memory only needs to store the value matrix $$\mathrm{V}$$; thus, the generation memory space complexity is $$O(dk)$$, constant with respect to input length. The $$k$$ can be arbitrarily configured to balance between performance and efficiency. In MemSizer, each memory slot value $$\mathrm{v}_{j\in\{1,...,k\}}$$ summarizes a global position-agnostic feature of the source context $$X^s$$.

The WikiText-103 dataset is used to evaluate language modeling, with hyperparameters: 32 layers, 8 heads, 128 head dimensions, 1024 model dimensions, 4096 fully connected dimensions, 0.2 dropout, and memory size $$k$$=32. The word embedding and softmax matrices are tied. Validation and test perplexity is measured by predicting the last 256 words out of the input of 512 consecutive words to avoid evaluating tokens in the beginning with limited context.

The WMT16 En-De, WMT14En-Fr, and WMT17 Zh-En datasets are used to evaluate machine translation (MT), with hyperparameters (for both encoder and decoder): 6 layers, 16 attention heads, 1024 model attentions, 4096 hidden dimensions, and 0.3 dropout. Beam search is used for decoding with beam size 5 and length penalty 1.0. Tokenized BLEU is used for evaluation. MemSizer is applied to both cross and causal attention in MT. Memory size $$k$$ is 32 and 4 for cross and causal attention, respectively.

Three previous recurrent Transformer models with linear time and constant space complexity with respect to sequence length are used as baseline models: ELU, RFA, and T2R. These models approximate the softmax attention kernel between $$\mathrm{q}$$ and $$\mathrm{k}$$ by projecting them via feature map function $$\phi(\cdot)$$. ELU uses exponential linear unit: $$\phi(x)=\mathrm{elu}(x)+1$$<sup>[\[19\]](#ref19)</sup>. RFA uses a random feature map with softmax temperature reparameterization<sup>[\[20\]](#ref20)</sup>. T2R uses trainable feature mapping which allows smaller feature size thus further improving the efficiency<sup>[\[21\]](#ref21)</sup>. Approximating self-attention softmax kernel typically needs additional steps to obtain intermediate feature mapping results. MemSizer employs a key-value memory module to avoid these intermediate steps and the output projection step in self-attention.

Language modeling results show that MemSizer outperforms ELU and RFA, and achieves comparable performance to T2R, but substantially underperforms vanilla Transformer. MemSizer also shows significantly faster generation speed and significantly smaller memory usage and model size than the three baseline models and vanilla Transformer. Machine translations results show that MemSizer, with ~17% smaller model size, outperforms RFA and T2R while being comparable to ELU in En-De and outperforms all baseline methods in En-Fr and Zh-En. Thus, MemSizer provides an improved tradeoff between accuracy of the vanilla transformer and efficiency of linear variants in language modeling and machine translation tasks.

MemSizer can generate a nearly constant number of tokens per second regardless of the sequence length, but vanilla transformer model becomes much slower at longer sequence. At 512 input sequence length for MT in En-De, MemSizer speed is about 20,000 tokens/s and Transformer speed is about 5,000 tokens/s. MemSizer also substantially outpaces the three linear recurrent baseline models, with the maximum speedup at length 64. The decoder memory consumption for MT in En-De is almost a constant over varying sequence lengths and is lower than other baselines consistently.

Increasing the number of memory slots $$k$$ (ranging from 8 to 128), improves the performance on the language modeling task, without considerable impact on time and memory cost. However, during training time, processing time per tokens are roughly linear to k, presumably because more intermediate states need to be stored for back-propagation. The number of attention heads slightly affects the test perplexity on the language modeling task, resulting in slightly better performance with more attention heads, without significant difference in training and inference overhead, as the multi-head computation is lightweight in MemSizer.

The importance of trainable memory keys $$\mathrm{K}$$ is studied by initializing $$\mathrm{K}$$ for each layer and each head with standard Xavier initialization and freezing them during the training process. In both language modeling and machine translation tasks, the performance dropped with a relatively small margin. As $$k\ll d$$, the keys in $$\mathrm{K}$$ almost orthogonal with Xavier initialization, thus less likely to "collide" with each other. Therefore, updating $$\mathrm{K}$$ becomes less essential comparing to other parts of model.

### **Special Input Tokens as Memory**

Using tokens as memory is a simple way to extract global context information, but its benefit is relatively small and inconsistent, probably because the number of tokens is limited.

#### **MemTransformer**

Burtsev et al. (2020)<sup>[\[15\]](#ref15)</sup> introduced Memory Transformer (MemTransformer) that extends Transformers by adding $$[\mathrm{mem}]$$ tokens at the beginning of the input sequence for storing non-local representations. Two additional variants of MemTransformer are also examined: MemCtrl Transformer that uses a dedicated subnetwork for processing $$[\mathrm{mem}]$$ tokens and MemBottleneck Transformer that processes $$[\mathrm{mem}]$$ tokens and input sequence of each layer in two stages, as illustrated in the Figure below.
<p align="center"><img src="../../../assets/images/memtransformer.png"></p>
In a single Transformer layer, multi-head attention $$MH(Q,K,V)$$ output and skip-connected input $$X$$ are summed up and layer-normalized (LN) to obtain aggregated representations $$A=LN(X+MH(X,X,X))$$ that is then passed through fully-connected feed-forward (FF) sub-layer to obtain hidden state output of the layer $$H=LN(A+FF(A))$$. In single MemTransformer layer, $$m$$ special $$[\mathrm{mem}]$$ tokens $$X^{mem}\in\mathrm{\mathbb{R}}^{m\times d}$$ are prepended to the standard input $$X^{seq}\in\mathrm{\mathbb{R}}^{n\times d}$$ and the concatenated input $$X^{mem+seq}=[X^{mem};X^{seq}]\in\mathrm{\mathbb{R}}^{(m+n)\times d}$$ is processed in the same standard way. In single MemCtrl Transformer layer, memory token representations and sequence token representations are processed by different subnetworks: $$A^{mem}=LN(X^{mem}+MH^{mem}(X^{mem},X^{mem+seq},X^{mem+seq}))$$, $$H^{mem}=LN(A^{mem}+FF^{mem}(A^{mem}))$$ for memory token representations, and $$A^{seq}=LN(X^{seq}+MH^{seq}(X^{seq},X^{mem+seq},X^{mem+seq}))$$, $$H^{seq}=LN(A^{seq}+FF^{seq}(A^{seq}))$$ for sequence token representations. In single MemBottleneck Transformer layer, memory token representations and sequence token representations are processed not only by different subnetworks, but also in sequential fashion: the memory token subnetwork is identical to those in MemCtrl Transformer, but the sequence token subnetwork uses the output hidden states of memory tokens as input for multi-head attention, $$A^{seq}=LN(X^{seq}+MH^{seq}(X^{seq},H^{mem},H^{mem}))$$ and $$H^{seq}=LN(A^{seq}+FF^{seq}(A^{seq}))$$. For all encoder-decoder variants of the memory transformers the decoder part is the same as in the baseline. Output of the last encoder layer [$$H^{mem};H^{seq}] is passed to the decoder layers.

Machine translation (MT), language modeling (LM), and language understanding (LU) tasks are used for evaluation. For ML task, a vanilla encoder-decoder Transformer is used as a reference model. For LM task, Transformer-XL is augmented with 20 $$[\mathrm{mem}]$$ tokens. For LU tasks, pre-trained BERT is augmented.

BLEU 4 scores of WMT-14 DE-EN translation task show that MemTransformer outperforms reference model and the performance improvement increases as the number of prepended $$[\mathrm{mem}]$$ tokens increases on 6-6-layer model, but not on 4-4-layer model. MemCtrl Transformer underperforms reference model, but outperforms it when all 6 encoder layers share parameters. MemBottleneck substantially underperforms reference model. Changing the number of prepended $$[\mathrm{mem}]$$ tokens at inference time to be different from the number at training time show that both decreasing and increasing $$[\mathrm{mem}]$$ tokens at inference time reduce performance and the extent of reduction is more prominent when the number at training time is larger. When further training is performed along with increasing $$[\mathrm{mem}]$$ tokens at inference time, the performance improves.

In language modeling task trained on WikiText-103, MemTransformer with 20 $$[\mathrm{mem}]$$ tokens modestly outperforms Transformer-XL on both word-level perplexity and character-level bits-per-character. In language understanding task on GLUE datasets, adding $$[\mathrm{mem}]$$ tokens to pre-trained BERT-base model modestly improves its performance on 6 of the 9 tasks.

Qualitative analysis of attention map, as illustrated in the Figure below, produced by the transformer heads trained for machine translation task reveal some attention patterns that can be interpreted as memory read/write as well as some in-memory processing operations such as copying and summation.
<p align="center"><img src="../../../assets/images/attentionmap.png"></p>

## **Memory Replay for Lifelong Language Learning**

In natural cognitive systems, such as human brain, learning new knowledge does not cause previously learned knowledge to be erased, which makes lifelong learning (or continual learning) possible. On the contrary, artificial neural network has been shown to exhibit catastrophic forgetting (or catastrophic interference) in learning multiple tasks sequentially<sup>[\[32\]](#ref32)[\[33\]](#ref33)</sup>, where the network substantially forgets previously learned knowledge upon learning new tasks because the weights important to previous tasks are modified. Several categories of methods have been developed for avoiding catastrophic forgetting (CF) in lifelong learning<sup>[\[34\]](#ref34)</sup>: (1) Replay-based methods store some training data of previous tasks and replay them in training new tasks to retain old knowledge; (2) Regularization-based methods add a regularization term or an optimization constraint to the loss to prevent model parameters from changing too much; (3) Gradient-based methods ensure that weights can only be modified in the direction orthogonal to the subspace spanned by all previously learned inputs; (4) Parameter isolation-based methods dedicate different subnetworks for different tasks to prevent interference during training and use learned combinations for inference; (5) Meta-learning-based methods either directly optimize the knowledge transfer among tasks or learn data representations robust to forgetting; (6) Adapter-based methods leverages capsules and dynamic routing to identify previous tasks that are similar to the new task and exploit their shared knowledge to help the new task learning and uses task masks to protect task-specific knowledge to avoid forgetting. The replay-based methods are among the best performing methods. This section covers recent studies of memory-replay-based methods for lifelong language learning using Transformer-based models.

### **Episodic Memory Model**

de Masson d'Autume et al. (2019)<sup>[\[36\]](#ref36)</sup> introduce an episodic memory model that performs a sparse experience replay and local adaptation to mitigate catastrophic forgetting in lifelong setting. At training time, newly acquired knowledge are consolidated with very sparsely replayed examples randomly selected from memory. It is shown that a 1% experience replay to learning new examples ratio is sufficient. At inference time, test input is used to retrieve relevant examples from the memory and the model parameters are updated (locally adapted) using the examples before being used to make a prediction. The lifelong learning setup in this paper assumes that the model only makes one pass over the training examples and there is no explicit identifier about the datasets (distributions) for training or testing examples. Two language learning tasks, text classification and question answering, are used for evaluation.

The model for lifelong learning needs to learn from a stream of training examples $$\{x_t,y_t\}_{t=1}^T$$ from multiple datasets of the same task, coming one after another in series. The goal of the learning is to find parameters $$\mathrm{W}$$ that minimize the negative log probability of training examples: $$\mathcal{L}(\mathrm{W})=-\sum\limits_{t=1}^T\log p(y_t\vert x_t;\mathrm{W})$$. The model consists of three main components: (i) an example encoder, (ii) a task decoder, and (iii) an episodic memory module<sup>[\[35\]](#ref35)</sup>, as illustrated in the Figure below.
<p align="center"><img src="../../../assets/images/EMM_lifelong.png"></p>
A pre-trained BERT is used as the example encoder to encode input example $$x_t$$. In text classification, $$x_t$$ is a document to be classified; BERT produces a vector representation for each token in $$x_t$$, which includes a special beginning-of-document symbol $$\mathrm{CLS}$$ as $$x_{t,0}$$. In question answering, $$x_t$$ is a concatenation of a context paragraph $$x_t^{\mathrm{context}}$$ and a question $$x_t^{\mathrm{question}}$$ separated by a special separator symbol $$\mathrm{SEP}$$.

For text classification, the task decoder is a linear transformation that takes the representation of $$x_{t,0}$$ from BERT as input and a softmax layer that predicts the class of $$x_{t,0}$$: $$p(y_t=c\vert x_t)=\frac{\exp(\mathrm{w}_c^{\top}\mathrm{x}_{t,0})}{\sum_{y\in\mathcal{Y}}\exp(\mathrm{w}_y^{\top}\mathrm{x}_{t,0})}$$. All the classes in all datasets are known in advance. For question answering, the task decoder predicts an answer span, represented as the start and end indices of the correct answer in the context, using two sets of parameters: $$\mathrm{w}_{\mathrm{start}}$$ and $$\mathrm{w}_{\mathrm{end}}$$. Given a context paragraph $$x_t^{\mathrm{context}}=\{x_{t,0}^{\mathrm{context}},...,x_{t,M}^{\mathrm{context}}\}$$, where $$M$$ is the length of the paragraph, and $$\mathrm{x}_{t,m}^{\mathrm{context}}$$ as the encoded representation of the $$m$$-th token in the context, the probability of each context token being the start of the answer is computed as: $$p(\mathrm{start}=x_{t,m}^{\mathrm{context}}\vert x_t)=\frac{\exp(\mathrm{w}_{\mathrm{start}}^{\top}\mathrm{x}_{t,m}^{\mathrm{context}})}{\sum_{n=0}^M\exp(\mathrm{w}_{\mathrm{start}}^{\top}\mathrm{x}_{t,n}^{\mathrm{context}})}$$. The probability of the end index of the answer is computed analogously using $$\mathrm{w}_{\mathrm{end}}$$. The predicted answer is the span with the highest probability after multiplying the start and end probabilities.

The episodic memory module is a key-value memory block. Another pre-trained BERT model, different from the example encoder, is used as the key network to encode the memory key $$\mathrm{u}_t$$ for the input $$x_t$$. The key network is not updatable to prevent key representations from drifting as data distribution changes. For text classification, the key vector is the encoded representation of the special beginning-of-document symbol (i.e., first token of the document to be classified), $$\mathrm{u}_t=\mathrm{x}_{t,0}$$. For question answering, the key vector is the encoded representation of the first token of the question part of the input, $$\mathrm{u}_t=\mathrm{x}_{t,0}^{\mathrm{question}}$$. For both tasks, the memory value is the input and the label $$\langle x_t,y_t\rangle$$. The memory write adopts random write strategy, where a newly seen example is written into the memory with some probability. The memory read adopts two retrieval mechanisms: random sampling and $$K$$-nearest neighbors, which are used for sparse experience replay and local adaptation, respectively. The sparse experience replay and local adaptation are used in training and inference, respectively, as illustrated in the algorithms below.
<p align="center"><img src="../../../assets/images/EMM_lifelong_algo.png"></p>
For sparse experience replay, at a certain interval during training, stored examples in the memory are randomly and uniformly sampled, and the retrieved examples are used to perform gradient updates of the example encoder-task decoder network. The experience replay procedure randomly retrieves 100 examples every 10,000 new examples. Only one gradient update is performed for the 100 retrieved examples. For local adaptation, given a test example at inference time, the key network encodes the test example into a query vector that is used to retrieve $$K$$ nearest neighbors from the memory using the Euclidean distance function. The $$K$$ retrieved examples $$\{x_i^k,y_i^k\}_{k=1}^K$$ for the $$i$$-th test example are used to perform gradient-based local adaptation by updating the parameters of the encoder-decoder model, denoted as $$\mathrm{W}$$, to obtain local parameters $$\mathrm{W}_i$$ to be used for the current prediction as: $$\mathrm{W}_i=\mathrm{argmin}_{\tilde{\mathrm{W}}}\lambda\Vert\tilde{\mathrm{W}}-\mathrm{W}\Vert_2^2-\sum\limits_{k=1}^K\alpha_k\log p(y_i^k\vert x_i^k;\tilde{\mathrm{W}})$$, where $$\lambda$$ is a hyperparameter, $$\alpha_k$$ is the weight of the $$k$$-th retrieved example and $$\sum_{k=1}^K\alpha_k=1$$. It is assumed that all $$K$$ retrieved examples are equally important regardless of their distance to the query vector and set $$\alpha_k=\frac{1}{K}$$. The $$\mathrm{W}_i$$ is only used to make a prediction for the $$i$$-the example, and the parameters are reset to $$\mathrm{W}$$ afterwards. Local adaptation gradient steps are performed only $$L$$ times, instead of finding the true minimum. The procedure locally adapts parameters of the encoder-decoder network to be better at predicting retrieved examples from the memory, while keeping it close to the base parameters $$\mathrm{W}$$.

The datasets for text classification are from 5 diverse domains (AGNews, Yelp, Amazon, DBPedia, Yahoo), with number of classes ranging from 4 to 14 and in total of 33. A balanced version of merged dataset is created to have the same number of randomly sampled examples from each source. For question answering, 3 datasets (SQuAD 1.1, TriviaQA, QuAC) of different characteristics are used. Seven models are compared: (1) ENC-DEC, a standard encoder-decoder model without episodic memory module; (2) A-GEM, Average Gradient Episodic Memory model that defines constraints on the gradients that are used to update model parameters based on retrieved examples from the memory (without dataset identifier); (3) REPLAY, a model that uses stored examples for sparse experience replay without local adaptation; (4) MBPA (memory-based parameter adaptation)<sup>[\[35\]](#ref35)</sup>, an episodic memory model that uses stored examples for local adaptation without sparse experience replay (fixed key network variant); (5) $$\mathrm{MBPA}_{++}^{\mathrm{rand}}$$, an episodic memory model with randomly retrieved examples for local adaptation (no key network); (6) MBPA++, the episodic memory model described in this paper; (7) MTL, a multitask model trained on all datasets jointly, used as a performance upper bound. The pre-trained $$\mathrm{BERT}_{\mathrm{BASE}}$$ model used for example encoder and key network has 12 transformer layers, 12 self-attention heads, and 768 hidden dimensions (110M parameters in total). The number of neighbors and the number of local adaptation steps are set as $$K$$ = 32 and $$L$$ = 30, respectively.

For text classification, the performance is in the order: MTL > MBPA++ (this paper) > MBPA (local adaptation only) > A-GEM > $$\mathrm{MBPA}_{++}^{\mathrm{rand}}$$ > REPLAY >  ENC-DEC. For question answering, the performance is in the order: MTL > MBPA++ (this paper) > MBPA (local adaptation only) > $$\mathrm{MBPA}_{++}^{\mathrm{rand}}$$ > REPLAY > A-GEM > ENC-DEC. Local adaptation (MBPA) and sparse experience replay (REPLAY) help mitigate catastrophic forgetting compared to ENC-DEC, but a combination of them is needed to achieve the best performance (MBPA++). The fact that MBPA++ substantially outperforms $$\mathrm{MBPA}_{++}^{\mathrm{rand}}$$ indicates that retrieving relevant examples from memory is crucial to ensure that the local adaptation phase is useful. The results also suggest that MBPA++ can be improved further by choosing relevant examples for sparse experience replay. MBPA++ not only shows the ability to retain performance of previously learned dataset in lifelong learning setting, but also demonstrates the ability of positive transfer, because it substantially outperforms single-dataset models on text classification.

The effect of memory capacity is studies by comparing models that store 100%, 50%, and 10% of the training examples. For the 50% and 10% cases, examples are randomly selected for memory write with probability of 0.5 and 0.1, respectively. The results demonstrate that while the performance of the model degrades as the number of stored examples decreases, the model is still able to maintain a reasonably high performance even with only 10% memory capacity of the full model. The model performance is also improved as the number of retrieved memory examples for local adaptation is increased.

The run time cost of memory replay in training is negligible because it is performed sparsely (1%), but the memory incurs higher space complexity. The local adaptation does reduce inference speed. The number of local adaptation steps needs to be at least 15 to have optimal performance; thus, this model has a slower inference speed.

## **Codes**

- [Nearest Neighbor Language Models (kNN-LM)](https://github.com/urvashik/knnlm)
- [Product-Key Memory Layers (PKM)](https://github.com/facebookresearch/XLM)
- [Infinite-former](https://github.com/deep-spin/infinite-former)
- [Sparse and Continuous Attention Mechanisms](https://github.com/deep-spin/mcan-vqa-continuous-attention)
- [Query-Key Normalization for Transformers](https://github.com/CyndxAI/QKNorm)
- [FAISS](https://github.com/facebookresearch/faiss), a library for efficient similarity search and clustering of dense vectors.
- [ScaNN (Scalable Nearest Neighbors)](https://github.com/google-research/google-research/tree/master/scann), a method for efficient vector similarity search at scale.
- [Adaptive Softmax](https://github.com/facebookresearch/adaptive-softmax), an efficient softmax approximation for graphical processing units (GPU).
- [Stanford Open Information Extraction](https://nlp.stanford.edu/software/openie.html)
- [Stanford Named Entity Recognizer](https://nlp.stanford.edu/software/CRF-NER.html)
- [JAX](https://github.com/google/jax) is a numerical computing library that combines NumPy, automatic differentiation, and first-class GPU/TPU support.
- [Haiku](https://github.com/deepmind/dm-haiku) is a simple neural network library for JAX that enables users to use object-oriented programming models while allowing full access to JAX's pure function transformations.
- [Moses, a statistical machine translation system](http://www2.statmt.org/moses/)
- [Hopfield Networks](https://github.com/ml-jku/hopfield-layers)

## **References**

<a name="ref1">[1]</a> Squire, L. R. and Zola, S. M. (1996) [Structure and function of declarative and nondeclarative memory systems](https://www.pnas.org/doi/epdf/10.1073/pnas.93.24.13515). Proc. Natl. Acad. Sci. USA, Vol. 93, pp. 13515–13522

<a name="ref2">[2]</a> McGaugh, J. L. (2000) [Memory--a Century of Consolidation](https://asset-pdf.scinapse.io/prod/2104643304/2104643304.pdf). Science, Vol. 287(5451), pp. 248-51

<a name="ref3">[3]</a> von Neumann, J. (1945) [First Draft of a Report on the EDVAC](https://web.mit.edu/STS.035/www/PDFs/edvac.pdf). IEEE Ann. Hist. Comput., Vol. 15, No. 4, pp. 27–75

<a name="ref4">[4]</a> Tay, Y., Tran, V. Q., Dehghani, M., Ni, J., Bahri, D., et al. (2022) [Transformer Memory as a Differentiable Search Index](https://arxiv.org/pdf/2202.06991.pdf). arXiv preprint arXiv:2202.06991

<a name="ref5">[5]</a> Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., et al. (2020) [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf). arXiv preprint arXiv:2001.08361

<a name="ref6">[6]</a> Abnar, S., Dehghani, M., Neyshabur, B., and Sedghi, H. (2021) [Exploring the Limits of Large Scale Pre-training](https://arxiv.org/pdf/2110.02095.pdf). arXiv preprint arXiv:2110.02095

<a name="ref7">[7]</a> Tay, Y., Dehghani, M., Rao, J., Fedus, W., Abnar, S, et al. (2021) [Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers](https://arxiv.org/pdf/2109.10686.pdf). arXiv preprint arXiv:2109.10686

<a name="ref8">[8]</a> Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., et al. (2020) [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/pdf/2006.16668.pdf). arXiv preprint arXiv:2006.16668

<a name="ref9">[9]</a> Fedus, W., Zoph, B., and Shazeer, N. (2021) [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961.pdf). arXiv preprint arXiv:2101.03961

<a name="ref10">[10]</a> Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., et al. (2021) [GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://arxiv.org/pdf/2112.06905.pdf). arXiv preprint arXiv:2112.06905

<a name="ref11">[11]</a> Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., et al. (2016) [Hybrid computing using a neural network with dynamic external memory](http://web.stanford.edu/class/psych209/Readings/GravesWayne16DNC.pdf). Nature, Vol. 538, pp. 471–476

<a name="ref12">[12]</a> Yogatama, D., de Masson d’Autume, C., and Kong, L. (2021) [Adaptive Semiparametric Language Models](https://arxiv.org/pdf/2102.02557). arXiv preprint arXiv:2102.02557

<a name="ref13">[13]</a> Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. (2019) [Generalization through Memorization: Nearest Neighbor Language Models](https://arxiv.org/pdf/1911.00172.pdf). arXiv preprint arXiv:1911.00172

<a name="ref14">[14]</a> Lample, G., Sablayrolles, A., Ranzato, M., Denoyer, L., and J$$\acute e$$gou, H. (2019) [Large Memory Layers with Product Keys](https://arxiv.org/pdf/1907.05242.pdf). arXiv preprint arXiv:1907.05242

<a name="ref15">[15]</a> Burtsev, M. S., Kuratov, Y., Peganov, A., and Sapunov, G. V. (2020) [Memory Transformer](https://arxiv.org/pdf/2006.11527.pdf). arXiv preprint arXiv:2006.11527

<a name="ref16">[16]</a> Wu, Q., Lan, Z., Gu, J., and Yu, Z. (2020) [Memformer: The Memory-Augmented Transformer](https://arxiv.org/pdf/2010.06891.pdf). arXiv preprint arXiv:2010.06891

<a name="ref17">[17]</a> Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. (2022) [Memorizing Transformers](https://arxiv.org/pdf/2203.08913.pdf). arXiv preprint arXiv:2203.08913

<a name="ref18">[18]</a> Zhang, Y. and Cai, D. (2022) [Linearizing Transformer with Key-Value Memory Bank](https://arxiv.org/pdf/2203.12644.pdf). arXiv preprint arXiv:2203.12644

<a name="ref19">[19]</a> Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. (2020) [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/pdf/2006.16236.pdf). arXiv preprint arXiv:2006.16236

<a name="ref20">[20]</a> Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N. A., and Kong, L. (2021) [Random Feature Attention](https://openreview.net/pdf?id=QtTKTdVrFBB). In Proc. of ICLR.

<a name="ref21">[21]</a> Kasai, J., Peng, H., Zhang, Y., Yogatama, D., Ilharco, G., et al. (2021) [Finetuning Pretrained Transformers into RNNs](https://arxiv.org/pdf/2103.13076.pdf). arXiv preprint arXiv:2103.13076

<a name="ref22">[22]</a> Martins, P. H., Marinho, Z., and Martins, A. F. T. (2021) [$$\infty$$-former: Infinite Memory Transformer](https://arxiv.org/pdf/2109.00301.pdf). arXiv preprint arXiv:2109.00301

<a name="ref23">[23]</a> Martins, A. F. T., Farinhas, A., Treviso, M., Niculae, V., Aguiar, P. M. Q., and Figueiredo, M. A. T. (2020) [Sparse and Continuous Attention Mechanisms](https://arxiv.org/pdf/2006.07214.pdf). arXiv preprint arXiv:2006.07214

<a name="ref24">[24]</a> Ramsauer, H., Schäfl, B., Lehner, J., Seidl, P., Widrich, M., et al. (2020) [Hopfield Networks is All You Need](https://arxiv.org/pdf/2008.02217.pdf). arXiv preprint arXiv:2008.02217

<a name="ref25">[25]</a> Hochreiter, S. and Schmidhuber, J. (1997) [Long Short-Term Memory](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf). Neural Computation 9(8): 1735–1780

<a name="ref26">[26]</a> Graves, A., Wayne, G., and Danihelka, I. (2014) [Neural Turing Machines](https://arxiv.org/pdf/1410.5401.pdf). arXiv preprint arXiv:1410.5401

<a name="ref27">[27]</a> Weston, J., Chopra, S., and Bordes, A. (2014) [Memory Networks](https://arxiv.org/pdf/1410.3916.pdf). arXiv preprint arXiv:1410.3916

<a name="ref28">[28]</a> Sukhbaatar, S., Szlam, A., Weston, J., and Fergus, R. (2015) [End-To-End Memory Networks](https://proceedings.neurips.cc/paper/2015/file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf). Advances in neural information processing systems, 28.

<a name="ref29">[29]</a> Rae, J. W., Hunt, J. J., Harley, I., Senior, A., Wayne, G., Graves, A., Lillicrap, T. P. (2016) [Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes](https://proceedings.neurips.cc/paper/2016/file/3fab5890d8113d0b5a4178201dc842ad-Paper.pdf). Advances in Neural Information Processing Systems, 29.

<a name="ref30">[30]</a> Gulcehre, C., Chandar, S., Cho, K., and Bengio, Y. (2016) [Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes](https://arxiv.org/pdf/1607.00036.pdf). arXiv preprint arXiv:1607.00036

<a name="ref31">[31]</a> Gulcehre, C., Chandar, S., and Bengio, Y. (2017) [Memory Augmented Neural Networks with Wormhole Connections](https://arxiv.org/pdf/1701.08718.pdf). arXiv preprint arXiv:1701.08718

<a name="ref32">[32]</a> McCloskey, M. and Cohen, N. (1989) [Catastrophic interference in connectionist networks: The sequential learning problem](https://www.andywills.info/hbab/mccloskeycohen.pdf). In: Psychology of Learning and Motivation, Vol. 24, pp. 109–165

<a name="ref33">[33]</a> French, R. M. (1999) [Catastrophic Forgetting in Connectionist Networks: Causes, Consequences and Solutions](https://www.researchgate.net/publication/12977135_Catastrophic_forgetting_in_connectionist_networks). In: Trends in Cognitive Sciences, 3(4), 128-135

<a name="ref34">[34]</a> Ermis, B., Zappella, G., Wistuba, M., and Archambeau, C. (2022) [Memory Efficient Continual Learning for Neural Text Classification](https://arxiv.org/pdf/2203.04640.pdf). arXiv preprint arXiv:2203.04640

<a name="ref35">[35]</a> Sprechmann, P., Jayakumar, S. M., Rae, J. W., Pritzel, A., Badia, A. P., et al. (2018) [Memory-based Parameter Adaptation](https://arxiv.org/pdf/1802.10542.pdf). arXiv preprint arXiv:1802.10542

<a name="ref36">[36]</a> de Masson d'Autume, C., Ruder, S., Kong, L., and Yogatama, D. (2019) [Episodic Memory in Lifelong Language Learning](https://arxiv.org/pdf/1906.01076.pdf). arXiv preprint arXiv:1906.01076