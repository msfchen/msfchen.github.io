---
layout: post
title: "Understanding the Family of Transformer Models. Part III - Open-Domain Chatbots"
date: 2021-05-16
---
The goal of an open-domain chatbot is to optimize long-term user engagement by satisfying the human need for communication, emotional connection, and social belonging. Due to the open-ended nature, an open-domain chatbot needs to possess sufficient breadth and depth of knowledge and a wide range of human-like conversational skills. Since 2014, many deep neural network-based methods have been applied in building open-domain chatbots<sup>[\[1\]](#ref1)</sup>, including sequence-to-sequence recurrent neural network-based methods, hierarchical recurrent encoder-decoder-based methods, variational autoencoder-based methods, reinforcement learning-based methods, generative adversarial network-based methods, and pre-training transformer model-based methods. In general, pre-training transformer model-based methods achieved the state-of-the-art performance<sup>[\[1\]](#ref1)</sup>. There are three types of approaches for generating responses in open-domain chatbots: retrieval-based, generation-based, and hybrid<sup>[\[2\]](#ref2)</sup>. In retrieval-based approaches, candidate responses are retrieved, according to some matching or ranking functions, from a pre-collected human conversational dataset consisting of context-response pairs. In generation-based approaches, responses are generated word-by-word using an autoregressive language model. In hybrid approaches, some prototypical responses are first retrieved from a dataset and then used to generate final responses. Transformer-based architectures have been applied to each of the three approaches.

- [Retrieval-based Approaches](#retrieval-based-approaches)
    - [Poly-encoders](#poly-encoders)
    - [BlendedSkillTalk](#blendedskilltalk)
    - [DF-QSM](#df-qsm)
    - [PHMN](#phmn)
- [Generation-based Approaches](#generation-based-approaches)
    - [DialoGPT](#dialogpt)
    - [Meena](#meena)
    - [PLATO-2](#plato-2)
    - [Style-Controlled Generation](#style-controlled-generation)
    - [DialogBERT](#dialogbert)
- [Hybrid Approaches](#hybrid-approaches)
    - [Recipes](#recipes)
- [Codes](#codes)
- [References](#references)

## **Retrieval-based Approaches**

Application of Transformer to retrieval-based open-domain chatbots typically involves using self-attention and cross-attention mechanisms to encode context, response candidates, and their matching functions. An example is the Poly-encoders. For a retrieval-based chatbot to gain specialized skills, such as persona-awareness, empathy, or expert knowledge, specially collected conversational datasets can be used to train (fine-tune) a model that has been pre-trained on a generic chat corpus. The BlendedSkillTalk explored different ways to combine the three different skills. For all utterances, not just the immediate prior utterance, in a multi-turn chat session to be considered in context-response matching, sessions need to be encoded and used in the retrieval algorithm. An example of session-aware chatbot is the Dialogue Flow Aware Query-to-Session Matching (DF-QSM) Model. Different people have different wording preference that can be extracted from user-specific dialogue history and included in the context-response matching process. An example of such chatbot is the Personalized Hybrid Matching Network (PHMN).

### **Poly-encoders**

Humeau et al., 2019<sup>[\[5\]](#ref5)</sup> introduced BERT-based Poly-encoders architecture for accurate and fast response retrieval by combining the accuracy of cross-encoders and the speed of bi-encoders. The figure below illustrates the three types of architectures. Bi-encoders map the context and a candidate separately into a common feature space wherein a dot product, cosine, or parameterized non-linearity is used to measure their similarity. The candidate encodings are independent of the context, which enables caching of the encoded representations of a large, fixed candidate set and thus faster evaluation. On the other hand, cross-encoders use the concatenation of the context and a candidate as the input to a nonlinear function that scores their match. BERT-based encoder used here enables self-attention within the concatenated input at every layer thus rich interaction between the context and the candidate. Therefore, cross-encoders have better prediction quality, at the cost of much slower training and inference speed.
<p align="center"><img src="../../../assets/images/polyencoder.png"></p>
All the Bi-, Cross-, and Poly-encoders in this study are based on the same architecture and dimension as BERT-base, which has 12 layers, 12 attention heads, and a hidden size of 768. Three different pre-training variants are considered: (1) directly using pre-trained BERT, (2) pre-training BERT from scratch on 150M of examples extracted from Wikipedia and the Toronto Books Corpus, and (3) pre-training BERT from scratch on 174M examples extracted from Reddit. Pre-training input is the concatenation of input and label [INPUT, LABEL], where both are surrounded with the special token [S]. When pre-training on Reddit, the input is the context, and the label is the next utterance. When pre-training on Wikipedia and Toronto Books, the input is one sentence and the label the next sentence in the text. Each input token is represented as the sum of three embeddings: the token embedding, the position (in the sequence) embedding and the segment embedding. Segments for input tokens are 0, and for label tokens are 1. Pre-training tasks include masked language modeling and next-sentence/next-utterance prediction, where an utterance may consist of several sentences.

The pre-trained model is then fine-tuned for one of the four multi-sentence selection tasks: (1) ConvAI2 task based on the Persona-Chat dataset, where the model has to pick the correct annotated utterance from a set of 20 choices; (2) DSTC7 challenge (Track 1) based on Ubuntu technical support chat logs, with 100 candidates per example; (3) Ubuntu v2, similar to DSTC7 but 10 times larger corpus, with 10 candidates per example; (4) Wikipedia Article Search, where a sentence from an article is given as a search query and the model is expected to retrieve the corresponding article, with 10K candidates per example.

In a Bi-encoder, both the input context and the candidate label are encoded into vectors: $$y_{ctxt}=red(T_1(ctxt))$$ and $$y_{cand}=red(T_2(cand))$$, where $$T_1$$ and $$T_2$$ are two identically pre-trained transformer encoders that are allowed to update separately during fine-tuning. $$T(x)=h_1,...,h_N$$ is the output of a transformer $$T$$ and $$red(\cdot)$$ is a function that reduces the sequence of vectors into one vector. As the input and the label are encoded separately, segment tokens are 0 for both. During pre-training, both the context and label are surrounded by the special token [S] and therefore $$h_1$$ corresponds to [S]. The $$red(\cdot)$$ in this study chooses the first output of the transformer (corresponding to the special token [S]) as the aggregated output. The score of a candidate $$cand_i$$ is given by the dot-product $$s(ctxt,cand_i) = y_{ctxt}\cdot y_{cand_i}$$. The network is trained to minimize a cross-entropy loss in which the logits are $$y_{ctxt}\cdot y_{cand_1},...,y_{ctxt}\cdot y_{cand_n}$$, where $$cand_1$$ is the correct label and the others are chosen from the training set. During training, the other labels in the same batch are used as negative. allowing for much faster training.

In a Cross-encoder, the context and candidate are surrounded by the special token [S] and concatenated into a single vector, which is encoded using one transformer. The first output of the transformer is considered as the context-candidate embedding: $$y_{ctxt,cand}=h_1=first(T(ctxt,cand))$$, where $$first$$ is the function that takes the first vector of the sequence of vectors produced by the transformer. To score one candidate, a linear layer $$W$$ is applied to the embedding $$y_{ctxt,cand}$$ to reduce it from a vector to a scalar: $$s(ctxt,cand_i)=y_{ctxt,cand_i}W$$. The network is trained to minimize a cross entropy loss where the logits are $$s(ctxt,cand_1),...,s(ctxt,cand_n)$$, where $$cand_1$$ is the correct candidate and the rest are negatives taken from the training set, not from others in the same batch. At inference time, every candidate must be concatenated with the input context and must go through a forward pass of the entire model. Thus, this method cannot scale to a large number of candidates.

In a Poly-encoder, two separate transformer encoders are used for the context and a candidate that is encoded into a single vector $$y_{cand_i}$$. Encoded responses can be implemented as precomputed cache. The input context, which is typically much longer than a candidate, is represented with $$m$$ vectors $$(y_{ctxt}^1,...,y_{ctxt}^m)$$ instead of just one as in the Bi-encoder. To derive the $$m$$ context vectors from the $$N$$ output vectors $$(h_{ctxt}^1,...,h_{ctxt}^N)$$ of the underlying transformer, four different approaches are considered: (1) **Poly-encoder (Learnt-m)** approach learns $$m$$ context codes $$(c_1,...,c_m)$$, where $$c_i$$ extracts representation $$y_{ctxt}^i$$ by attending over all the outputs of the previous layer. That is $$y_{ctxt}^i=\sum_j w_j^{c_i}h_j$$ where $$(w_1^{c_i},...,w_N^{c_i})=\mathrm{softmax}(c_i\cdot h_1,...,c_i\cdot h_N)$$. The $$m$$ context codes are randomly initialized, and learnt during fine-tuning. Unless otherwise specified, Poly-encoder simply means Poly-encoder (Learnt-m). (2) **Poly-encoder (First-m)** approach takes the first $$m$$ outputs $$(h_{ctxt}^1,...,h_{ctxt}^m)$$. (3) **Poly-encoder (Last-m)** approach takes the last $$m$$ outputs. (4) **Poly-encoder (Last-m and first)** approach concatenates the last $$m$$ outputs with the first one, $$h_{ctxt}^1$$. Finally, given the $$m$$ global context features, $$y_{cand_i}$$ is used as query to attend over them: $$y_{ctxt}=\sum_i w_i y_{ctxt}^i$$ where $$(w_1,...,w_m)=\mathrm{softmax}(y_{cand_i}\cdot y_{ctxt}^1,...,y_{cand_i}\cdot y_{ctxt}^m)$$. The final score for that candidate label is then $$y_{ctxt}\cdot y_{cand_i}$$ as in a Bi-encoder. As $$m<N$$, where $$N$$ is the number of tokens, and the context-candidate attention is only performed at the top layer, allowing faster inference time.

Two metrics are used in the experiments: (1) $$Recall@k/C$$ where each test example has $$C$$ possible candidates to select from and (2) mean reciprocal rank (MRR). The validation performance of $$R@1/20$$ on ConvAI2 after fine-tuning a Bi-encoder using pre-trained weight of BERT-base shows higher performance with a larger batch size, ranging from 32 to 512. The Cross-encoder must recompute the embeddings for the (context, candidate) pair each time. Thus, the batch size of Cross-encoders is limited at 16 for DSTC7 and Ubuntu V2 and at 20 for ConvAI2. The fine-tuning performance is related to the layers being updated during fine-tuning, with the order for both Bi-encoder and Cross-encoder: all layers but embeddings > all layers > top 4 layers > top layer. Overall, Cross-encoders outperform Bi-encoders which in turn outperform previous stat-of-the-art models on all 3 dialogue tasks, ConvAI2, DSTC7, and Ubuntu v2 using all 3 pre-training strategies: pre-trained BERT, BERT pre-trained from scratch on Wikipedia and the Toronto Books Corpus or on Reddit. The Poly-encoder outperforms the Bi-encoder on all the tasks, with more codes (larger $$m$$) generally yielding larger improvements, using all 4 approaches of context vectors. Thus, it is recommended to use as large a code size as compute time allows. However, for the same $$m$$, Learnt-m is not always better than First-m or Last-m, dependent upon $$m$$ and the fine-tuning dataset.

Pre-training on Reddit gives further state-of-the-art performance over corresponding results with BERT on all three dialogue tasks, and all three architectures, indicating that the choice of dataset used to pre-train the models impact the final results. Given that the two pre-training datasets are of similar size, it is concluded that choosing a pre-training task (e.g., dialogue data) that is similar to the downstream tasks of interest (e.g., dialogue) is a likely explanation for these performance gains. On inference speed comparison, the difference between the Bi-encoder and the Poly-encoder is minimal when there are only 1000 candidates for the model to consider, but Poly-encoder is 5~6x slower than Bi-encoder when considering 100K candidates. The Cross-encoder, however, is 2 orders of magnitude slower than the Bi-encoder and Poly-encoder, rendering it intractable for real-time inference for chatbot or information retrieval. Thus, Poly-encoders, given their desirable performance and speed trade-off, are the preferred method.

### **BlendedSkillTalk**

Chatbots with single capability, also known as skill, of persona, in-depth knowledge, or empathy have been developed separately using specially collected datasets. Smith et al., 2020<sup>[\[6\]](#ref6)</sup> made the first attempt to blend the three capabilities into a single chatbot and compared four different blending methods, all based on poly-encoder architecture. In order to automatically evaluate the four different methods, a new dataset, BlendedSkillTalk, that combines multiple capabilities in single conversations was developed.

The ConvAI2 dataset, an extension of PersonaChat dataset, captures the ability to talk about oneself and get to know one's chat partner. It comprises more than 140K utterances of conversations between two crowdworkers, each is assigned a persona consisting of a few sentences. The Wizard of Wikipedia (WoW) dataset captures conversation informed by expert knowledge from Wikipedia and provides about 194K utterances of conversations on 1,250 topics. The EmpatheticDialogues (ED) dataset consists of about 50k utterances between a Speaker who is talking
about an emotional situation, and a Listener who is tasked to respond in an empathetic manner, acknowledging the other person’s feelings. The BlendedSkillTalk is a small crowdsourced dataset of about 5K conversations where workers are instructed to try and be knowledgeable, empathetic, or give personal details about their given persona, whenever appropriate. The dataset consists of 4,819 train-set conversations, 1,009 validation-set conversations, and 980 test-set conversations. On average, there are 11.2 utterances (5.6 pairs from the two workers) in each conversation in the train set. During the conversation collection, one of the two workers, referred to as "guided" worker, is provided with three suggested responses, one each from three single-task poly-encoder models trained on ConvAI2, ED, and WoW datasets. That "guided" worker is free to either use and modify or ignore those responses. Guided workers often, in 79.5% of utterances, choose not to use the suggestions. For the other 20.5% utterances, chosen suggestions are reasonably balanced, with 5.9%, 8.2%, 6.4% from ConvAI2, ED, and WoW, respectively. In 46.1% of the time, versus 33.3% by chance, the unguided worker continues in the same mode as the previous utterance by the guided worker. Thus, the BlendedSkillTalk dataset mimics natural conversation by featuring both continuity (“stickiness” in the conversation mode) and mode blending within a single conversation.

Each conversation in the BlendedSkillTalk dataset comes with three types of initial context: (1) each speaker is assigned a pair of sentences from randomly chosen personas from the ConvAI2 dataset; (2) each conversation is seeded with a randomly selected pair of utterances from ConvAI2, WoW, or ED, with equal probability; (3) workers are provided with the topic being discussed if the conversation seed is from WoW, or the situation description if it is from ED. It is shown that the fraction of utterances resembling a dataset increases when the seed context is from the same dataset; however, the conversations remain blended, with 47.8% of unguided utterances showing three modes, 43.2% showing two modes, and 9.1% showing single mode. The data quality of the collected conversations is controlled by a set of rules to filter out low quality conversations.

Individual utterances in the validation set of the BlendedSkillTalk dataset are also annotated by crowdsource workers as exhibiting one of four possible modes: (1) Knowledge, for using factual information, (2) Empathy, for understanding and acknowledging implied feeling, (3) Personal situations, for describing circumstances in a person's life, (4) Personal background, for describing a person's personality. Multiple modes for an utterance are allowed. Over 70% of conversations annotated contained at least 3 of 4 modes. Overall, workers’ annotation counts are 43.7% for personal background, 20.5% for knowledge, 20.3% for empathy, and 15.4% for personal situations. Thus, human evaluation is consistent with utterance classifier on the finding that the vast majority of conversations of the BlendedSkillTalk dataset feature more than one mode.

The base architecture used throughout the study is the 256-million parameter poly-encoder that contains 12 encoder layers, embedding size of 768, and (context length, label length, batch size) of (360, 72, 512). The poly-encoder is first pre-trained on the pushshift.io Reddit dataset and then fine-tuned on individual datasets. At test time, these models retrieve from the set of training utterances to output a response. Models were trained until validation-set hits@1 failed to improve for 10 epochs. Model selection during fine-tuning is performed by choosing the model that scores highest on hits@1 on the validation set. This architecture is then leveraged in the following four methods to combine the three capabilities in a single model: (1) Fine-tuning on the BlendedSkillTalk dataset (BST), (2) Fine-tuning using multi-tasking training on the three single-skill datasets (MT Single-Skills), (3) after MT Single-Skills, further fine-tuning on the BlendedSkillTalk dataset (MT Single-Skills + BST), (4) training a three-class classifier on top of BERT-base to determine the source dataset of an utterance and using it to predict which skill to use on each turn and returning the utterance produced by the corresponding single-skill model (MT Two-Stage). In the second and third methods, a debiasing procedure is applied to MT Single-Skills, wherein a persona and a topic are prepended to the first utterance if they are not already present. In addition, a Random-Skill model randomly chooses a single-skill model each turn to produce a response.

Both automatic metrics and human evaluation are used. For automatic metrics, hits@1 is reported on the test set (or validation set in the case of ConvAI2) out of 20 candidates for ConvAI2, and 100 candidates for ED and WoW. For human evaluation, crowdsource workers are asked to chat with various models and then rate, in 5-point Likert scale, the conversation on Knowledge, Empathy, Personal, and Overall.

Automatic metrics on the three single-skill benchmarks show that poly-encoder models trained on single tasks match or exceed the metrics published originally with the corresponding benchmarks, except for ED. Overall, none of the blending methods matches the single-skill models on their corresponding benchmarks, except the MT Single-Skills on WoW. On the other hand, all the blending methods show higher average scores of the three single-skill benchmarks than the three single-skill models, indicating the performance of blended models are more balanced. However, the blending method 1 (BST) performs at the same level as Random-Skill model on the ED and WoW benchmarks, but outperforms Random-Skill model on ConvAI2 benchmark, probably because the persona skill is covered in all conversation instances of the BlendedSkillTalk dataset. Among the different blending methods, the performance is in the order: the method 2 (MT Single-Skills) outperforms the method 4 (MT Two-Stage) that outperforms the method 1 (BST), on all three single-skill benchmarks and their averages.

Automatic metrics on the BlendedSkillTalk test set are evaluated either zero-shot (without having been fine-tuned on the BST train set) or after being fine-tuned on the BST train set. All single-skill models show improved blending benchmark performance once fine-tuned on the BST train set, even outperform the blending method 1 (BST). The blending method 4 (MT Two-Stage) performs at the same level as the Random-Skill model and both outperform single-skill models of WoW and ED, but not of ConvAI2. The blending methods 2 (MT Single-Skills) and 3 (MT Single-Skills + BST) outperform all single-skill model baselines in zero-shot and fine-tuned fashion, respectively, despite being the same size.

Human evaluation results show that the Overall Quality is in the order: MT Single-Skills + BST (3.6) $$>$$ MT Two-Stage (3.5) $$>$$ MT Single-Skills (3.4) $$>$$ BST (3.3) $$>$$ ConvAI2 (3.0), ED (3.0) $$>$$ Random-Skill (2.7) $$>$$ WoW (2.6). The top two methods have different advantages. The MT Single-Skills + BST method is more compact in model size, but requires joint multi-task training, then fine-tuning. The MT Two-Stage method only requires training a classifier, but is a much bigger model that uses large models for each of the three single-skill and the classifier models.

### **DF-QSM**

Traditional multi-turn retrieval-based chatbots select the most appropriate response from a set of candidate responses based on query-to-response matching models, without considering other turns within the same chat session. Fu et al., 2020<sup>[\[8\]](#ref8)</sup> introduced Dialogue Flow Aware Query-to-Session Matching (DF-QSM) Model that selects the most appropriate response from a set of candidate sessions, each of which contains multiple turns. The DF-QSM model outperforms existing state-of-the-art methods by a large margin on three common benchmarks.

In the query-to-session matching task, each example in the training set is denoted as $$\{Q,S,l\}$$, where $$Q$$ is the query, $$S$$ is the candidate session, and $$l\in\{0,1\}$$ is the label which indicates whether the response $$R$$ in session $$S$$ is an appropriate response to the query $$Q$$. The session $$S=\{H,R,F\}$$ consists of the candidate response $$R$$ and its corresponding history $$H$$ and future $$F$$. The query $$Q$$, history $$H$$, and future $$F$$ are all sequences of utterances which can be formulated as $$Q=\{Q_0,...,Q_{T_q-1}\},$$ $$H=\{H_0,...,H_{T_h-1}\},$$ and $$F=\{F_0,...,F_{T_f-1}\},$$ where $$Q_j,H_j,F_j$$ are utterances and $$T_q,T_h,T_f$$ are the max turn numbers for the query, history, and future, respectively. The response $$R$$ is a single utterance. Given the query $$Q$$ and candidate session $$S$$, the goal is to predict the label $$l$$ correctly.

The model, shown in the Figure below, contains three layers: the representation layer, the dialogue flow layer, and the interaction layer. The representation layer uses an attentive module to encode the utterances. The dialogue flow layer models the dialogue flow through local and global memory networks and explores how much information of utterances should be written to the dialogue flow. The  interaction layer utilizes an attentive module and cross-attention mechanism to obtain the interaction matching representation between the query and candidate session. Finally, the interaction representations are used to predict the query-to-session matching score $$p$$. The Figure below only shows the whole global dialogue flow's updating of the first step in the query as an example.
<p align="center"><img src="../../../assets/images/DF-QSM.png"></p>
The attentive module is a variant of the encoder of the Transformer with single-head attention. The attentive layer is composed of a single-head self-attention sub-layer and a  position-wise fully connected feed-forward sub-layer. A residual connection is employed around each of the two sub-layers, followed by layer normalization. It is abstracted as $$f_{att}(Q,K,V)\in\mathrm{\mathbb{R}}^{t\times d_k}$$, where $$Q,K,V\in\mathrm{\mathbb{R}}^{t\times d_k}$$ are matrices representing the query input, the key input, and the value input, respectively. $$t$$ is the sentence length and $$d_k$$ is the dimension of the word embedding.

Using the $$i$$-th utterance in the query $$Q_i$$ as an example, the representation layer first transforms $$Q_i$$ into word representation $$\mathrm{E}_i^q\in\mathrm{\mathbb{R}}^{t\times d_k}$$, by looking up the word embedding table. Then the word representation is fed into an attentive module to get the utterance representation $$\mathrm{U}_i^q\in\mathrm{\mathbb{R}}^{t\times d_k}:\mathrm{U}_i^q=f_{\mathrm{att}}(\mathrm{E}_i^q,\mathrm{E}_i^q,\mathrm{E}_i^q)$$. The history, the future, and the response can be encoded using the same approach. The $$𝑖$$-th utterance of the history and future can be represented as $$\mathrm{U}_i^h$$ and $$\mathrm{U}_i^f$$. The response can be represented as $$\mathrm{U}^r$$.

The dialogue flow model is designed to extract useful information from the utterances with different importance for the query-to-session matching task. It can be viewed as a generalized memory network. As the dialogue flows, the useful information of each utterance is updated to the dialogue flow memory. The memory stores the dialogue information from the start-point to the current checkpoint. The dialogue flow of the query, the history, and the future all take the utterance nearest to the response as the start-point, and flow to the farthest utterance from the response. Two types of dialogue flow strategies are considered: local dialogue flow and global dialogue flow. The former considers utterances within the query, the history, and the future separately. The latter considers the whole query-session pair in an interrelated view.

The exemplary explanation of the local dialogue flow for the future below can be applied to the query and the history similarly. Given the future representations $$\{\mathrm{U}_0^f,...,\mathrm{U}_{T_f}^f\}$$, the dialogue flow is from the first utterance $$\mathrm{U}_0^f$$ to the last utterance $$\mathrm{U}_{T_f}^f$$. The $$𝑖$$-th local memory $$\mathrm{S}_{l,i}^f\in\mathrm{\mathbb{R}}^{t\times d_k}$$ represents the dialogue flow up to now. The goal of dialogue flow updating is to add or delete the message of the $$i$$-th utterance $$\mathrm{U}_i^f$$ to or from the memory $$\mathrm{S}_{l,i}^f$$, forming the next memory $$\mathrm{S}_{l,i+1}^f$$. $$\mathrm{S}_0^f$$ is initialized by the first utterance in the future: $$\mathrm{S}_0^f=\mathrm{U}_0^f$$. The first step is to find the utterance updating information $$\mathrm{S}_{u,i}^f\in\mathrm{\mathbb{R}}^{t\times d_k}$$ that decides which information in current utterance representation $$\mathrm{U}_i^f$$ is related to the local dialogue flow memory $$\mathrm{S}_{l,i}^f$$ and will be used to update the $$\mathrm{S}_{l,i}^f$$. The $$\mathrm{S}_{u,i}^f$$ is calculated using attention mechanism:

$$\mathrm{S}_{u,i}^f=\mathrm{Softmax}(\frac{\mathrm{S}_{l,i}^f{\mathrm{U}_i^f}^{\top}}{\sqrt{d_k}})\mathrm{U}_i^f$$

where $$\mathrm{S}_{u,i}^f$$ indicates the weighted sum of rows in $$\mathrm{U}_i^f$$ and the weights are controlled by $$\mathrm{S}_{l,i}^f$$. The step 2 is to find the updating weight per turn $$\alpha_i$$:

$$\alpha_i=\tanh(\mathrm{MLP}([\mathrm{S}_{l,i}^f,\mathrm{S}_{u,i}^f]))$$

where $$[,]$$ means concatenation and $$\mathrm{MLP}$$ represents the multi-layer perceptron. $$\alpha_i\in(-1,1)$$ is the updating weight with real value between -1 and 1. The positive or negative sign of $$\alpha_i$$ decides whether $$\mathrm{S}_{u,i}^f$$ is added to or deleted from $$\mathrm{S}_{l,i}^f$$. The step 3 is to perform the updating:

$$\mathrm{S}_{l,i+1}^f=\mathrm{S}_{l,i}^f+\alpha_i\mathrm{S}_{u,i}^f$$

The Figure below illustrates local (left) and global (right) dialogue flow updating. The crucial difference between the local dialogue flow and the global dialogue flow is that the utterance representation in the global dialogue flow first attends to the global representation G to obtain an intermediate representation.
<p align="center"><img src="../../../assets/images/LDF_vs_GDF.png"></p>
For global dialogue flow, the global query-session pair representation G is constructed as $$\mathrm{G}=[\mathrm{U}^q,\mathrm{U}^h,\mathrm{U}^f]$$ where $$\mathrm{U}^f\in\mathrm{\mathbb{R}}^{T_ft\times d_k}$$ is the concatenation of $$\{\mathrm{U}_0^f,...,\mathrm{U}_{T_f}^f\}$$. $$\mathrm{U}^q$$ and $$\mathrm{U}^h$$ are constructed in the same way. The current utterance $$\mathrm{U}_i^f$$ first attends to the global representation G to form the global-aware utterance representation $$\mathrm{U}_{g,i}^f\in\mathrm{\mathbb{R}}^{t\times d_k}$$:

$$\mathrm{U}_{g,i}^f=\mathrm{Softmax}(\frac{\mathrm{U}_i^f\mathrm{G}^{\top}}{\sqrt{d_k}})\mathrm{G}$$

Then the $$\mathrm{U}_{g,i}^f$$ replaces the utterance representation $$\mathrm{U}_i^f$$ in the step 1 of local dialogue flow to form the global-aware dialogue flow updating:

$$\mathrm{S}_{u,i}^f=\mathrm{Softmax}(\frac{\mathrm{S}_{g,i}^f{\mathrm{U}_{g,i}^f}^{\top}}{\sqrt{d_k}})\mathrm{U}_{g,i}^f$$

$$\alpha_i=\tanh(\mathrm{MLP}([\mathrm{S}_{g,i}^f,\mathrm{S}_{u,i}^f]))$$

$$\mathrm{S}_{g,i+1}^f=\mathrm{S}_{g,i}^f+\alpha_i\mathrm{S}_{u,i}^f$$

Finally, the local and global dialogue flow memories are concatenated into the final dialogue flow representations $$\mathrm{S}_l^f\in\mathrm{\mathbb{R}}^{T_ft\times d_k}$$ and $$\mathrm{S}_g^f\in\mathrm{\mathbb{R}}^{T_ft\times d_k}$$, respectively, where $$\mathrm{S}_l^f=[\mathrm{S}_{l,0}^f,...,\mathrm{S}_{l,T_f}^f]$$ and $$\mathrm{S}_g^f=[\mathrm{S}_{g,0}^f,...,\mathrm{S}_{g,T_f}^f]$$. The local and global dialogue flow of query $$\{\mathrm{S}_l^q,\mathrm{S}_g^q\}$$ and history $$\{\mathrm{S}_l^h,\mathrm{S}_g^h\}$$ can be obtained similarly.

The interaction layer then makes interaction between query and dialogue flow memories through cross-attention mechanism to obtain the interaction matching representations. The inputs of the interaction layer can be the utterance representations $$\{\mathrm{U}^q,\mathrm{U}^h,\mathrm{U}^f\}$$, or the local dialogue flow memory $$\{\mathrm{S}_l^q,\mathrm{S}_l^h,\mathrm{S}_l^f\}$$, or the global dialogue flow memory $$\{\mathrm{S}_g^q,\mathrm{S}_g^h,\mathrm{S}_g^f\}$$ to learn interaction representations in different levels, where $$\mathrm{U}^{\ast}\in\mathrm{\mathbb{R}}^{T_{\ast}t\times d_k}$$, $$\mathrm{S}_l^{\ast}\in\mathrm{\mathbb{R}}^{T_{\ast}t\times d_k}$$, and $$\mathrm{S}_g^{\ast}\in\mathrm{\mathbb{R}}^{T_{\ast}t\times d_k}$$. The Figure below takes the query-history matching in the local dialogue flow as an example to illustrate the mechanism of the interaction layer.
<p align="center"><img src="../../../assets/images/DF-QSM-Interaction.png"></p>
The first step is to calculate the two-level cross-attention matrix between query and response’s history: (1) the word level cross-attention matrix $$\mathrm{M}_1^h\in\mathrm{\mathbb{R}}^{T_ht\times T_ht}$$ and (2) the dialogue flow level cross-attention matrix $$\mathrm{M}_2^h\in\mathrm{\mathbb{R}}^{T_ht\times T_ht}$$. Each element of the cross-attention matrix $$\mathrm{M}_1^h$$ and $$\mathrm{M}_2^h$$ is represented as $$\mathrm{M}_{l,1,a,b}^h=\{\mathrm{E}^q\}_a^{\top}\cdot\{\mathrm{E}^h\}_b$$ for word-level and $$\mathrm{M}_{l,2,a,b}^h=\{\mathrm{S}_l^q\}_a^{\top}\cdot\{\mathrm{S}_l^h\}_b$$ for dialogue-flow-level in which $$\{\mathrm{E}^q\}_a$$ is the $$a$$-th row of $$\mathrm{E}^q$$ and $$\{\mathrm{S}_l^h\}_b$$ is the $$b$$-th row of $$\mathrm{S}_l^h$$. The second step is to calculate attentive cross-attention matrix $$\mathrm{M}_{l,3,a,b}^h=\{\mathrm{S}_l^{\prime q}\}_a^{\top}\cdot\{\mathrm{S}_l^{\prime h}\}_b$$, where $$\mathrm{S}_l^{\prime q}=f_{att}(\mathrm{S}_l^q,\mathrm{S}_l^h,\mathrm{S}_l^h)$$ is to learn the self-attentive query representation and the history-aware query representation, and $$\mathrm{S}_l^{\prime h}=f_{att}(\mathrm{S}_l^h,\mathrm{S}_l^q,\mathrm{S}_l^q)$$ is to learn the self-attentive history representation and the query-aware history representation. The third step is for the projection sublayer to stack the three cross-attention matrices into one matrix $$\mathrm{M}_l^h=f_{stack}(\mathrm{M}_{l,1}^h,\mathrm{M}_{l,2}^h,\mathrm{M}_{l,3}^h)$$ with $$\mathrm{M}_l^h\in\mathrm{\mathbb{R}}^{3\times T_ht\times T_ht}$$ that is then projected by a 2-layer 2-D CNN to matching features. The output of the CNN are flattened and mapped into low-dimension vector representation $$\mathrm{d}_l^h=f_{flat}(f_{CNN}(\mathrm{M}_l^h))$$. The three steps of the interaction layer for the query-history interaction can be abstracted as $$\mathrm{d}_l^h=f_{inter}(\mathrm{S}_l^q,\mathrm{S}_l^h,\mathrm{E}^q,\mathrm{E}^h)$$. Other interactions can be obtained similarly for $$(\mathrm{d}_u^h,\mathrm{d}_u^f)$$ at utterance level, $$(\mathrm{d}_l^h,\mathrm{d}_l^f)$$ at the local dialogue flow level, and $$(\mathrm{d}_g^h,\mathrm{d}_g^f)$$ at the global dialogue flow level, using corresponding inputs.

The six interaction representations $$\{\mathrm{d}_u^h,\mathrm{d}_u^f,\mathrm{d}_l^h,\mathrm{d}_l^f,\mathrm{d}_g^h,\mathrm{d}_g^f\}$$ are concatenated and fed into an MLP to predict the query-to-session matching score $$p=\mathrm{MLP}([\mathrm{d}_u^h,\mathrm{d}_u^f,\mathrm{d}_l^h,\mathrm{d}_l^f,\mathrm{d}_g^h,\mathrm{d}_g^f])$$. The cross-entropy loss is calculated by

$$\mathcal{L}=-\frac{1}{|D|}\sum\limits_{(Q,S,l)\in D}l\log p + (1-l)\log (1-p)$$

where $$D$$ represents all the training samples. The six interaction representations are also fed into an MLP individually to obtain six matching prediction scores $$\{p_u^h,p_u^f,p_l^h,p_l^f,p_g^h,p_g^f\}$$. The cross-entropy is also calculated for the six scores. The average of the cross-entropy loss for the six scores is added to the cross-entropy $$\mathcal{L}$$ to train the models. The final ranking score $$p_r$$ is calculated by:

$$p_r=p+\frac{\beta p_u^h+\beta p_u^f+\gamma p_l^h+\gamma p_l^f+\gamma p_g^h+\gamma p_g^f}{2\beta + 4\gamma}$$

where $$\beta$$ controls the contribution of utterance level matching scores, and $$\gamma$$ controls the dialogue flow level contributions. In this study, $$\beta=2$$ and $$\gamma=1$$ to balance the utterance level matching scores and dialogue flow level matching scores.

Three datasets in the format of query-history-response-future (QHRF) pairs are constructed by modifying the original query-response pairs of the three corpora: (1) the Ubuntu Dialogue Corpus, containing technical support conversations related to Ubuntu, (2) Douban Conversation Corpus, containing open-domain conversations from the Chinese social network Douban, and (3) E-commerce Dialogue Corpus, containing customer service conversations from Taobao, the largest e-commerce platform in China. A randomly selected sentence $$r_a$$ from a conversation $$C_a$$ is used as the response; the sentences before and after the $$r_a$$ in the $$C_a$$ are used as query $$q_a$$ and future $$f_a$$, respectively. Thus, $$C_a=q_a+r_a+f_a$$. Another conversation $$C_b$$ containing the sentence $$r_a (r_a = r_b)$$ is used to obtain the response's history $$h_b$$ so that $$C_b=h_b+r_b+f_b$$. The resulting QHRF pairs from $$C_a$$ and $$C_b$$ are $$[q_a, unk, r_a, f_a]$$ and $$[q_a,h_b,r_b,f_b]$$, respectively.

Evaluation metrics used in this study include Mean Reciprocal Rank (MRR), $$\mathrm{R}_{10}@1$$, $$\mathrm{R}_{10}@2$$, $$\mathrm{R}_{10}@5$$, and $$\mathrm{R}_{2}@1$$, where $$\mathrm{R}_n@k$$ calculates the recall of the true positive responses among the $$k$$ selected candidates from $$n$$ available candidates. The word embedding dimension is 200 and they are pre-trained through GloVe for the corpora separately and tuned during the model training. The max turn numbers of the query, history, and future are all 5 and the max utterance length is 20. Text of different lengths are handled with padding. The two convolution layers in the projection sublayer use stride sizes (1, 1) and (3, 3) for the convolution and max-pooling layer, respectively. The filter sizes are all (3, 3). The output channels of the two convolution layers are 32 and 16, respectively.

DF-QSM model significantly outperforms the state-of-the-art IoI (Interaction-over-Interaction) model in the QRM approach, proving the superiority of query-to-session matching strategy over the conventional query-to-response strategy. Compared with IoI-QSM that uses concatenated history-response-future as response for the input to the IoI model, the DF-QSM achieves SOTA on all the metrics, indicating that dialogue flow is helpful for the query-to-session matching. Dialogue flow ablation studies with QSM w/o GDF, QSM w/o LDF, and QSM w/o DF show that the dialogue flow strategies are helpful and that the local and global dialogue flow strategies working together obtain the best performance. Session ablation studies with QSM w/o H, QSM w/o F, and DF-QSM Base show that the history and future are all helpful for the response selection and that the future is more useful than the history.

Average inference time comparison studies show that DF-QSM and IoI-QRM have similar inference efficiency but IoI-QSM is about 6-fold slower than IoI-QRM. The relationship between memory updating weight $$\alpha$$ and the turn index shows that the farther the utterance to the response, the less this utterance contributes to the QSM matching task. The relationship between the future size and the performance shows that the performance increases as the future (and session) size increases, especially for the Ubuntu corpus and E-commerce corpus. For the Douban corpus, it becomes almost flattening after the second turn, probably because the Douban corpus contains more open-domain conversation in which the topics change a lot with the dialogue progressing.

### **PHMN**

To leverage personal wording behavior in context-response matching process, Li et al., 2021<sup>[\[9\]](#ref9)</sup> introduced Personalized Hybrid Matching Network (PHMN) that incorporates hybrid representations in context and response and personalized dialogue history. As shown in the figure below, the PHMN comprises three main sub-modules: (1) hybrid representation learning module, (2) personalized dialogue content modeling, (3) aggregation and fusion.
<p align="center"><img src="../../../assets/images/PHMN.png"></p>
A dataset $$\mathcal{D}$$ is denoted as $$\mathcal{D}=\{(c_i,r_i,m_i,y_i)\}_{i=1}^N$$, where $$c_i,r_i,m_i,y_i$$ represent dialogue context, response candidate, user dialogue history, and the binary label of the response candidate, respectively. The subscript $$i$$ denotes the case index in $$\mathcal{D}$$. A dialogue context $$c$$ is represented as $$c=(u_1,u_2,...,u_j,...,u_{n_c})$$, where $$u_j$$ represents an utterance with length $$n_{u_j}$$ in the $$j$$-th turn of the dialogue context and there are $$n_c$$ utterances in the dialogue context. A dialogue history $$m$$ is represented as $$m=(u_{m,1},u_{m,2},...,u_{m,k},...,u_{m,n_m})$$, where $$u_{m,k}$$ represents an utterance with length $$n_{u_{m,k}}$$. The number of words in a candidate response $$r$$ is denoted as $$n_r$$. When a given candidate response is proper for the context and the corresponding user dialogue history, $$y=1$$; otherwise, $$y=0$$. The goal of the task is to learn a matching function $$f(\cdot)$$ from the given dataset that can yield a matching score between the dialogue context and the given response candidate with the help of user dialogue history.

Two large open-datasets with user-id, P-Ubuntu dialogue corpus in English and P-Weibo dataset in Chinese, are used in this study. Users who spoke less than 30 utterances in P-Ubuntu and 10 utterances in Weibo are filtered out. The remaining users are considered as valid users and their utterances are used as their dialogue history. The user's dialogue histories are truncated to the max length of 100 for P-Ubuntu and 50 for P-Weibo. Dialogue sessions are collected from the raw corpora only when both speakers are valid users. Dialogue cases are created from dialogue sessions by splitting them into several fragments each of which is composed of several consecutive dialogue utterances. The last utterance in the fragment is used as the gold response, and the remaining utterances are used as the dialogue context. A sliding window is used to split out dialogue cases from sessions. The maximum dialogue context turn is set to 10 for both corpora and the minimum dialogue context turn is set to 5 for P-Ubuntu and 3 for P-Weibo. Each dialogue case is paired with its users' information, containing user-id and dialogue history of both speakers. For each dialogue case, the dialogue history of the two speakers are made sure not overlapping with the dialogue session that the current dialogue case comes from. The pre-processing processes yield 600K positive cases for each corpus, which are split to 500K/50K/50K for training/validation/testing. Negative responses are randomly sampled from other responses to get 1:1 ratio of positive:negative samples for training and 1:9 for validation/testing.

The **hybrid representations** include word-level representations, phrase-level representations, and dependency representations, which together result in five interaction matrices. The **word-level representation** simply utilizes word embeddings initialized with pre-trained Word2Vec. The word-level representation of an utterance $$u_j$$ is $$U_j=[e_{u_j,1},e_{u_j,2},...,e_{u_j,k},...,e_{u_j,n_{u_j}}]\in\mathrm{\mathbb{R}}^{n_{u_j}\times d_w}$$, where $$d_w$$ is the dimension of word embedding. Similarly, a response candidate $$r$$ is denoted as $$R=[e_{r,1},e_{r,2},...,e_{r,k},...,e_{r,n_r}]\in\mathrm{\mathbb{R}}^{n_r\times d_w}$$. The **phrase-level representation** is captured by 1-D convolution on the word-level representation of a given utterance $$U_j$$ with window size $$l$$ from 1 to 3, corresponding to uni-gram, bi-gram, and tri-gram. There are $$d_f$$ filters for each window size and the stride length is 1. The $$l$$-gram phrase representation in the $$k$$-th location is calculated as $$o_k^l=ReLU(Z_k^l W_l+b_l)$$ where $$W_l$$ and $$b_l$$ are parameters of the convolutional filter with window size $$l$$, and $$Z_k^l\in\mathrm{\mathbb{R}}^{l\times d_w}$$ stands for the input unigram embeddings in the current sliding window which is formulated as: $$Z_k^l=[e_{k-\lfloor\frac{1}{2}(l-1)\rfloor},...,e_k,...,e_{k+\lfloor\frac{1}{2}l\rfloor}]$$ where $$e_k$$ is the word embedding representation of a word in either the dialogue context, $$e_{u_j,k}$$, or the response, $$e_{r,k}$$. The $$d_f$$ is set the same as $$d_w$$. The output sequence of vectors of the convolution has the same length as the input sequence of vectors by utilizing the zero-padding strategy. Thus, a given utterance $$u_j$$ is transformed to three matrices, $$U_j^1=[o_1^1,o_2^1,...,o_{n_{u_j}}^1]$$, $$U_j^2=[o_1^2,o_2^2,...,o_{n_{u_j}}^2]$$, and $$U_j^3=[o_1^3,o_2^3,...,o_{n_{u_j}}^3]$$, where $$U_j^1$$, $$U_j^2$$, and $$U_j^3$$ correspond to {1, 2, 3}-gram phrase-level representation, respectively. Similarly, the same convolutional filters are used to conduct 1-D convolution on the word-level representation of a given response $$R=[e_{r,1}, e_{r,2},...,e_{r,k},...,e_{r,n_r}]$$ to obtain three phrase-level representation matrices $$R^1$$, $$R^2$$, and $$R^3$$. The **dependency representation** is captured by the scaled dot-product multi-head self-attention mechanism of the Transformer. It takes a query sentence $$Q=[e_i]_{i=0}^{n_Q-1}$$, a key sentence $$K=[e_i]_{i=0}^{n_K-1}$$, and a value sentence $$V=[e_i]_{i=0}^{n_V-1}$$ as input, where $$n_Q,n_K,n_V$$ are number of words, $$n_K=n_V$$, and $$e_i$$ is the $$d_w$$-dimension word embedding of a word. It then performs scaled dot-product attention according to the formula $$Att(Q,K,V)=softmax(\frac{QK^{\top}}{\sqrt{d_w}})V$$, where $$K=V$$ in practice. There are $$h$$ number of heads, the $$i$$-th head of which produces output $$O_i=Att(QW_i^Q,KW_i^K,VW_i^V)$$ where $$W_i^Q,W_i^K,W_i^V\in\mathrm{\mathbb{R}}^{d_w\times(d_w/h)}$$ are trainable parameters for linear transformations. The outputs of the $$h$$ heads are concatenated to obtain the attention representations $$O=(O_1\oplus O_2\oplus ...\oplus O_h)W_O$$ where $$\oplus$$ represents column-wise concatenation operation and $$W_O\in\mathrm{\mathbb{R}}^{d_w\times d_w}$$ is trainable. Then, a layer normalization is applied and a residual connection is applied to add the output $$O$$ to the query sentence $$Q$$. The whole attentive module is denoted as $$Attention(Q,K,V)$$. In this study, $$Q=K=V$$. For a given context utterance $$u_j$$, its attention-based representation $$U_j^a$$ is the output of the $$Attention(U_j,U_j,U_j)$$ that captures word dependency within the utterance. Similarly, the dependency representation of a given response is $$R^a=Attention(R,R,R)$$. Putting together, a context utterance $$u_j$$ and a response $$r$$ have 5-channel representations $$U_j, U_j^1, U_j^2, U_j^3, U_j^a$$ (each $$\in\mathrm{\mathbb{R}}^{n_{u_j}\times d_w}$$) and $$R, R^1, R^2, R^3, R^a$$ (each $$\in\mathrm{\mathbb{R}}^{n_r\times d_w}$$), respectively. Then, five **interaction matrices** are constructed, one for each of the five utterance-response pairs, $$U_j-R, U_j^1-R^1, U_j^2-R^2, U_j^3-R^3, U_j^a-R^a$$, by direct matrix multiplications, for example, $$M_j=R\cdot U_j^{\top}$$, to obtain $$M_j, M_j^1, M_j^2, M_j^3, M_j^a$$ (each $$\in\mathrm{\mathbb{R}}^{n_r\times n_{u_j}}$$).

Personalized dialogue content modeling includes two perspectives: (1) using personalized attention scores determined from dialogue history to weight the 5-channel hybrid representations, and (2) using user's wording behavior in dialogue history to match the wording behavior of a response candidate. In the **personalized attention** perspective, each user's dialogue history is treated as a document and used to construct personalized TF-IDF corpus. Then, {1, 2, 3}-gram TF-IDF scores are computed for each given utterance. In doing so, each {1, 2, 3}-gram phrase in the response candidate is allocated with a weight. For the given response $$r$$, its {1, 2, 3}-gram personalized weights are calculated as $$a^1, a^2, a^3$$ ($$\in\mathrm{\mathbb{R}}^{n_r\times 1}$$). Then these score vectors are copied $$n_{u_j}$$ times in the column direction to form the personalized mask matrices $$A^1, A^2, A^3$$ ($$\in\mathrm{\mathbb{R}}^{n_r\times n_{u_j}}$$). Then, element-wise multiplication is applied to multiply $$A^1$$ to $$M_j,M_j^1,M_j^a$$, multiply $$A^2$$ to $$M_j^2$$, and multiply $$A^3$$ to $$M_j^3$$. The resulting new interaction matrices are denoted as $$M_j^{\prime},M_j^{1\prime},M_j^{2\prime},M_j^{3\prime},M_j^{a\prime}$$ for each context utterance response pair. In the **wording behavior matching** perspective, wording behavior is extracted from {1, 2, 3, 4}-grams. 1-D convolution is conducted on a response candidate $$R=[e_{r,1},e_{r,2},...,e_{r,n_r}]$$ and a history utterance $$U_{m,k}=[e_{m,k,1},e_{m,k,2},...,e_{m,k,n_{u_{m,k}}}]$$, where the convolution window size is from 1 to 4. There are $$\frac{1}{4}d_f$$ convolution filters for each window size, and the stride length is 1. The zero-padding is used to maintain the same length for the input and the output sequences of the convolution operation. Thus, a history utterance $$u_{m,k}$$ has four corresponding matrices $$U_{m,k}^1, U_{m,k}^2, U_{m,k}^3, U_{m,k}^4$$ ($$\in\mathrm{\mathbb{R}}^{n_{u_{m,k}}\times\frac{1}{4}d_f}$$) that are concatenated together to form the final representation of wording behavior $$U_{m.k}^c=(U_{m,k}^1\oplus U_{m,k}^2\oplus U_{m,k}^3\oplus U_{m,k}^4)$$ where $$U_{m.k}^c\in\mathrm{\mathbb{R}}^{n_{u_{m,k}}\times d_f}$$. The wording behavior representation of a response $$R_m^c\in\mathrm{\mathbb{R}}^{n_r\times d_f}$$ is obtained similarly. The wording behavior matching structure and patterns between $$U_{m,k}^c$$ and $$R_m^c$$ is calculated as $$M_{m,k}=R_m^c\cdot {U_{m,k}^c}^{\top}$$.

To aggregate matching information between a context utterance and a response, two layers of 2-D convolution and max-pooling operation and ReLU activation are stacked on the 5-channel interaction matrices $$M_j^{\prime},M_j^{1\prime},M_j^{2\prime},M_j^{3\prime},M_j^{a\prime}$$. Then a concatenation operation and an MLP with one hidden layer are used to flatten the output of the stacked CNN and generate a low-dimension vector for each context utterance response pair, denoted as $$v_j$$. For multi-turn context-response matching, PHMN computes the aggregated matching vector between each utterance in context $$c=(u_1,u_2,...,u_j,...,u_{n_c})$$ and the corresponding response candidate $$r$$, resulting in a sequence of matching vectors $$v_1,v_2,...,v_j,...,v_{n_c}$$. Utterances in a context have a temporal relationship; thus, an RNN with GRU cell is used to process the aggregated matching vectors $$v_1,v_2,...,v_j,...,v_{n_c}$$ and use the last state of the RNN as the aggregated matching degree, denoted as $$m^{rnn}\in\mathrm{\mathbb{R}}^{d_h\times 1}$$. To aggregate matching information between a history utterance and a response, the same 2-D CNN with two layers are performed on the interaction matrix $$M_{m,k}$$. After the concatenation and flatten layer, a vector $$v_{m,k}$$ is obtained as the aggregation of $$M_{m,k}$$. The dimensions of $$v_j$$ and $$v_{m,k}$$ are both $$d_h$$. In matching between dialogue history and response, PHMN outputs a bag of matching vectors $$v_{m,1},v_{m,2},...,v_{m,k},...,v_{m,n_m}$$ between each utterance in history $$m=(u_{m,1},u_{m,2},...,u_{m,k},...,u_{m,n_m})$$ and the response candidate $$r$$. Utterances in dialogue history are parallel; thus, an attention mechanism is used to fuse the matching vectors $$v_{m,1},v_{m,2},...,v_{m,k},...,v_{m,n_m}$$ by computing the weighted sum as the aggregated matching degree, denoted as $$m^{att}\in\mathrm{\mathbb{R}}^{d_h\times 1}$$. To facilitate the combination of context-response matching information and history-response matching degree, a dynamic gate mechanism is used: $$\lambda=\sigma(U^{rnn}m^{rnn}+V^{att}m^{att})$$ where $$m^{rnn}$$ is the fused context-response matching degree and $$m^{att}$$ corresponds to history-response matching, $$\sigma$$ represents the *sigmoid* activation function. The final combination of $$m^{rnn}$$ and $$m^{att}$$ is computed by $$m^t=(1-\lambda)\oplus m^{att}+\lambda\oplus m^{rnn}$$ where $$\oplus$$ denotes element-wise multiplication. $$m^t$$ is then processed by a fully connected layer followed by a softmax function to obtain a binary output.

The training objective in learning the matching function $$f(\cdot)$$ is to minimize the cross-entropy with dataset $$\mathcal{D}$$:

$$L=-\sum\limits_{i=1}^N y_i\log(f(c_i,m_i,r_i))+(1-y_i)\log(1-f(c_i,m_i,r_i))$$

Two auxiliary loss functions are constructed to enhance the training process. The first is for learning the binary classification outputs only based on context-response matching with matching function $$g_1$$:

$$L_1=-\sum\limits_{i=1}^N y_i\log(g_1(c_i,r_i))+(1-y_i)\log(1-g_1(c_i,r_i))$$

The second is for learning the binary classification outputs only based on history-response matching with matching function $$g_2$$:

$$L_2=-\sum\limits_{i=1}^N y_i\log(g_2(m_i,r_i))+(1-y_i)\log(1-g_2(m_i,r_i))$$

Nine previously developed retrieval-based approaches are used as baselines in this study: (1) TF-IDF, which represents utterance as addition of word embeddings weighted by TF-IDF scores of corresponding words and matches context and response by the cosine similarity of their utterance embeddings; (2) LSTM, which concatenates all utterances of the context into a long sentence, uses a shared LSTM network to transform context and response into vector representations, and matches the vectors through a bi-linear function with sigmoid activation; (3) Multi-View, which integrates word sequence view and utterance sequence view to model two different levels of dependency; (4) Sequential Matching Network (SMN), which uses a CNN to learn a matching vector between each context utterance and a response and then uses an RNN to to aggregate the matching vector to a matching score; (5) Deep Attention Matching Network (DAM), which builds a similar matching calculation pipeline upon the SMN, while the dependency between context utterances and response candidates are captured by stacked self-attention and cross-attention mechanisms; (6) Multi-Representation Fusion Network (MRFN), which performs context-response matching based on multiple types of sentence representations and fuses matching information from different channels effectively; (7) Interaction-Over-Interaction (IOI) network, which performs deep-level matching by stacking multiple interaction blocks, i.e. extracting and aggregating the matching information within an utterance-response pair in an iterative fashion; (8) Multi-hop Selector Network (MSN), which firstly adopts a multi-hop selector to select the relevant utterances as context and then matches the candidate response with the filtered context to get a matching score; (9) fine-tuned BERT-base model ($$\mathrm{BERT}_{ft}$$), which is initialized with BERT-base-uncased and BERT-base-Chinese for P-Ubuntu and P-Weibo, respectively, takes the concatenation of the context and the candidate response as the input, utilizes stacked self-attention layers to extract fine-grained representations, and computes matching scores with an MLP built upon the top layer.

Evaluation metrics in this study include $$R_2@1,R_{10}@1,R_{10}@2,R_{10}@5$$ and the mean reciprocal rank (MRR), where $$R_n@k$$ denotes whether top-$$k$$ retrieved responses from $$n$$ candidates contain the positive response and

$$MRR=\frac{1}{|\mathcal{T}|}\sum\limits_{\langle c,m\rangle\in\mathcal{T}}\frac{1}{rank(\langle c,m\rangle)}$$

where $$\mathcal{T}$$ indicates the context set for testing; $$rank(\langle c,m\rangle)$$ is the position of the true response regarding to the input $$\langle c,m\rangle$$ in the candidate ranking list.

The PHMN model significantly outperforms all other models in all metrics and achieves the new state-of-the-art results on P-Ubuntu dialogue corpus and P-Weibo dataset. Especially for $$R_{10}@1$$, PHMN achieves significant improvement over the strongest model without using BERT and its variations, i.e., MSN, on both datasets. $$\mathrm{BERT}_{ft}$$ outperforms other baselines by a large margin, but with the cost of model complexity and time efficiency. IOI and MSN are the strongest baselines to date without BERT and its variations. MRFN slightly underperforms IOI/MSN but substantially outperforms DAM that in turn substantially outperforms SMN. SMN performs much better than Multi-view, LSTM, and TF-IDF models on both datasets. Ablation of wording behavior confirms that wording behavior matching between user-specific dialogue history and response candidate significantly enhances multi-turn response selection. Ablation of personalized attention confirms that it effectively improves the accuracy of context-response matching. Performance improvement achieved by using personalized attention is less than by modeling wording behavior in dialogue history, indicating that wording behavior modeling is more important than personalized attention. Ablation of Fusion Gate or Auxiliary Loss show that both are helpful to the performance. Increasing the number of utterances in the dialogue history improves the models' performance, but also increases inference latency. PHMN with 100 utterances in dialogue history, 7.6M parameters, and 1.834 ms inference time significantly outperforms $$\mathrm{BERT}_{ft}$$ with 110M parameters and 17.2 ms inference time, indicating that the state-of-the-art performance of PHMN comes from the novel personalized strategies rather than a larger model size.

## **Generation-based Approaches**

Application of Transformer to generation-based open-domain chatbots typically involves directly using a Transformer variant-based autoregressive language model, without complex modules, such as dialog manager. Although both are based on Transformer variants, a generation-based chatbot differs from an autoregressive language model in three main areas: (1) pre-training dataset, (2) decoding algorithms, and (3) auxiliary pre-training objectives. Chatbots are pre-trained with large conversational corpora collected from social media, instead of general text, such as web scrapes, internet-based books corpora, or Wikipedia. Generation-based chatbots tend to produce generic and dull responses, which have led to several novel decoding algorithms, such as the maximum mutual information (MMI) re-ranking method in DialoGPT, Sample-and-Rank at Temperature T method in Meena, latent speech act and coherence estimation models in PLATO-2, minimum beam length in Recipes (a.k.a. Blender), and conditional generation or plug-and-play methods in style-controlled dialogues. In DialogBERT, a hierarchical Transformer encoder architecture is used to encode utterances and two utterance-based auxiliary objectives are used to train the model.

### **DialoGPT**

Zhang et al., 2019<sup>[\[3\]](#ref3)</sup> introduced DialoGPT (**Dialo**gue **G**enerative **P**re-trained **T**ransformer), a conversational response generation model based on GPT-2. The primary difference between GPT-2 and DialoGPT is the pre-training dataset: the former uses WebText, a high-quality web scrapes dataset, and the latter uses comment-reply chains scraped from Reddit spanning from 2005 till 2017. Reddit discussion threads are treated as tree-structured reply chains and each path from the root node to the leaf node is extracted as a training instance containing multi-turn dialogue. A set of filtering rules are applied to remove low-quality instances and the resulting dataset comprises about 147M dialogue instances in total 1.8B words.

DialoGPT model architecture inherits from GPT-2. Three different model sizes are trained, with (layers, embedding dimension, total parameters, batch size per GPU) as (12, 768, 117M, 128) for small, (24, 1024, 345M, 64) for medium, and (36, 1280, 762M, 32) for large. Text is tokenized with byte pair encodings, with a vocabulary of 50,257 entries. A multi-turn dialogue session is modeled as a long text and response generation is modeled as language modeling. All dialog turns within a session are concatenated into a long sequence of $$N$$ tokens $$x_1,...,x_N$$, ended by the end-of-text token. Dialog history is treated as source sentence $$S=x_1,...,x_m$$ and the ground truth response is treated as target sentence $$T=x_{m+1},...,x_N$$. The conditional probability of $$P(T$$\|$$S)$$ can be written as the product of a series of conditional probabilities:

$$p(T|S)=\prod\limits_{n=m+1}^{N}p(x_n|x_1,...,x_{n-1})$$

For a multi-turn dialogue session $$T_1,...,T_K$$, the conditional probability can be written as:

$$p(T_K,...,K_2|T_1)=\prod\limits_{i=2}^{K}p(T_i|T_1,...,T_{i-1})$$

Optimizing the single objective $$p(T_K,...,K_2$$\|$$T_1)$$ is equivalent to optimizing all the $$p(T_i$$\|$$T_1,...,T_{i-1})$$ source-target pairs. To reduce the common problem of generating bland or uninformative responses, a maximum mutual information (MMI) scoring function is implemented. MMI uses a pre-trained backward model to predict $$P(Source$$\|$$target)$$ of the source sentence from given responses. A set of hypotheses are generated first using top-K sampling, then the hypotheses are reranked using the probability $$P(Source$$\|$$Hypothesis)$$ that is likely to be lower for frequent and repetitive hypotheses that are associated with many possible source query.

Two different datasets are used to automatically evaluate the performance of the DialoGPT: DSTC-7 test data and a new Reddit multi-reference dataset. The former contains conversation threads from Reddit data, of which only conversation sessions containing 6 or more responses are used. For each instance, one of the 6 responses is held out as human response in automatic evaluation, leaving the others as a 5-reference test. Given other filtering criteria such as turn length, the test dataset contains 2208 instances. The latter contains 6K instances. Five metrics are used in the automatic evaluation: BLEU, METEOR, NIST (a variant of BLEU that indirectly penalizes uninformative n-grams), Entropy, and Dist-n. The last two are aimed at evaluating lexical diversity. As a baseline model, the sequence-to-sequence model PersonalityChat trained on Twitter data is used for comparison.

In the DSTC-7 tests, DialoGPT-medium with beam search achieved the highest automatic score across most metrics. Beam search (with beam width 10) dramatically improves BLEU and DIST scores, but only marginally improves NIST and METEOR scores. The automatic scores of DialoGPT are higher than those for the held-out human responses, which does not mean that the generated responses are more realistic than human but means that there exist many valid or good responses for any given message.

In the new Reddit multi-reference dataset test, two settings are compared: pre-training from scratch and fine-tuning pre-trained GPT-2. In both settings, a larger model consistently outperforms a smaller one. To evaluate the effect of re-ranking using MMI, a DialoGPT-medium fine-tuned from GPT-2-medium, is used to generate 16 responses for each input source sentence using top-K sampling; then, a backward model, another DialoGPT-medium fine-tuned from GPT-2-medium, is used to re-rank. The response that yields lowest backward model loss is selected for evaluation. Compared to Beam Search, the MMI re-ranking outperforms on METEOR and Entropy, but significantly underperforms on BLEU, NIST, and Dist.

Observations on generated examples suggest that DialoGPT is able to deal with multi-turn generation better than an RNN counterpart and tends to be more consistent with respect to context. Pairwise human evaluation by crowd-sourcing on relevance, informativeness, and human-likeness indicates that DialoGPT strongly outperforms PersonalityChat, consistent with the results from the automatic evaluation.

### **Meena**

Adiwardana et al., 2020<sup>[\[4\]](#ref4)</sup> introduced Meena, a multi-turn open-domain chatbot, based on the Evolved Transformer, a parameter-efficient variant of the Transformer. To evaluate the quality of the chatbot, the authors developed a human evaluation metric called Sensibleness and Specificity Average.

For every generated response, crowd workers are asked to label whether it completely makes sense in the given context. If a response is labeled as sensible, the crowd workers are asked to further determine if it is specific to the given context. Responses labeled as not sensible are considered as not specific. Majority votes out of 5 judges are used as the label for each response. Percentages of total responses evaluated as sensible and specific are reported as Sensibleness and Specificity scores, respectively. The average of the two scores is called SSA (**S**ensibleness and **S**pecificity **A**verage). The human evaluation is run in two different settings: static and interactive. In static evaluation, a collection of 1,477 conversational contexts with between 1 and 3 conversation turns are used as a common benchmark, called Mini-Turing Benchmark (MTB). In total, it contains 315 single-turn, 500 two-turn, and 662 three-turn contexts. All MTB contexts are fed to the models or presented to humans to obtain responses. The resulting $$(context, response)$$ pairs are then evaluated by crowd workers to obtain SSA. In interactive evaluation, the crowd workers can chat freely with a chatbot on any topic or domain. A conversation is required to last at least 14 turns (7 from chatbot) and at most 28 turns. 100 such conversations are collected for each model. The percentages of labeled turns that are sensible and specific are then used to calculate interactive SSA. In addition to SSA, crowd workers are also asked to assess whether a response is "humanlike" or not, using the static evaluation dataset. For automatic evaluation, perplexity is used, which measures how accurately a model anticipates what people will say next. The evaluation metrics are used to compare Meena with human and 4 other chatbots: XiaoIce, Mitsuku, Cleverbot, and DialoGPT. The results show that there is a high positive correlation between static SSA and Human Likeness and a strong negative correlation between static/interactive SSA and Perplexity, as shown below. Human SSAs are 82% static (94% sensibleness, 69% specificity) and 86% interactive (97% sensibleness, 75% specificity).
<p align="center"><img src="../../../assets/images/meena.png"></p>
Meena's pre-training dataset is mined and filtered from public domain social media conversations. Any path along a message tree constitutes a conversation in which each message is a conversation turn. Training instances are in the form of $$(context, response)$$ pair, where each turn is treated as a response and all the previous turns (up to 7) are treated as context. A set of filtering rules are applied to remove low quality messages. When a message is removed, all sub-trees under it are also removed. The resulting dataset contains 867M pairs. The text is tokenized with byte-pair-encoding using a vocabulary of 8K subwords. The final dataset contains 341GB of text with 40B words.

The best performing Meena model is an Evolved Transformer (ET) seq2seq model with 2.6B parameters, which includes 1 ET encoder block and 13 ET decoder blocks. The Evolved Transformer is an evolutionary neural architecture search (NAS) architecture based on the Transformer. Meena's hidden size is 2,560 and attention head count is 32. The embeddings are shared across the encoder, the decoder, and the softmax layer. Both encoder and decoder take max 128 tokens with 256 tokens combined. The end-to-end trained Meena model with lowest perplexity, 10.2, is referred to as Meena (base).

Meena uses a simple sample-and-rank decoding strategy, where $$N$$ independent candidate responses are sampled using plain random sampling with temperature $$T$$ and then the candidate response with the highest probability is selected as the final output. Temperature $$T$$ is a hyperparameter that regulates the probability distribution $$p_i = \frac{\exp(z_i/T)}{\sum_j\exp(z_j/T)}$$ of the next token during decoding, where $$z_i$$ is the logits. $$T=1$$ yields the unmodified distribution. Larger values of $$T$$ favor contextually rare tokens and smaller values of $$T$$ favor more common words. Beam-search decoding tends to generate repetitive and uninteresting responses, while sample-and-rank decoding tends to provide diverse and content-rich responses. The length-normalized log-likelihood scores $$\frac{\log P}{L}$$, where $$P$$ is the likelihood of the response and $$L$$ is the number of tokens, of the responses generated by beam-search decoding are much higher than those generated by sample-and-rank with temperature decoding.

Another version of Meena is referred to as Meena (full) or just Meena with the following improvements. The interactive SSA is increased from 72% to 74% by changing from $$T=0.88$$ and sampling from the whole vocabulary to $$T=1.0$$ and sampling from top-k (k=40). The number of samples, $$N$$, in sample-and-rank is swept over {1, 20, 400}; $$N=20$$ yields the highest SSA. The interactive SSA is further increased from 74% to 79% by automatically removing candidate responses that are detected as repetition based on the presence of long common sub-sequences with an earlier turn. An additional classifier layer is added at serving time to automatically filter out potentially sensitive or toxic response candidates.

### **PLATO-2**

Bao et al., 2020<sup>[\[10\]](#ref10)</sup> introduced **PLATO-2** that uses a 2-stage curriculum learning to build an open-domain chatbot that achieves new state-of-the-art results. The idea of curriculum learning is to first learn easier aspects of the task or easier subtasks, and then gradually increase the difficulty level. In the first stage of this study, a coarse-grained model is trained for general response generation, just like DialoGPT and Meena above. In the second stage, a fine-grained generation model and an evaluation model are trained for diverse response generation and response coherence estimation, respectively.

The backbone of PLATO-2 is consisted of the encoder portion of the Transformer blocks with pre-normalization. The model parameters are shared across different learning objectives. Different self-attention masks are used to control access to context for each word token, which is bi-directional context encoding and uni-directional response decoding, as illustrated in the Figure below.
<p align="center"><img src="../../../assets/images/PLATO-2.png"></p>
During stage 1, the coarse-grained baseline model is first trained to learn general response generation under the simplified relationship of one-to-one context-response mapping. Given one training sample
of context and response $$(c,r)$$, the objective is to minimized the following negative log-likelihood (NLL) loss:

$$\mathcal{L}_{NLL}^{\mathrm{Baseline}}=-\mathrm{\mathbb{E}}\log p(r|c)=-\mathrm{\mathbb{E}}\sum_{t=1}^T\log p(r_t|c,r_{<t})$$

, where $$T$$ is the length of the target response $$r$$ and $$r_{<t}$$ denotes previously generated words.

During stage 2.1, a discrete latent variable $$z$$ is introduced for the one-to-many context-response relationship modeling. $$z$$ is a $$K$$-way categorical variable; each of the $$K$$ values corresponds to a particular latent speech act in the response. The model will first estimate the latent act distribution of the training sample $$p(\mathrm{\mathbf{z}}$$\|$$c,r)$$ and then generate the response with the sampled latent variable $$p(r$$\|$$c,z)$$. It is notable that these two tasks of response generation and latent act recognition are trained jointly within the shared network. The posterior distribution over latent values is estimated through the task of latent act recognition:

$$p(\mathrm{\mathbf{z}}|c,r)=\mathrm{softmax}(W_1h_{[\mathrm{M}]}+b_1)\in\mathrm{\mathbb{R}}^K$$

, where $$h_{[\mathrm{M}]}\in\mathrm{\mathbb{R}}^D$$ is the final hidden state of the special mask token $$[\mathrm{M}]$$, $$W_1\in\mathrm{\mathbb{R}}^{K\times D}$$ and $$b_1\in\mathrm{\mathbb{R}}^K$$ denote the weight matrices of one fully-connected layer. The NLL loss of diverse response generation is defined as:

$$\mathcal{L}_{NLL}^{\mathrm{Generation}}=-\mathrm{\mathbb{E}}_{z\sim p(\mathrm{\mathbf{z}}|c,r)}\log p(r|c,z)=-\mathrm{\mathbb{E}}_{z\sim p(\mathrm{\mathbf{z}}|c,r)}\sum_{t=1}^T\log p(r_t|c,z,r_{<t})$$

, where $$z$$ is the latent act sampled from $$p(\mathrm{\mathbf{z}}$$\|$$c,r)$$. To facilitate the training process of discrete latent variables, the bag-of-words (BOW) loss is also employed:

$$\mathcal{L}_{BOW}^{\mathrm{Generation}}=-\mathrm{\mathbb{E}}_{z\sim p(\mathrm{\mathbf{z}}|c,r)}\sum_{t=1}^T\log p(r_t|c,z)=-\mathrm{\mathbb{E}}_{z\sim p(\mathrm{\mathbf{z}}|c,r)}\sum_{t=1}^T\log\frac{e^{f_{r_t}}}{\sum_{v\in V}e^{f_v}}$$

, where $$V$$ refers to the whole vocabulary. The function $$f$$ tries to predict the words within the target response in a non-autoregressive way: $$f=W_2h_z+b_2\in\mathrm{\mathbb{R}}^{\mid V\mid}$$, where $$h_z$$ is the final hidden state of the latent variable. $$f_{r_t}$$ denotes the estimated probability of word $$r_t$$. In comparison with NLL loss, the BOW loss discards word orders and forces the latent variable to capture the global information of target response. The objective of the fine-grained generation model is to minimize the following integrated loss:

$$\mathcal{L}^{\mathrm{Generation}}=\mathcal{L}_{NLL}^{\mathrm{Generation}}+\mathcal{L}_{BOW}^{\mathrm{Generation}}$$

During stage 2.2, the most appropriate response is selected from a set of diverse candidate responses, each of which corresponds to a distinct value of the latent variable used by the fine-grained generation model. The approach is to train an evaluation model for the estimation of the coherence between each candidate response and the given dialogue context. The loss of response coherence estimation (RCE) is defined as follows:

$$\mathcal{L}_{RCE}^{\mathrm{Evaluation}}=-\log p(l_r=1|c,r)-\log p(l_{r^-}=0|c,r^-)$$

The positive training samples come from the dialogue context and corresponding target response $$(c,r)$$, with coherence label $$l_r=1$$. The negative samples are created by randomly selecting responses from the corpus $$(c,r^-)$$, with coherence label $$l_{r^-}=0$$. The discriminative function $$p(l_r$$\|$$c,r)$$ considers the bi-directional information flow between the dialogue context and response. To maintain the capacity of distributed representation, the task of masked language model (MLM) is also included in the evaluation network, which randomly masks 15% of the input tokens for the network to recover. The MLM loss is defined as:

$$\mathcal{L}_{MLM}^{\mathrm{Evaluation}}=-\mathrm{\mathbb{E}}\sum_{m\in M}\log p(x_m|x_{\backslash M})$$

, where $$x$$ refers to the input tokens of context and response. $$\{x_m\}_{m\in M}$$ stands for masked tokens and $$x_{\backslash M}$$ denotes the unmasked tokens. The objective of the evaluation model is to minimize the following integrated loss:

$$\mathcal{L}^{\mathrm{Evaluation}}=\mathcal{L}_{RCE}^{\mathrm{Evaluation}}+\mathcal{L}_{MLM}^{\mathrm{Evaluation}}$$

The inference is carried out with the second stage's models in two steps: (1) Diverse response generation, in which the fine-grained generation model $$p(r_z$$\|$$c,z)$$ produces $$K$$ candidate responses, each $$r_z$$ corresponding to a latent value $$z\in\{1,...,K\}$$; (2) Response coherence estimation, in which the evaluation model performs ranking and selects the one with the highest coherence value as the final response $$r^*=\mathrm{argmax}_{r_z}p(l_{r_z}=1$$\|$$c,r_z)$$

The English training data is extracted from pushshift.io Reddit. After elaborate filtering, the data is split into training and validation sets in chronological order, containing 684M and 0.2M (context, response) samples, respectively. The Chinese training data is collected from public domain social medias. After filtering, there are 1.2B, 0.1M, 0.1M (context, response) samples in the training, validation, and test sets, respectively. The English vocabulary and the Chinese vocabulary contain 8K and 30K BPE tokens, respectively. English PLATO-2 has three sizes: 9.3M, 314M, and 1.6B parameters. Chinese PLATO-2 has only one size, 336M parameters. The maximum sequence lengths of context and response are all set to 128. $$K$$ is set to 20 for the discrete latent variable.

Both automatic and human evaluations are used in this study. In automatic evaluation, the corpus-level metric of distinct-1/2 is used to assess the model's capacity on lexical diversity, which is defined as the number of distinct uni- or bi-grams divided by the total number of generated words. In human evaluation, four metrics are used, including two utterance-level metrics: coherence and informativeness, two dialogue-level metrics: engagingness and humanness. Three crowd-sourcing workers are asked to score the response/dialogue quality on a scale of
[0, 1, 2], with the final score determined through majority voting. The higher score, the better.

In self-chat evaluations, a model plays the role of both partners in the conversation and the chat logs are evaluated by crowd-sourcing workers. Each self-chat conversation is started with a given pre-selected topic (from the classical 200 questions). There are 10 utterances in each dialogue, including the input start utterance. Automatic evaluation is carried out on the 200 self-chat logs and human evaluation is conducted on 50 randomly selected conversations. Models are compared in three pairs: (PLATO-2 93M, PLATO 132M), (PLATO-2 310M, DialoGPT 345M), and (PLATO-2 1.6B, Blender 2.7B). Two self-chat logs with the same start topic from a pair are displayed to three crow-sourcing workers who are instructed to evaluate only one speaker within a dialogue. The self-chat evaluation results indicate that PLATO-2 1.6B model obtains the best performance across human and automatic evaluations. PLATO-2 outperforms Blender, DiaoGPT, and PLATO in each of the three pairs. The results also show that enlarging model scales and exploiting human annotated conversations (Blender) help improve the dialogue quality.

In the Chinese evaluation, the pair (PLATO-2 336M Chinese, Microsoft XiaoIce) are compared using human-bot chat evaluation where each interactive conversation is started with a pre-selected topic and continues for 7~14 rounds. The collected human-bot conversations are evaluated by crowd-sourcing workers. The XiaoIce obtains higher distinct values, suggesting that a retrieval-based strategy may yield better lexical diversity than a generation-based approach. The human evaluations indicate that PLATO-2 significant outperforms XiaoIce across all the human evaluation metrics.

To include Meena for comparison, static evaluation is used, where each model produces a response towards the given multi-turn context. The 60 static samples in the Meena paper are used. In all the three human evaluation metrics, coherence, informativeness, and engagingness, the performance is in the order: $$PLATO$$-$$2$$ $$1.6B > Blender > Meena > DialoGPT$$. Some case analyses show that Blender tends to switch topics quickly in a short conversation, but PLATO-2 can stick to the start topic and conduct in-depth discussions.

Furthermore, PLATO-2 has achieved the first place in three tasks of DSTC9, including interactive evaluation of open-domain conversation (Track3-task2), static evaluation of knowledge grounded dialogue (Track3-task1), and end-to-end task-oriented conversation (Track2-task1)<sup>[\[11\]](#ref11)</sup>.

### **Style-Controlled Generation**

It has been shown that utilization of style traits (e.g., curious, businesslike, emotional, knowledgeable, etc.) promotes engagingness in image grounded conversation task<sup>[\[13\]](#ref13)</sup>. Smith et al., 2020<sup>[\[12\]](#ref12)</sup> studied style-controlled open-domain dialogue generation using three different approaches: (1) a retrieve-and-style-transfer approach, (2) a plug-and-play language model (PPLM) approach, and (3) a conditional language model approach. The results show that the third approach performs best in controlling the target style of the generation and has the best inference speed.

The style space is provided by the Image-Chat (IC) dataset<sup>[\[13\]](#ref13)</sup> of 3-turn conversations discussing an image. Each partner in each conversation conveys a given style from a set of 217 styles that are partitioned into three groups: positive, neutral, and negative. The distribution of styles in the dataset is reasonably balanced. This dataset is not a purely textual conversational dataset because both conversation partners are referring to an image. But the IC dataset can be used to train a style classifier that can be used to assign style labels for utterances in purely textual conversational dataset. The purely textual conversational datasets used in this study include pushshift.io Reddit dataset for pre-training and four specialized dialogue datasets, collectively denoted as $$D$$, for fine-tuning training: (1) ConvAI2, (2) Wizard of Wikipedia, (3) Empathetic Dialogues, (4) BlendedSkillTalk (BST). The style classifier is run on $$D$$ to augment each utterance with a style label and the resulting dataset is denoted as $$D+$$.

The retrieve-and-style-transfer (RnST) is a hybrid approach that combines a retriever and a style-controlled generator. The retriever is a 660M-parameter poly-encoder model adopted from the Recipes<sup>[\[7\]](#ref7)</sup>. The generator, denoted as $$R$$, is a pushshift.io Reddit pre-trained 2.7B-parameter standard Seq2Seq Transformer model, also adopted from Recipes<sup>[\[7\]](#ref7)</sup>. To enable style control, the $$R$$ is then fine-tuned on IC with the ground-truth style appended to the dialogue context. Because IC is not purely conversational, the generator is further fine-tuned on $$D$$, which does not contain style labels. The resulting model is denoted as RnST-IC+D. The style classifier also consists of the $$R$$ with an added linear layer with a hidden dimension of 2560 on top of the decoder output. The classifier is fine-tuned on all weights using turns 2 and 3 of the IC training set with the provided labels. The retrieved reply in the RnST+IC+D is appended to the input of the generator. The style classifier is also used to classify style-controlled generations to determine the accuracy of style control. The accuracy of generations at matching target style by RnST-IC+D is 15.8%, lower than 16.7% by style-conditioned generative model (C100-IC+D), indicating that adding a retrieved utterance to the context string hurts style control. Also, the accuracy from RnST-IC+D on test context of IC and BST are 15.8% and 3.3%, respectively, indicating that style control is not transferring well from IC to BST.

The plug-and-play language model (PPLM) approach in this study is modified from Dathathri et al. (2020)<sup>[\[14\]](#ref14)</sup> that requires a generative model to plug in, with a classifier head on top. The generative model here is the $$R$$. The PPLM generative method requires no fine-tuning on the given generative model. The classifier head here is a simple linear layer with an input dimension of 2560, and as many output units as classes, fine-tuned either on SST-5 (binary sentiment of movie review task) or on turns 2 and 3 of IC, with the decoder output averaged across time. The style control is obtained gradually through iteratively refining generations to match target styles at inference time. $$k$$ tokens are picked at each timestep by sampling the token distribution with top-$$k$$ filtering $$(k=10)$$. A generation is stopped when it hits an end-of-sentence token. The use of a guiding classifier to directly modify output activation allows to go not only “towards” a desired style, but also “away” from it. However, the inference is also much more costly. Experimental results show that the PPLM approach requires much fewer GPU-memory-hours and converges much faster during fine-tuning training, compared to conditional generation. However, during generation, PPLM performs much worse than conditional generation (C75) on the accuracy of IC style matching (1.7% vs 7.1%) and generation time (45.6s vs 1.7s).

The conditional language model approach simply relies on conditioning tokens appended to the dialogue context. The models are based on the $$R$$, the 2.7B pushshift.io Reddit pre-trained generator. This method requires whole-architecture fine-tuning to learn to use the augmented input, but inference is straightforward. The $$R$$ is fine-tuned on $$D+$$, with a kind of 'style dropout' that appends a separator and the style label to the end of context only a percentage of times. These models are denoted as $$C$$. Three fine-tuned versions, $$C0$$, $$C75$$, and $$C100$$ are built by randomly appending target style for 0%, 75%, and 100% of the training examples, respectively. During training, examples are sampled from the ConvAI2, ED, WoW, and BST datasets with a ratio of 1:2:1:1. For style-controlled generation with the fine-tuned models, the following parameters are used: beam search with a beam size of 10, a minimum beam length of 20, and n-gram blocking of size 3 in both the beams and the context, following the Recipes<sup>[\[7\]](#ref7)</sup>. Generations take roughly 2.0 seconds per generation, with a batch size of 32 across 4 GPUs, and generation speeds are roughly equivalent with and without style conditioning. Analyses of sampled results show that style can be controlled with a clear differentiation between different styles, while keeping the responses both fluent and relevant to the dialogue context. The style accuracies on the IC test set as context, with target label distribution according to the distribution that the models were fine-tuned on, are 1.1%, 29.3%, and 31.6% for C0, C75, and C100, respectively. For human evaluations, evaluators are asked to converse with the models and then try to guess the style that the model was conditioned on out of a set of 5 choices. Accuracies of the guesses are 14.2%, 34.9%, and 41.3% for C0, C75, and C100, respectively. The evaluators are also asked to rate in scale of 1 to 5 how empathetic, relevant, human-like, and engaging the model’s responses are. Models conditioned on style during generation are somewhat less human-like. In conclusion, the conditional generation approach can convincingly generate sets of varied conversational replies that display the desired style and its style accuracy is higher than those of the RnST and PPLM approaches in comparable settings.

### **DialogBERT**

All the generation-based approaches above consider the dialogue context as a linear sequence of tokens and learn to generate the next word through token-level attention without considering the relationships between utterances of the dialogue context. To alleviate the issue, Gu et al., 2020<sup>[\[15\]](#ref15)</sup> introduced DialogBERT that employs a hierarchical Transformer architecture to encode the utterances of dialogue context and captures discourse-level coherence among utterances using two training objectives analogous to the original BERT training: (1) masked context regression, which masks a randomly-selected utterance and predicts the encoding vector for the masked utterance directly; and (2) distributed
utterance order ranking, which organizes randomly shuffled utterances of a conversation into a coherent dialogue context through a Learning-to-Rank neural network.

Given a dialogue $$\mathcal{D}=(u_1,u_2,...,u_T)$$ with $$T$$ number of utterances, the dialogue context (history) is $$\mathcal{C}=(u_1,u_2,...,u_{T-1})$$ and the response is $$u_T$$. The $$i$$-th utterance in $$\mathcal{C}$$ is $$u_i=(w_1^i,w_2^i,...,w_{\vert u_i\vert}^i)$$, where $$w_j^i$$ is the $$j$$-th word in $$u_i$$. The goal here is to generate the next utterance (response) $$u_T$$ that is coherent to the context $$\mathcal{C}$$. Two bidirectional transformer encoders (BERT models) are hierarchically nested: an utterance encoder $$f_{\theta}(\cdot)$$ to transform each utterance in C to a vector and a context encoder $$g_{\phi}(\cdot)$$ to learn utterance representations given their surrounding utterances in the context, as illustrated in the figure below. The [CLS] and [SEP] tokens are added at the first and the last positions of each utterance $$u_i$$, respectively, so that $$w_1^i=$$[CLS] and $$w_{\vert u_i\vert}^i=$$[SEP]. An embedding layer maps $$u_i$$ onto a continuous space: $$\mathrm{e}_i=(\mathrm{w}_1^i+\mathrm{p}_1,\mathrm{w}_2^i+\mathrm{p}_2,...,\mathrm{w}_{\vert u_i\vert}^i+\mathrm{p}_{\vert u_i\vert})$$ where $$\mathrm{w}_j^i$$ and $$\mathrm{p}_j$$ are the word and positional embeddings of $$w_j^i$$, respectively. Then, the utterance encoder $$f_{\theta}(\cdot)$$ transforms $$\mathrm{e}_i$$ into a list of hidden representations $$(\mathrm{u}_1^i,\mathrm{u}_2^i,...,\mathrm{u}_{\vert u_i\vert}^i)=f_{\theta}(\mathrm{e}_i)$$. The first hidden representation $$\mathrm{u}_1^i$$, i.e., the representation at the [CLS] token, is taken as the representation of the utterance $$u_i$$. The utterance position $$\mathrm{p}_i$$ is also incorporated into the final representation of $$u_i$$ as $$\mathrm{u}_i=\mathrm{u}_1^i+\mathrm{p}_i$$. Then, the context encoder $$g_{\phi}(\cdot)$$ transforms the sequence of utterance representations $$(\mathrm{u}_1,\mathrm{u}_2,...,\mathrm{u}_{\vert \mathcal{C}\vert})$$ into context sensitive utterance representations $$\mathrm{H}=(\mathrm{h}_1,\mathrm{h}_2,...,\mathrm{h}_{\vert \mathcal{C}\vert})=g_{\phi}(\mathrm{u}_1,\mathrm{u}_2,...,\mathrm{u}_{\vert \mathcal{C}\vert})$$.
<p align="center"><img src="../../../assets/images/dialogBERT1.png"></p>
The primary training objective is to generate the next utterance (response) given the dialog context. A Transformer decoder $$p_{\psi}(\cdot)$$ is used to generate the next utterance $$u_T=(w_1^T,...,w_N^T)$$, where $$w_1^T=$$[CLS]. The decoder predicts each word $$w_j^T$$ conditioned on $$w_1^T,...,w_{j-1}^T$$ and $$\mathrm{h}_1,...,\mathrm{h}_{\vert \mathcal{C}\vert}$$ by estimating the following probability distribution: $$p(u_T\vert\mathcal{C},\theta,\phi,\psi)=\prod_{j=1}^Np_{\psi}(w_j^T\vert w_{<j}^T,\mathrm{H})$$, where $$N$$ represents the maximum sequence length for decoding and $$\theta$$, $$\phi$$, and $$\psi$$ denote the model parameters of the utterance encoder, the context encoder, and the decoder, respectively. The next utterance generation task aims to minimize the cross-entropy loss in the decoder:

$$\mathcal{L}_{dec}(\theta,\phi,\psi|w_1^T,...,w_N^T,\mathcal{C})=-\sum\limits_{i=1}^N\log p_{\psi}(w_i^T|w_{<i}^T,\mathrm{H})$$

where $$N$$ denotes the maximum sequence length of the generated response.

The first auxiliary task for enhancing context representation learning is the masked utterance regression (MUR), as illustrated in the figure below. Given a dialogue context $$\mathcal{C}=(u_1,u_2,...,u_{T-1})$$, one utterance is randomly selected from $$\mathcal{C}$$, which is replaced with a mask utterance [CLS, MASK, SEP] 80% of the time, replaced with a random utterance from the training set 10% of the time, and unchanged 10% of the time. The masked context is denoted as $$\tilde{\mathcal{C}}=(\tilde{u}_1,\tilde{u}_2,...,\tilde{u}_{\vert\mathcal{C}\vert})$$. The goal is to predict the original utterance vectors from $$\tilde{\mathcal{C}}$$. The $$\tilde{\mathcal{C}}$$ is first transformed by the hierarchical encoder to its context sensitive utterance representations $$(\tilde{\mathrm{h}}_1,\tilde{\mathrm{h}}_2,...,\tilde{\mathrm{h}}_{\vert \mathcal{C}\vert})$$. Then, these representations are transformed back to the original utterance vectors using a fully connected neural network (Encoding Converter): $$\hat{\mathrm{u}}_i=\mathrm{W}\tilde{\mathrm{h}}_i+\mathrm{b}$$ where $$\hat{\mathrm{u}}_i$$ denotes the predicted original utterance vector and $$\mathrm{W}$$ and $$\mathrm{b}$$ are trainable parameters. The objective aims to minimize the mean squared error (MSE) between the estimated representations of masked utterances and their original vectors:

$$\mathcal{L}_{mur}(\theta,\phi,\mathrm{W},\mathrm{b}|\tilde{\mathrm{u}}_1,...,\tilde{\mathrm{u}}_{\vert \mathcal{C}\vert},\mathcal{C},\tilde{\mathcal{C}})=\frac{1}{|\tilde{\mathcal{C}}\backslash\mathcal{C}|}\sum\limits_{u_i\in\tilde{\mathcal{C}}\backslash\mathcal{C}}\Vert\hat{\mathrm{u}}_i-\mathrm{u}_i\Vert_2^2$$

where $$\tilde{\mathcal{C}}\backslash\mathcal{C}$$ denotes the set of masked utterances and $$\theta$$, $$\phi$$, $$\mathrm{W}$$, and $$\mathrm{b}$$ are training parameters.
<p align="center"><img src="../../../assets/images/dialogBERT2.png"></p>
The second auxiliary task in learning the representation of dialogue contexts is the utterance re-ordering task that is to organize randomly shuffled utterances of a conversation into a coherent dialogue context. Given a context $$\mathcal{C}=[u_{o_1},u_{o_2},...,u_{o_{\vert\mathcal{C}\vert}}]$$ with order $$o=[o_1,o_2,...,o_{\vert\mathcal{C}\vert}]$$, the goal is to find an ordered context $$\mathcal{C}^*=[u_{o_1^*},u_{o_2^*},...,u_{o_{\vert\mathcal{C}\vert}^*}]$$ where $$o^*=[o_1^*,o_2^*,...,o_{\vert\mathcal{C}\vert}^*]$$ is the most coherent permutation of utterances. For example, the correct order for the utterances in the figure below is $$o^*$$ = [3, 1, 2]. A distributed order ranking network (DORN) is placed on top of the context encoder to predict the order index of each utterance in a distributed manner. As shown in the figure below, DORN takes as input the hidden status of the shuffled utterances from the context encoder and produces a score for each individual utterance. These scores are then used for re-ordering these utterances (i.e., sorting these scores provides the correct
ordering in the context). The order prediction network computes the pairwise inner products between hidden states of utterances and then calculates the scores $$s_i$$ for each utterance $$u_i$$ by averaging all its inner products to other utterances: $$s_i=\frac{1}{\vert\mathcal{C}\vert}\sum\limits_{j=1}^{\vert\mathcal{C}\vert}\mathrm{W}\mathrm{h}_i^{\top}\mathrm{h}_j$$ where $$\mathrm{W}$$ denotes the parameters of DORN. The predicted scores are viewed as the extent to which each utterance is ranked in the first place in a context. The "rank-1" probability for each utterance is estimated by softmax of the predicted scores: $$\hat{P}(u_i)=\frac{exp(s_i)}{\sum_{j=1}^{\vert\mathcal{C}\vert}exp(s_j)}$$. A gold target score $$y_i$$ is assigned to each utterance $$u_i$$ to indicate the ground truth order. In this study, $$y_i=\frac{i}{\vert\mathcal{C}\vert}$$, $$y_i\in[0,1]$$. Then, the "rank-1" probability estimated by the gold target scores is given by: $$P(u_i)=\frac{exp(y_i)}{\sum_{j=1}^{\vert\mathcal{C}\vert}exp(y_j)}$$. The goal is to minimize the KL divergence between the two distributions:

$$\mathcal{L}_{duor}(\theta,\phi,\mathrm{W}|P,C)=\mathrm{KL}(\hat{P}(u)\Vert P(u))=\sum\limits_{k=1}^{\vert\mathcal{C}\vert}\hat{P}(u_k)\log(\frac{\hat{P}(u_k)}{P(u_k)})$$

<p align="center"><img src="../../../assets/images/dialogBERT3.png"></p>
The final objective is defined as the weighted sum of each loss function:

$$\mathcal{L}_{total}=\mathcal{L}_{dec}+\lambda_0\times\mathcal{L}_{mur}+\lambda_1\times\mathcal{L}_{duor}$$

, where $$\lambda_0$$ and $$\lambda_1$$ denote the coefficients of each loss. In this study, both $$\lambda_0$$ and $$\lambda_1$$ are set to 1.

Three dialogue datasets are used for evaluations: Weibo, MultiWOZ, and DailyDialog. For the Weibo dataset, the utterance encoder, the context encoder, and the decoder all use the hyperparameter settings of "BERT-base-Chinese" ($$\mathrm{L}$$=12, $$\mathrm{H}$$=768, $$\mathrm{A}$$=12). For MultiWOZ and DailyDialog, the settings of reduced "BERT-base-uncased" $$\mathrm{L}$$=6, $$\mathrm{H}$$=256, $$\mathrm{A}$$=2 are used. The utterance number in each context and the word number in each utterance are limited to 7 and 30, respectively. All of the experiments use the default BERT tokenizer. During response generation, top-1 sampling is performed according to the probabilities estimated by the decoder. Three other Transformer-based response generation methods are used as baseline models: (1) BART, (2) DialoGPT, and (3) ContextPretrain (using the Transformer-based decoder in place of the RNN decoder). The same hyperparameter settings are used for all baseline models. Automatic evaluation metrics include perplexity, BLEU, and NIST (a BLEU variant that penalizes uninformative n-grams by assigning weights to n-grams according to their information gain).

DialogBERT outperforms BART and DialoGPT (both have flat context encoding) on all metrics with a large margin across all three datasets, affirming the superiority of the hierarchical Transformer architecture of DialogBERT. Ablation studies show that both the proposed masked utterance regression and distributed utterance order prediction achieve substantial improvements over a simple hierarchical Transformer. Furthermore, combining both objectives further enhances the performance. The improvement on the Weibo dataset is relatively more significant, probably due to the richness of the data, allowing more room for fitting using the auxiliary objectives. Human evaluation on three criteria, coherence, informativeness, and human-likeness, also show that DialogBERT moderately outperforms the baseline models. Case analyses of generated responses show that DialogBERT generates more coherent responses than the baseline models.

Due to the extra "context encoder", DialogBERT is much larger than the baseline models: (337.6M, 139.4M, 102.1M, 20.5M) for (DialogBERT, BART, DialoGPT, ContextPretrain) on Weibo and (40.2M, 24.2M, 12.7M, 23.3M) for (DialogBERT, BART, DialoGPT, ContextPretrain) on MultiWOZ and DailyDialog. It is unclear how much of the performance improvement is contributed by simply increasing the number of parameters, instead of capturing utterance-level relationships.

## **Hybrid Approaches**

Retrieval-based approaches cannot generate sentences that are not already existing in the pre-collected dataset. On the other hand, generation-based approaches tend to produce dull and repetitive responses and may hallucinate knowledge. Hybrid approach can alleviate these problems by combining a retrieval step before generation.

### **Recipes**

Roller et al., 2020<sup>[\[7\]](#ref7)</sup> introduced **Recipes** (also known as **Blender**) for building an open-domain chatbot by using Blended Skill Talk (BST) set-up and comparing the performance of retrieval, generative, and hybrid models. Their best models significantly outperform Meena in human evaluations of engagingness and humanness.

The retriever model uses poly-encoder architecture<sup>[\[5\]](#ref5)</sup>. Two sizes are considered: 256M and 622M parameter models, both using $$N=64$$, the number of encoded representations (codes) for the context. The generator model uses standard Seq2Seq Transformer architecture. Three sizes are considered: 90M, 2.7B, and 9.4B parameters. The 2.7B parameter model roughly mimics the architecture choices of Meena, with 2 encoder layers, 24 decoder layers, 2560-dimensional embedding, and 32 attention heads. The 9.4B parameter model has 4 encoder layers, 32 decoder layers, 4096-dimensional embedding, and 32 attention heads. The hybrid model is referred to as a Retrieve and Refine (RetNRef), where a retrieval step is done before the generation. Two variants for the retrieval step are considered: dialogue retrieval and knowledge retrieval. In the dialogue retrieval variant, the retrieval step simply uses the poly-encoder retriever. The retriever first produces a response for the given dialogue history. The response is then appended to the input sequence of the generator, along with a special separator token. The generator then outputs a response as normal given this modified input sequence. In the knowledge retrieval variant, the retrieval system first uses a TF-IDF-based inverted index lookup over a Wikipedia dump to produce an initial set of knowledge candidates. Then, a Transformer-based poly-encoder retriever model is used to rank the candidates and select a single sentence that is used to condition generation. Additionally, a Transformer-based two-class classifier is trained to determine whether a context requires knowledge or not in fine-tuning tasks.

The training objective of the retrieval models is to minimize a cross-entropy loss in which the logits are $$y_{cand_{1}},...,y_{cand_{n}}$$, where $$y_{cand_{1}}$$ is the score of the correct response and the rest are sampled negatives. During training, the other responses in the batch are used for negatives. The training objective of the generative models is to minimize the loss of Maximum Likelihood Estimation (MLE) for a given dataset $$\mathcal{D}=\{(\mathrm{x}^{(i)},\mathrm{y}^{(i)})\}:$$

$$\mathcal{L}_{\mathrm{MLE}}^{(i)}(p_{\theta},\mathrm{x}^{(i)},\mathrm{y}^{(i)})=-\sum\limits_{t=1}^{|y^{(i)}|}\log p_{\theta}(y_{t}^{(i)}|\mathrm{x}^{(i)},y_{<t}^{(i)}),$$

where $$\mathrm{x}^{(i)}$$ is a gold input context and $$\mathrm{y}^{(i)}$$ is a gold next-utterance, and $$y_t^{(i)}$$ is the $$t$$-th token of $$\mathrm{y}^{(i)}$$. In dialogue retrieval variant, the retrieved response is replaced with the gold response $$\alpha$$% of the time, treating $$\alpha$$ as a hyperparameter to be tuned. This gives a smooth transition between retrieval and generator-only systems. In knowledge retrieval variant, only the gold knowledge of the fine-tuning datasets is used during training. Generative models tend to show phrase repetitions and overrepresentation of common vocabulary tokens, which can be reduced by unlikelihood loss training. The unlikelihood loss penalizes a set of negative candidate tokens $$\mathcal{C}_t$$ at each time-step,

$$\mathcal{L}_{\mathrm{UL}}^{(i)}(p_{\theta},\mathcal{C}_{1:T},\mathrm{x},\mathrm{y})=-\sum\limits_{t=1}^{|y|}\sum\limits_{y_c\in\mathcal{C}_t}\log(1-p_{\theta}(y_c|\mathrm{x},y_{<t})),$$

where $$\mathcal{C}_t\subseteq\mathcal{V}$$ is a subset of vocabulary. The overall objective in unlikelihood training is a weighted mixture of the likelihood and unlikelihood losses:

$$\mathcal{L}_{\mathrm{ULE}}^{(i)}=\mathcal{L}_{\mathrm{MLE}}^{(i)}+\alpha\mathcal{L}_{\mathrm{UL}}^{(i)},$$

where $$\alpha\in\mathrm{\mathbb{R}}$$ is the mixing hyper-parameter. Likelihood pushes up the probability of a gold token $$y_t^{(i)}$$ while unlikelihood pushes down the probability of negative candidate tokens $$y_c\in\mathcal{C}_t$$. In this study, negative candidates are chosen by a running count of the distribution of n-grams that appear when generating from the model and choosing tokens from these n-grams when their counts are above the human distribution counts as measured from the gold responses.

The choice of decoding algorithm to generate a response is of critical importance in generative models at inference time. Two models with the same perplexity but different decoding algorithms can give drastically different results. Four well-known approaches are compared in this study: (1) Beam Search, (2) Sampling, (3) Response Length, (4) Subsequence Blocking. Beam search tends to generate responses that are shorter than the human utterances they were trained on. Longer responses can be less dull, more informative and engaging. Two length control methods are considered in this study: (1) Minimum Length, where end token is forced to not be generated until a minimum length is achieved, (2) Predictive Length, where a 4-class classifier is used to predict the length range of the next conversation turn and set the minimum generation length constraint accordingly. Both beam search and sampling approaches are known to repeat subsequences. Standard beam blocking of repeated n-grams ($$n=3$$) is implemented both within the generated utterance (in-turn) and the input sequence (cross-turn).

The pre-training dataset is the Reddit discussion dataset available on pushshift.io, containing 1.5B training examples with a vast range of topics. Models are trained with maximum context and response lengths of 128 BPE tokens and longer examples are truncated. After heuristic rules filtering, the final dataset contains 1.5B comments totaling 88.8B context tokens and 56.8B response tokens. Fine-tuning datasets are smaller, cleaner, and more focused on some desirable traits, including (1) ConvAI2, (2) Empathetic Dialogues (ED), (3) Wizard of Wikipedia (WoW), and (4) Blended Skill Talk (BST). ConvAI2 is based on PersonaChat dataset, containing 140K conversation utterances between two crowdworkers, each of which is given a persona to play. The persona and the dialogue history are concatenated to condition the generation. ED dataset contains 50K utterances of crowdworker conversations grounded in an emotional situation, where one speaker describes a personal situation and the other displays empathy during the discussion. The WoW dataset consists of 194K utterances over 1,250 topics, which are grounded on Wikipedia during the crowdworker conversations. The BST dataset contains 76K utterances, collected with a guided and unguided human speaker, where the guided speaker can select utterances suggested by bots trained on the three individual datasets.

The main evaluation method in this study is human evaluation based on ACUTE-Eval procedure, whereby human evaluators are asked to make pairwise evaluations of complete multi-turn human-model dialogues. This allows conversations collected in previous trials and by other systems to be directly compared with a new system. Two evaluation questions are asked: (1) Engagingness question: “Who would you prefer to talk to for a long conversation?”, (2) Humanness question: “Which speaker sounds more human?”. Self-Chat ACUTE-Eval is a variant of ACUTE-Eval, in which models are used for both sides of a conversation. This reduces the resource requirements of the conversation collection. Results from model-model self-chat experiments highly correlate with those of human-model chat experiments, for most, but not all systems. The retrieval models are automatically evaluated by Hits@1/K metrics after fine-tuning with each of the 4 datasets. The generative and RetNRef models are automatically evaluated by perplexity before and after fine-tuning each of the 4 datasets. Fine-tuning gives relatively large improvements in perplexity on these tasks, which could translate into improved ability at these skills when conducting open-domain dialogue.

In self-chat evaluation on engagingness, the 3 types of models are compared, all of which are fine-tuned on BST dataset and generation uses standard beam search (beam size 10) without minimum length constraint, but with 3-gram repeat blocking. The performance is in the order: Retrieval (256M) > RetNRef > pure Generator (90M). This initial result comes with the caveat that relative performance may be different for differently sized models, or for different training or decoding strategies. Both Minimum Length and Predictive Length methods significantly improve the engagingness of the Generative 2.7B model over not forcing the minimum length. Beam blocking does not significantly improve engagingness. Beam Search with beam size in {1, 10, 30} or sampling using either Top-k (k=40) or Sample (20 times)+Rank do not show statistically significant difference on engagingness. Fine-tuning the pre-trained generative model on BST dataset greatly improves the engagingness, indicating the importance of personality, knowledge, and empathy on engagingness. Adding persona context for generative model fine-tuned on BST dataset does not significantly improve engagingness. Unlikelihood training with $$\alpha=0.25$$ does not significantly improve engagingness.

In human-bot chat evaluations, conversation data are collected from open-ended chat that begins with the message "Hi!" from the human to the bot, and has a minimum interactive conversation length of 14 turns, collecting 100 conversations per model via crowdworkers. With minimum length constraint 20 in beam search decoding, the Generative model (90M) and the RetNRef model are both greatly outperform the Retrieval model (256M) on engagingness, but there is no difference between the Generative model and the RetNRef model. Larger generative and RetNRef models with BST fine-tuning and length-controlled decoding greatly outperform Meena on both engagingness and humanness. The average response length of Meena and human (in human-human chat) are 10.4 and 18.0, respectively. Those of the Generative BST (2.7B) model without and with minimum length constraint (of 20) are 9.5 and 21.3, respectively. Humans speaking to models (or other humans) often match response length if they are engaged in the conversation, and there appears to be correlation of their average response length with engagement.

## **Codes**

- [Poly-encoder](https://github.com/facebookresearch/ParlAI/blob/master/parlai/agents/transformer/polyencoder.py) or [Poly-encoder](https://github.com/facebookresearch/ParlAI/tree/master/projects/polyencoder)
- [BlendedSkillTalk](https://parl.ai/projects/bst/)
- [DF-QSM](https://github.com/fuzhenxin/Query-Session-Matching-CIKM2020)
- [DialoGPT](https://github.com/microsoft/DialoGPT) or [DialoGPT](https://huggingface.co/microsoft/DialoGPT-medium)
- [Recipes](https://parl.ai/projects/recipes/)
- [PLATO](https://github.com/PaddlePaddle/Research/tree/master/NLP/Dialogue-PLATO)
- [PLATO-2](https://github.com/PaddlePaddle/Knover/tree/develop/projects/PLATO-2)
- [Style-Controlled Generation](https://github.com/facebookresearch/ParlAI/blob/master/parlai/zoo/style_gen/c75_labeled_dialogue_generator.py)
- [Plug and Play Language Model](https://github.com/uber-research/PPLM)
- [DialogBERT](https://github.com/guxd/DialogBERT)

## **References**

<a name="ref1">[1]</a> Sun, B. and Li, K. (2021) [Neural Dialogue Generation Methods in Open Domain: A Survey](https://www.atlantis-press.com/article/125954217.pdf). Natural Language Processing Research, Vol. 1(3-4), pp. 56–70

<a name="ref2">[2]</a> Huang, M., Zhu, X., and Gao, J. (2020) [Challenges in Building Intelligent Open-domain Dialog Systems](https://arxiv.org/pdf/1905.05709.pdf). ACM Transactions on Information Systems, Vol. 38, No. 3, pp. 1-32

<a name="ref3">[3]</a> Zhang, Y., Sun, S., Galley, M., Chen, Y., Brockett, C., Gao, X., Gao, J., Liu, J., and Dolan, B. (2019) [DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/pdf/1911.00536.pdf). arXiv preprint arXiv:1911.00536

<a name="ref4">[4]</a> Adiwardana, D., Luong, M., So, D., Hall, J., Fiedel, N., Thoppilan, R., Yang, Z., Kulshreshtha, A., Nemade, G., Lu, Y., and Le, Q. (2020) [Towards a Human-like Open-Domain Chatbot](https://arxiv.org/pdf/2001.09977.pdf). arXiv preprint arXiv:2001.09977

<a name="ref5">[5]</a> Humeau, S., Shuster, K., Lachaux, M., and Weston, J. (2019) [Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring](https://arxiv.org/pdf/1905.01969.pdf). arXiv preprint arXiv:1905.01969

<a name="ref6">[6]</a> Smith, E. M., Williamson, M., Shuster, K., Weston, J., and Boureau, Y-L. (2020) [Can you put it all together: Evaluating conversational agents' ability to blend skills](https://arxiv.org/pdf/2004.08449.pdf). arXiv preprint arXiv:2004.08449

<a name="ref7">[7]</a> Roller, S., Dinan, E., Goyal, N., Ju, D., Williamson, M., Liu, Y., Xu, J., Ott, M. et al. (2020) [Recipes for building an open-domain chatbot](https://arxiv.org/pdf/2004.13637.pdf). arXiv preprint arXiv:2004.13637

<a name="ref8">[8]</a> Fu, Z., Cui, S., Ji, F. Zhang, J., Chen, H., Zhao, D., and Yan, R. (2020) [Query-to-Session Matching: Do NOT Forget History and Future during Response Selection for Multi-Turn Dialogue Systems](https://dl.acm.org/doi/10.1145/3340531.3411938). In: Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM ’20), October 19–23, 2020, Virtual Event, Ireland. ACM, New York, NY, USA, 10 pages.

<a name="ref9">[9]</a> Li, J., Liu, C., Tao, C., Chan, Z., Zhao, D., Zhang, M., and Yan, R. (2021) [Dialogue History Matters! Personalized Response Selection in Multi-turn Retrieval-based Chatbots](https://arxiv.org/pdf/2103.09534.pdf). arXiv preprint arXiv:2103.09534

<a name="ref10">[10]</a> Bao, S., He, H., Wang, F., Wu, H., Wang, H., Wu, W., Guo, Z., Liu, Z., Xu, X. (2020) [PLATO-2: Towards Building an Open-Domain Chatbot via Curriculum Learning](https://arxiv.org/pdf/2006.16779.pdf). arXiv preprint arXiv:2006.16779

<a name="ref11">[11]</a> Bao, S., Chen, B., He, H., Tian, X., Zhou, H., Wang, F., Wu, H., Wang, H., Wu, W., and Lin, Y. (2021) [A Unified Pre-training Framework for Conversational AI](https://arxiv.org/pdf/2105.02482.pdf). arXiv preprint arXiv:2105.02482

<a name="ref12">[12]</a> Smith, E. M., Gonzalez-Rico, D., Dinan, E., and Boureau, Y-L. (2020) [Controlling Style in Generated Dialogue](https://arxiv.org/pdf/2009.10855.pdf). arXiv preprint arXiv:2009.10855

<a name="ref13">[13]</a> Shuster, K., Humeau, S., Bordes, A., and Weston, J. (2018) [Image-Chat: Engaging Grounded Conversations](https://arxiv.org/pdf/1811.00945.pdf). arXiv preprint arXiv:1811.00945

<a name="ref14">[14]</a> Dathathri, S., Madotto, A., Lan, J., Hung, J., Frank, E., Molino, P., Yosinski, J., and Liu, R. (2020) [Plug and play language models: A simple approach to controlled text generation](https://openreview.net/pdf?id=H1edEyBKDS). In: International Conference on Learning Representations.

<a name="ref15">[15]</a> Gu, X., Yoo, K. M., and Ha, J.-W. (2020) [DialogBERT: Discourse-Aware Response Generation via Learning to Recover and Rank Utterances](https://arxiv.org/pdf/2012.01775.pdf). arXiv preprint arXiv:2012.01775
